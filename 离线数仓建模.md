# 离线数仓建模

## 第一章 数仓分层

### 1.1 数据分层目的

1. 复杂问题简单化：分解复杂的任务逻辑，方便定位问题
2. 减少重复开发：通过合理的设计中间层数据，极大地减少重复的计算，增加计算的复用性
3. 隔离原始数据：解耦真实数据与统计数据



### 1.2 数据集市与数据仓库

- 数据集市是一种微型的数据仓库，是**部门级**的，一般服务于某个范围内的管理人员
- 数据仓库是**企业级**的，为整个企业的各个部门的运行提供支持



### 1.3 数仓命名规范

#### 1.3.1 表的命名

- ODS层表的命名使用ods_前缀
- DWD层表的命名使用dwd_前缀
- DWS层表的命名使用dws_前缀
- DWT层表的命名使用dwt_前缀
- ADS层表的命名使用ads_前缀
- 临时表的命名使用_tmp后缀
- 用户行为表的命名使用_log后悔



#### 1.3.2 脚本的命名

- [数据源]\_to\_[目标]_db/log.sh
- 用户行为脚本使用log后缀，业务数据脚本使用db后缀



#### 1.3.3 表字段类型

- 整形数值使用bigint
- 金额类型使用decimal(16,2)
  16表示整数+小数位共16位
- 字符串使用string
- 主键和外键使用string
- 时间戳使用bigint



## 第二章 数仓理论

### 2.1 范式理论

- 定义：设计一张数据表时，遵循的标准规范的级别。
- 遵循范式设计的优缺点
  - 优点：**降低数据冗余**
  - 缺点：统计数据时需要进行大量的多表关联操作（Join）
- 范式级别：第一范式（1NF），第二范式（2NF），第三范式（3NF），巴斯-科德范式（BCNF），第四范式（4NF），第五范式（5NF）
  每一个范式都是在前一个范式的前提上追加规定，常用级别为三范式（3NF）

#### 2.1.1 函数依赖

1. 完全函数依赖：c=f(a,b)，称c完全依赖于a,b
2. 部分函数依赖：c=f(a)，称c部分依赖于a,b
3. 传递函数依赖：c=f(b)，b=f(a)，称c传递依赖于a



#### 2.1.2 三范式

1. 第一范式1NF：属性不可切分
   对于数据库中一张表的每一个列（字段），其代表的信息都应细化到不可继续拆分的程度。
2. 第二范式2NF：不能存在部分函数依赖
   表中的每个的字段完全依赖于主键。
3. 第三范式3NF：不能存在传递函数依赖
   表中多个字段含义与主键字段含义直接相关，不存在间接关系。



### 2.2 关系建模与维度建模

- OLTP与OLAP定义

  - 联机事务处理OLTP（On-Line Transaction Processing）：关系型数据库系统的主要应用
  - 联机分析处理OLAP（On-Line Analytical Processing）：数据仓库系统的主要应用

- OLTP与OLAP对比

  | **对比属性** | OLTP                       | OLAP                       |
  | ------------ | -------------------------- | -------------------------- |
  | **读特性**   | 每次查询只返回少量记录     | 对大量记录进行汇总         |
  | **写特性**   | 随机、低延时写入用户的输入 | 批量导入                   |
  | **使用场景** | 用户，Java EE项目          | 内部分析师，为决策提供支持 |
  | **数据表征** | 最新数据状态               | 随时间变化的历史状态       |
  | **数据规模** | GB                         | TB到PB                     |



#### 2.2.1 关系建模

- 关系建模严格遵循三范式，表的数量多，数据冗余非常少，功能性强
- 关系建模在处理大规模数据时，会进行大量的多表关联操作，导致执行效率低



#### 2.2.2 维度建模

- 维度建模的三种模型
  1. 星型模型：基于一张事实表建模，维度表仅有一层，直接通过事实表外键关联
  2. 雪花模型：基于一张事实表建模，维度表可有多层，尽可能遵循3NF
  3. 星座模型：基于多张事实表建模，维度表可有一或多层
- 维度建模模型的选择
  1. 根据实际需求确认事实表数量，若需要多张事实表则选择星座模型
  2. 若事实表仅有一张，优先业务性能时使用星型模型（常用），优先灵活度时选择雪花模型



### 2.3 维度表与事实表

#### 2.3.1 维度表

- 定义：对事实的描述信息，每一张维表对应现实世界的一类对象或概念。
- 特征
  1. 范围宽泛，字段相对较多
  2. 数据量较小
  3. 数据变化量小，内容相对固定
- 举例：用户表，日期表，商品表



#### 2.3.2 事实表

- 定义：业务事件的描述信息，事实表的每一行数据代表一个实际的业务事件，“事实”指该业务事件的度量值。
- 事实表的字段必须包括
  1. 事件的度量值：用于统计的字段（个数、金额等）
  2. 用于关联维度表的外键
- 特征
  1. 字段相对较少
  2. 数据量庞大
  3. 数据变化量大，随时间不断增加
- 事实表分类
  1. 事务型事实表
     - 以每个事务的发生为单位，逐条向事实表中追加数据，数据一旦插入不再更改，只做增量更新
     - 举例：支付记录
  2. 周期型快照事实表
     - 不记录每一次具体的业务操作变化，而是根据指定时间间隔，周期性的向表中插入当前时刻的事务状态数据
     - 举例：每月的用户账户余额，购物车商品
  3. 累计型快照事实表
     - 事务数据插入后会更改，跟踪事务的状态，不断更新事实表中的记录
     - 举例：订单状态



### 2.4 数据仓库建模

#### 2.4.1 ODS层

- 保持数据原貌，不作任何修改，起到备份的作用
- 数据压缩存储（lzo）
- 使用分区表存储



#### 2.4.2 DWD层

- 采用星型模型或星座模型对DWD层构建维度模型
- 构建过程
  1. 确定业务过程，一个业务过程对应一张事实表（如下订单、支付等）；
  2. 确定数据粒度，即事实表中一行数据描述的信息，为了满足后续的需求，应**尽可能选择最小粒度**，尽量细化；
  3. 确定维度，即事实表中某些字段数据的扩展关联表，用来描述该字段的扩展信息（时间，地点，用户等）；
  4. 确定事实，指事实表中业务过程的**度量值**（如订单金额，支付金额等），用于后续的统计分析



#### 2.4.3 DWS层

- 根据DWT层的主题需求，进行小范围（如当日）汇总计算



#### 2.4.4 DWT层

- 根据主题对象的需求，构建全量宽表



#### 2.4.5 ADS层

- 对电商系统各个指标进行综合分析



## 第三章 ODS层搭建

### 3.1 创建hive数据库

1. 启动hdfs，yarn，hive

   ```shell
   start-dfs.sh
   start-yarn.sh
   hivejdbc start
   hive
   ```

2. 创建数据库

   ```mysql
   create database gmall;
   ```

3. 使用数据库

   ```mysql
   use gmall;
   ```



### 3.2 用户行为日志数据

#### 3.2.1 创建日志表

1. 创建支持lzo压缩存储的外部分区表

   ```mysql
   drop table if exists ods_log;
   create external table ods_log (`line` string)
   partitioned by (`dt` string)
   stored as
     inputformat 'com.hadoop.mapred.DeprecatedLzoTextInputFormat'
     outputformat 'org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
   location '/warehouse/gmall/ods/ods_log';
   ```

2. 根据分区日期，加载数据到表中

   ```mysql
   load data inpath '/origin_data/gmall/log/topic_log/2020-06-24' into table ods_log partition(dt='2020-06-24');
   ```

3. 为数据创建索引

   ```shell
   hadoop jar /opt/module/hadoop-3.1.3/share/hadoop/common/hadoop-lzo-0.4.20.jar com.hadoop.compression.lzo.DistributedLzoIndexer -Dmapreduce.job.queuename=hive /warehouse/gmall/ods/ods_log/dt=2020-06-24
   ```



#### 3.2.2 Shell中的单双引号使用

- 单引号中的变量不被解析
- 双引号中的变量会被解析
- 嵌套使用时，最外层的单引号或双引号决定是否解析内部变量



#### 3.2.3 ODS层日志数据脚本

1. 创建目标表

2. 编写数据加载脚本

   ```shell
   cd /home/atguigu/bin
   vi hdfs_to_ods_log.sh
   ```

   ```shell
   #!/bin/bash
   
   #配置数据库名
   APP=gmall
   #配置hive命令路径
   hive=/opt/module/hive/bin/hive
   #配置hadoop命令路径
   hadoop=/opt/module/hadoop-3.1.3/bin/hadoop
   
   #未传参时使用前一天日期,传参时使用指定日期,日期格式为2020-01-01
   if [ -n "$1" ] ;then
      do_date=$1
   else 
      do_date=`date -d "-1 day" +%F`
   fi 
   
   echo ================== 日志日期为 $do_date ==================
   sql="
   load data inpath '/origin_data/gmall/log/topic_log/$do_date' into table ${APP}.ods_log partition(dt='$do_date');
   "
   
   $hive -e "$sql"
   
   $hadoop jar /opt/module/hadoop-3.1.3/share/hadoop/common/hadoop-lzo-0.4.20.jar com.hadoop.compression.lzo.DistributedLzoIndexer -Dmapreduce.job.queuename=hive /warehouse/gmall/ods/ods_log/dt=$do_date
   ```

3. 添加执行权限

   ```shell
   chmod 777 hdfs_to_ods_log.sh
   ```

4. 使用脚本

   ```shell
   hdfs_to_ods_log.sh 2020-06-24
   ```



### 3.3 业务数据

#### 3.3.1 创建业务表

```mysql
-- 订单表（增量及更新）
drop table if exists ods_order_info;
create external table ods_order_info (
    `id` string COMMENT '订单号',
    `final_total_amount` decimal(16,2) COMMENT '订单金额',
    `order_status` string COMMENT '订单状态',
    `user_id` string COMMENT '用户id',
    `out_trade_no` string COMMENT '支付流水号',
    `create_time` string COMMENT '创建时间',
    `operate_time` string COMMENT '操作时间',
    `province_id` string COMMENT '省份ID',
    `benefit_reduce_amount` decimal(16,2) COMMENT '优惠金额',
    `original_total_amount` decimal(16,2)  COMMENT '原价金额',
    `feight_fee` decimal(16,2)  COMMENT '运费'
) COMMENT '订单表'
PARTITIONED BY (`dt` string)
row format delimited fields terminated by '\t'
STORED AS
  INPUTFORMAT 'com.hadoop.mapred.DeprecatedLzoTextInputFormat'
  OUTPUTFORMAT 'org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
location '/warehouse/gmall/ods/ods_order_info/';

-- 订单详情表（增量）
drop table if exists ods_order_detail;
create external table ods_order_detail( 
    `id` string COMMENT '订单编号',
    `order_id` string  COMMENT '订单号', 
    `user_id` string COMMENT '用户id',
    `sku_id` string COMMENT '商品id',
    `sku_name` string COMMENT '商品名称',
    `order_price` decimal(16,2) COMMENT '商品价格',
    `sku_num` bigint COMMENT '商品数量',
    `create_time` string COMMENT '创建时间',
    `source_type` string COMMENT '来源类型',
    `source_id` string COMMENT '来源编号'
) COMMENT '订单详情表'
PARTITIONED BY (`dt` string)
row format delimited fields terminated by '\t' 
STORED AS
  INPUTFORMAT 'com.hadoop.mapred.DeprecatedLzoTextInputFormat'
  OUTPUTFORMAT 'org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
location '/warehouse/gmall/ods/ods_order_detail/';

-- SKU商品表（全量）
drop table if exists ods_sku_info;
create external table ods_sku_info( 
    `id` string COMMENT 'skuId',
    `spu_id` string   COMMENT 'spuid', 
    `price` decimal(16,2) COMMENT '价格',
    `sku_name` string COMMENT '商品名称',
    `sku_desc` string COMMENT '商品描述',
    `weight` string COMMENT '重量',
    `tm_id` string COMMENT '品牌id',
    `category3_id` string COMMENT '品类id',
    `create_time` string COMMENT '创建时间'
) COMMENT 'SKU商品表'
PARTITIONED BY (`dt` string)
row format delimited fields terminated by '\t'
STORED AS
  INPUTFORMAT 'com.hadoop.mapred.DeprecatedLzoTextInputFormat'
  OUTPUTFORMAT 'org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
location '/warehouse/gmall/ods/ods_sku_info/';

-- 用户表（增量及更新）
drop table if exists ods_user_info;
create external table ods_user_info( 
    `id` string COMMENT '用户id',
    `name`  string COMMENT '姓名',
    `birthday` string COMMENT '生日',
    `gender` string COMMENT '性别',
    `email` string COMMENT '邮箱',
    `user_level` string COMMENT '用户等级',
    `create_time` string COMMENT '创建时间',
    `operate_time` string COMMENT '操作时间'
) COMMENT '用户表'
PARTITIONED BY (`dt` string)
row format delimited fields terminated by '\t'
STORED AS
  INPUTFORMAT 'com.hadoop.mapred.DeprecatedLzoTextInputFormat'
  OUTPUTFORMAT 'org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
location '/warehouse/gmall/ods/ods_user_info/';

-- 商品一级分类表（全量）
drop table if exists ods_base_category1;
create external table ods_base_category1( 
    `id` string COMMENT 'id',
    `name`  string COMMENT '名称'
) COMMENT '商品一级分类表'
PARTITIONED BY (`dt` string)
row format delimited fields terminated by '\t'
STORED AS
  INPUTFORMAT 'com.hadoop.mapred.DeprecatedLzoTextInputFormat'
  OUTPUTFORMAT 'org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
location '/warehouse/gmall/ods/ods_base_category1/';

-- 商品二级分类表（全量）
drop table if exists ods_base_category2;
create external table ods_base_category2( 
    `id` string COMMENT ' id',
    `name` string COMMENT '名称',
    category1_id string COMMENT '一级品类id'
) COMMENT '商品二级分类表'
PARTITIONED BY (`dt` string)
row format delimited fields terminated by '\t'
STORED AS
  INPUTFORMAT 'com.hadoop.mapred.DeprecatedLzoTextInputFormat'
  OUTPUTFORMAT 'org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
location '/warehouse/gmall/ods/ods_base_category2/';

-- 商品三级分类表（全量）
drop table if exists ods_base_category3;
create external table ods_base_category3(
    `id` string COMMENT ' id',
    `name`  string COMMENT '名称',
    category2_id string COMMENT '二级品类id'
) COMMENT '商品三级分类表'
PARTITIONED BY (`dt` string)
row format delimited fields terminated by '\t'
STORED AS
  INPUTFORMAT 'com.hadoop.mapred.DeprecatedLzoTextInputFormat'
  OUTPUTFORMAT 'org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
location '/warehouse/gmall/ods/ods_base_category3/';

-- 支付流水表（增量）
drop table if exists ods_payment_info;
create external table ods_payment_info(
    `id`   bigint COMMENT '编号',
    `out_trade_no`    string COMMENT '对外业务编号',
    `order_id`        string COMMENT '订单编号',
    `user_id`         string COMMENT '用户编号',
    `alipay_trade_no` string COMMENT '支付宝交易流水编号',
    `total_amount`    decimal(16,2) COMMENT '支付金额',
    `subject`         string COMMENT '交易内容',
    `payment_type`    string COMMENT '支付类型',
    `payment_time`    string COMMENT '支付时间'
)  COMMENT '支付流水表'
PARTITIONED BY (`dt` string)
row format delimited fields terminated by '\t'
STORED AS
  INPUTFORMAT 'com.hadoop.mapred.DeprecatedLzoTextInputFormat'
  OUTPUTFORMAT 'org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
location '/warehouse/gmall/ods/ods_payment_info/';

-- 省份表（特殊）
drop table if exists ods_base_province;
create external table ods_base_province (
    `id`   bigint COMMENT '编号',
    `name`        string COMMENT '省份名称',
    `region_id`    string COMMENT '地区ID',
    `area_code`    string COMMENT '地区编码',
    `iso_code` string COMMENT 'iso编码,superset可视化使用'
)  COMMENT '省份表'
row format delimited fields terminated by '\t'
STORED AS
  INPUTFORMAT 'com.hadoop.mapred.DeprecatedLzoTextInputFormat'
  OUTPUTFORMAT 'org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
location '/warehouse/gmall/ods/ods_base_province/';

-- 地区表（特殊）
drop table if exists ods_base_region;
create external table ods_base_region (
    `id` string COMMENT '编号',
    `region_name` string COMMENT '地区名称'
)  COMMENT '地区表'
row format delimited fields terminated by '\t'
STORED AS
  INPUTFORMAT 'com.hadoop.mapred.DeprecatedLzoTextInputFormat'
  OUTPUTFORMAT 'org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
location '/warehouse/gmall/ods/ods_base_region/';

-- 品牌表（全量）
drop table if exists ods_base_trademark;
create external table ods_base_trademark (
    `tm_id`   string COMMENT '编号',
    `tm_name` string COMMENT '品牌名称'
)  COMMENT '品牌表'
PARTITIONED BY (`dt` string)
row format delimited fields terminated by '\t'
STORED AS
  INPUTFORMAT 'com.hadoop.mapred.DeprecatedLzoTextInputFormat'
  OUTPUTFORMAT 'org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
location '/warehouse/gmall/ods/ods_base_trademark/'；

-- 订单状态表（增量）
drop table if exists ods_order_status_log;
create external table ods_order_status_log (
    `id`   string COMMENT '编号',
    `order_id` string COMMENT '订单ID',
    `order_status` string COMMENT '订单状态',
    `operate_time` string COMMENT '修改时间'
)  COMMENT '订单状态表'
PARTITIONED BY (`dt` string)
row format delimited fields terminated by '\t'
STORED AS
  INPUTFORMAT 'com.hadoop.mapred.DeprecatedLzoTextInputFormat'
  OUTPUTFORMAT 'org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
location '/warehouse/gmall/ods/ods_order_status_log/';

-- SPU商品表（全量）
drop table if exists ods_spu_info;
create external table ods_spu_info(
    `id` string COMMENT 'spuid',
    `spu_name` string COMMENT 'spu名称',
    `category3_id` string COMMENT '品类id',
    `tm_id` string COMMENT '品牌id'
) COMMENT 'SPU商品表'
PARTITIONED BY (`dt` string)
row format delimited fields terminated by '\t'
STORED AS
  INPUTFORMAT 'com.hadoop.mapred.DeprecatedLzoTextInputFormat'
  OUTPUTFORMAT 'org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
location '/warehouse/gmall/ods/ods_spu_info/';

-- 商品评论表（增量）
drop table if exists ods_comment_info;
create external table ods_comment_info(
    `id` string COMMENT '编号',
    `user_id` string COMMENT '用户ID',
    `sku_id` string COMMENT '商品sku',
    `spu_id` string COMMENT '商品spu',
    `order_id` string COMMENT '订单ID',
    `appraise` string COMMENT '评价',
    `create_time` string COMMENT '评价时间'
) COMMENT '商品评论表'
PARTITIONED BY (`dt` string)
row format delimited fields terminated by '\t'
STORED AS
  INPUTFORMAT 'com.hadoop.mapred.DeprecatedLzoTextInputFormat'
  OUTPUTFORMAT 'org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
location '/warehouse/gmall/ods/ods_comment_info/';

-- 退单表（增量）
drop table if exists ods_order_refund_info;
create external table ods_order_refund_info(
    `id` string COMMENT '编号',
    `user_id` string COMMENT '用户ID',
    `order_id` string COMMENT '订单ID',
    `sku_id` string COMMENT '商品ID',
    `refund_type` string COMMENT '退款类型',
    `refund_num` bigint COMMENT '退款件数',
    `refund_amount` decimal(16,2) COMMENT '退款金额',
    `refund_reason_type` string COMMENT '退款原因类型',
    `create_time` string COMMENT '退款时间'
) COMMENT '退单表'
PARTITIONED BY (`dt` string)
row format delimited fields terminated by '\t'
STORED AS
  INPUTFORMAT 'com.hadoop.mapred.DeprecatedLzoTextInputFormat'
  OUTPUTFORMAT 'org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
location '/warehouse/gmall/ods/ods_order_refund_info/';

-- 加购表（全量）
drop table if exists ods_cart_info;
create external table ods_cart_info(
    `id` string COMMENT '编号',
    `user_id` string  COMMENT '用户id',
    `sku_id` string  COMMENT 'skuid',
    `cart_price` decimal(16,2)  COMMENT '放入购物车时价格',
    `sku_num` bigint  COMMENT '数量',
    `sku_name` string  COMMENT 'sku名称 (冗余)',
    `create_time` string  COMMENT '创建时间',
    `operate_time` string COMMENT '修改时间',
    `is_ordered` string COMMENT '是否已经下单',
    `order_time` string  COMMENT '下单时间',
    `source_type` string COMMENT '来源类型',
    `source_id` string COMMENT '来源编号'
) COMMENT '加购表'
PARTITIONED BY (`dt` string)
row format delimited fields terminated by '\t'
STORED AS
  INPUTFORMAT 'com.hadoop.mapred.DeprecatedLzoTextInputFormat'
  OUTPUTFORMAT 'org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
location '/warehouse/gmall/ods/ods_cart_info/';

-- 商品收藏表（全量）
drop table if exists ods_favor_info;
create external table ods_favor_info(
    `id` string COMMENT '编号',
    `user_id` string  COMMENT '用户id',
    `sku_id` string  COMMENT 'skuid',
    `spu_id` string  COMMENT 'spuid',
    `is_cancel` string  COMMENT '是否取消',
    `create_time` string  COMMENT '收藏时间',
    `cancel_time` string  COMMENT '取消时间'
) COMMENT '商品收藏表'
PARTITIONED BY (`dt` string)
row format delimited fields terminated by '\t'
STORED AS
  INPUTFORMAT 'com.hadoop.mapred.DeprecatedLzoTextInputFormat'
  OUTPUTFORMAT 'org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
location '/warehouse/gmall/ods/ods_favor_info/';

-- 优惠券领用表（新增及变化）
drop table if exists ods_coupon_use;
create external table ods_coupon_use(
    `id` string COMMENT '编号',
    `coupon_id` string  COMMENT '优惠券ID',
    `user_id` string  COMMENT '用户id',
    `order_id` string  COMMENT '订单id',
    `coupon_status` string  COMMENT '优惠券状态',
    `get_time` string  COMMENT '领取时间',
    `using_time` string  COMMENT '使用时间(下单)',
    `used_time` string  COMMENT '使用时间(支付)'
) COMMENT '优惠券领用表'
PARTITIONED BY (`dt` string)
row format delimited fields terminated by '\t'
STORED AS
  INPUTFORMAT 'com.hadoop.mapred.DeprecatedLzoTextInputFormat'
  OUTPUTFORMAT 'org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
location '/warehouse/gmall/ods/ods_coupon_use/';

-- 优惠券表（全量）
drop table if exists ods_coupon_info;
create external table ods_coupon_info(
    `id` string COMMENT '购物券编号',
    `coupon_name` string COMMENT '购物券名称',
    `coupon_type` string COMMENT '购物券类型 1 现金券 2 折扣券 3 满减券 4 满件打折券',
    `condition_amount` decimal(16,2) COMMENT '满额数',
    `condition_num` bigint COMMENT '满件数',
    `activity_id` string COMMENT '活动编号',
    `benefit_amount` decimal(16,2) COMMENT '减金额',
    `benefit_discount` decimal(16,2) COMMENT '折扣',
    `create_time` string COMMENT '创建时间',
    `range_type` string COMMENT '范围类型 1、商品 2、品类 3、品牌',
    `spu_id` string COMMENT '商品id',
    `tm_id` string COMMENT '品牌id',
    `category3_id` string COMMENT '品类id',
    `limit_num` bigint COMMENT '最多领用次数',
    `operate_time`  string COMMENT '修改时间',
    `expire_time`  string COMMENT '过期时间'
) COMMENT '优惠券表'
PARTITIONED BY (`dt` string)
row format delimited fields terminated by '\t'
STORED AS
  INPUTFORMAT 'com.hadoop.mapred.DeprecatedLzoTextInputFormat'
  OUTPUTFORMAT 'org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
location '/warehouse/gmall/ods/ods_coupon_info/';

-- 活动表（全量）
drop table if exists ods_activity_info;
create external table ods_activity_info(
    `id` string COMMENT '编号',
    `activity_name` string  COMMENT '活动名称',
    `activity_type` string  COMMENT '活动类型',
    `start_time` string  COMMENT '开始时间',
    `end_time` string  COMMENT '结束时间',
    `create_time` string  COMMENT '创建时间'
) COMMENT '活动表'
PARTITIONED BY (`dt` string)
row format delimited fields terminated by '\t'
STORED AS
  INPUTFORMAT 'com.hadoop.mapred.DeprecatedLzoTextInputFormat'
  OUTPUTFORMAT 'org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
location '/warehouse/gmall/ods/ods_activity_info/';

-- 活动订单关联表（增量）
drop table if exists ods_activity_order;
create external table ods_activity_order(
    `id` string COMMENT '编号',
    `activity_id` string  COMMENT '优惠券ID',
    `order_id` string  COMMENT '订单id',
    `create_time` string  COMMENT '领取时间'
) COMMENT '活动订单关联表'
PARTITIONED BY (`dt` string)
row format delimited fields terminated by '\t'
STORED AS
  INPUTFORMAT 'com.hadoop.mapred.DeprecatedLzoTextInputFormat'
  OUTPUTFORMAT 'org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
location '/warehouse/gmall/ods/ods_activity_order/';

-- 优惠规则表（全量）
drop table if exists ods_activity_rule;
create external table ods_activity_rule(
    `id` string COMMENT '编号',
    `activity_id` string  COMMENT '活动ID',
    `condition_amount` decimal(16,2) COMMENT '满减金额',
    `condition_num` bigint COMMENT '满减件数',
    `benefit_amount` decimal(16,2) COMMENT '优惠金额',
    `benefit_discount` decimal(16,2) COMMENT '优惠折扣',
    `benefit_level` string  COMMENT '优惠级别'
) COMMENT '优惠规则表'
PARTITIONED BY (`dt` string)
row format delimited fields terminated by '\t'
STORED AS
  INPUTFORMAT 'com.hadoop.mapred.DeprecatedLzoTextInputFormat'
  OUTPUTFORMAT 'org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
location '/warehouse/gmall/ods/ods_activity_rule/';

-- 编码字典表（全量）
drop table if exists ods_base_dic;
create external table ods_base_dic(
    `dic_code` string COMMENT '编号',
    `dic_name` string  COMMENT '编码名称',
    `parent_code` string  COMMENT '父编码',
    `create_time` string  COMMENT '创建日期',
    `operate_time` string  COMMENT '操作日期'
) COMMENT '编码字典表'
PARTITIONED BY (`dt` string)
row format delimited fields terminated by '\t'
STORED AS
  INPUTFORMAT 'com.hadoop.mapred.DeprecatedLzoTextInputFormat'
  OUTPUTFORMAT 'org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
location '/warehouse/gmall/ods/ods_base_dic/';
```



#### 3.3.2 ODS层业务数据脚本

1. 创建目标表

2. 编写数据加载脚本

   ```shell
   cd /home/atguigu/bin
   vi hdfs_to_ods_db.sh
   ```

   ```shell
   #!/bin/bash
   
   APP=gmall
   hive=/opt/module/hive/bin/hive
   
   # 如果是输入的日期按照取输入日期；如果没输入日期取当前时间的前一天
   if [ -n "$2" ] ;then
       do_date=$2
   else 
       do_date=`date -d "-1 day" +%F`
   fi
   
   sql1=" 
   load data inpath '/origin_data/$APP/db/order_info/$do_date' OVERWRITE into table ${APP}.ods_order_info partition(dt='$do_date');
   
   load data inpath '/origin_data/$APP/db/order_detail/$do_date' OVERWRITE into table ${APP}.ods_order_detail partition(dt='$do_date');
   
   load data inpath '/origin_data/$APP/db/sku_info/$do_date' OVERWRITE into table ${APP}.ods_sku_info partition(dt='$do_date');
   
   load data inpath '/origin_data/$APP/db/user_info/$do_date' OVERWRITE into table ${APP}.ods_user_info partition(dt='$do_date');
   
   load data inpath '/origin_data/$APP/db/payment_info/$do_date' OVERWRITE into table ${APP}.ods_payment_info partition(dt='$do_date');
   
   load data inpath '/origin_data/$APP/db/base_category1/$do_date' OVERWRITE into table ${APP}.ods_base_category1 partition(dt='$do_date');
   
   load data inpath '/origin_data/$APP/db/base_category2/$do_date' OVERWRITE into table ${APP}.ods_base_category2 partition(dt='$do_date');
   
   load data inpath '/origin_data/$APP/db/base_category3/$do_date' OVERWRITE into table ${APP}.ods_base_category3 partition(dt='$do_date'); 
   
   load data inpath '/origin_data/$APP/db/base_trademark/$do_date' OVERWRITE into table ${APP}.ods_base_trademark partition(dt='$do_date'); 
   
   load data inpath '/origin_data/$APP/db/activity_info/$do_date' OVERWRITE into table ${APP}.ods_activity_info partition(dt='$do_date'); 
   
   load data inpath '/origin_data/$APP/db/activity_order/$do_date' OVERWRITE into table ${APP}.ods_activity_order partition(dt='$do_date'); 
   
   load data inpath '/origin_data/$APP/db/cart_info/$do_date' OVERWRITE into table ${APP}.ods_cart_info partition(dt='$do_date'); 
   
   load data inpath '/origin_data/$APP/db/comment_info/$do_date' OVERWRITE into table ${APP}.ods_comment_info partition(dt='$do_date'); 
   
   load data inpath '/origin_data/$APP/db/coupon_info/$do_date' OVERWRITE into table ${APP}.ods_coupon_info partition(dt='$do_date'); 
   
   load data inpath '/origin_data/$APP/db/coupon_use/$do_date' OVERWRITE into table ${APP}.ods_coupon_use partition(dt='$do_date'); 
   
   load data inpath '/origin_data/$APP/db/favor_info/$do_date' OVERWRITE into table ${APP}.ods_favor_info partition(dt='$do_date'); 
   
   load data inpath '/origin_data/$APP/db/order_refund_info/$do_date' OVERWRITE into table ${APP}.ods_order_refund_info partition(dt='$do_date'); 
   
   load data inpath '/origin_data/$APP/db/order_status_log/$do_date' OVERWRITE into table ${APP}.ods_order_status_log partition(dt='$do_date'); 
   
   load data inpath '/origin_data/$APP/db/spu_info/$do_date' OVERWRITE into table ${APP}.ods_spu_info partition(dt='$do_date'); 
   
   load data inpath '/origin_data/$APP/db/activity_rule/$do_date' OVERWRITE into table ${APP}.ods_activity_rule partition(dt='$do_date'); 
   
   load data inpath '/origin_data/$APP/db/base_dic/$do_date' OVERWRITE into table ${APP}.ods_base_dic partition(dt='$do_date'); 
   "
   
   #省份表与地区表单独导入,且只导入一次
   sql2=" 
   load data inpath '/origin_data/$APP/db/base_province/$do_date' OVERWRITE into table ${APP}.ods_base_province;
   
   load data inpath '/origin_data/$APP/db/base_region/$do_date' OVERWRITE into table ${APP}.ods_base_region;
   "
   
   #仅在首次导入数据时导入省份与地区表
   case $1 in
   "first"){
       $hive -e "$sql1$sql2"
   };;
   "all"){
       $hive -e "$sql1"
   };;
   esac
   ```

3. 添加执行权限

   ```shell
   chmod 777 hdfs_to_ods_db.sh
   ```

4. 使用脚本导入业务表数据

   ```shell
   hdfs_to_ods_db.sh first 2020-06-24
   ```



## 第四章 DWD层搭建

### 4.1 用户行为日志解析

- 在ODS层的ods_log表中，line字段的每一行数据为一整条json格式的日志，而日志包含以下两种

  1. 页面埋点日志：记录用户在某一页面的所有行为，具体包含

     ```json
     {
         "common": {
             "ar": "230000",              -- 地区编码
             "ba": "iPhone",              -- 手机品牌
             "ch": "Appstore",            -- 渠道
             "md": "iPhone 8",            -- 手机型号
             "mid": "YXfhjAYH6As2z9Iq",   -- 设备id
             "os": "iOS 13.2.9",          -- 操作系统
             "uid": "485",                -- 会员id
             "vc": "v2.1.134"             -- app版本号
         },
         "actions": [
             {
                 "action_id": "favor_add",   --动作id
                 "item": "3",                --目标id
                 "item_type": "sku_id",      --目标类型
                 "ts": 1585744376605         --动作时间戳
             }
         ]，
         "displays": [
         	{
         		"displayType": "query",       --曝光类型
         		"item": "3",                  --曝光对象id
         		"item_type": "sku_id",        --曝光对象类型
         		"order": 1                    --出现顺序
     		}
     	],
     	"page": {
         	"during_time": 7648,        --持续时间毫秒
         	"item": "3",                --目标id
         	"item_type": "sku_id",      --目标类型
         	"last_page_id": "login",    --上页类型
         	"page_id": "good_detail",   --页面ID
         	"sourceType": "promotion"   --来源类型
     	},
     	"err":{
         	"error_code": "1234",      	 --错误码
         	"msg": "***********"       	 --错误信息
     	},
     	"ts": 1585744374423  			--跳入时间戳
     }
     ```

  2. 启动日志：记录用户启动APP的所有相关信息，具体包含

     ```json
     {
         "common": {
             "ar": "370000",
             "ba": "Honor",
             "ch": "wandoujia",
             "md": "Honor 20s",
             "mid": "eQF5boERMJFOujcp",
             "os": "Android 11.0",
             "uid": "76",
             "vc": "v2.1.134"
         },
         "start": {   
             "entry": "icon",        --启动方式
             "loading_time": 18803,  --启动加载时间
             "open_ad_id": 7,        --广告页ID
             "open_ad_ms": 3449,     --广告总共播放时间
             "open_ad_skip_ms": 1989 --用户跳过广告时点
         },
         "err":{      
                "error_code": "1234",    --错误码
                "msg": "***********"     --错误信息
               },
         "ts": 1585744304000
     }
     ```



#### 4.1.1 json函数

- get_json_object函数可用于解析json字符串，提取目标数据

  ```mysql
  get_json_object(param1,param2)
  #参数1 JSON字符串
  #参数2 提取的字段
  ```

- 示例

  ```mysql
  select get_json_object('[{"name":"大郎","sex":"男","age":"25"},{"name":"西门庆","sex":"男","age":"47"}]','$[0]');
  # 返回 {"name":"大郎","sex":"男","age":"25"}
  
  select get_json_object('[{"name":"大郎","sex":"男","age":"25"},{"name":"西门庆","sex":"男","age":"47"}]','$[0].age');
  #返回 25
  ```



#### 4.1.2 启动日志表

- 启动日志表需要读取**启动日志**数据，提取其中的**公共字段、启动信息、启动时间**数据

- 建表语句

  ```mysql
  use gmall;
  drop table if exists dwd_start_log;
  CREATE EXTERNAL TABLE dwd_start_log(
      `area_code` string,
      `brand` string, 
      `channel` string, 
      `model` string, 
      `mid_id` string, 
      `os` string, 
      `user_id` string, 
      `version_code` string, 
      `entry` string,
      `loading_time` bigint,
      `open_ad_id` string,
      `open_ad_ms` bigint, 
      `open_ad_skip_ms` bigint, 
      `ts` bigint
  )
  PARTITIONED BY (dt string)
  stored as parquet
  LOCATION '/warehouse/gmall/dwd/dwd_start_log'
  TBLPROPERTIES('parquet.compression'='lzo'); 
  ```

- 导入数据

  ```mysql
  #设置文件输入流为HiveInputFormat,避免解析lzo的index文件
  set hive.input.format=org.apache.hadoop.hive.ql.io.HiveInputFormat;
  insert overwrite table dwd_start_log partition(dt='2020-06-24')
  select 
      get_json_object(line,'$.common.ar'),
      get_json_object(line,'$.common.ba'),
      get_json_object(line,'$.common.ch'),
      get_json_object(line,'$.common.md'),
      get_json_object(line,'$.common.mid'),
      get_json_object(line,'$.common.os'),
      get_json_object(line,'$.common.uid'),
      get_json_object(line,'$.common.vc'),
      get_json_object(line,'$.start.entry'),
      get_json_object(line,'$.start.loading_time'),
      get_json_object(line,'$.start.open_ad_id'),
      get_json_object(line,'$.start.open_ad_ms'),
      get_json_object(line,'$.start.open_ad_skip_ms'),
      get_json_object(line,'$.ts')
  from ods_log
  where dt='2020-06-24'
  and get_json_object(line,'$.start') is not null;
  ```



#### 4.1.3 页面日志表

- 页面日志表需要读取**页面埋点日志**，并提取其中的**公共字段、页面信息、时间戳**数据

- 建表语句

  ```mysql
  use gmall;
  drop table if exists dwd_page_log;
  CREATE EXTERNAL TABLE dwd_page_log(
      `area_code` string,
      `brand` string, 
      `channel` string, 
      `model` string, 
      `mid_id` string, 
      `os` string, 
      `user_id` string, 
      `version_code` string, 
      `during_time` bigint,
      `page_item` string, 
      `page_item_type` string, 
      `last_page_id` string, 
      `page_id` string,
      `source_type` string, 
      `ts` bigint
  )
  PARTITIONED BY (dt string)
  stored as parquet
  LOCATION '/warehouse/gmall/dwd/dwd_page_log'
  TBLPROPERTIES('parquet.compression'='lzo');
  ```

- 导入数据

  ```mysql
  set hive.input.format=org.apache.hadoop.hive.ql.io.HiveInputFormat;
  insert overwrite table dwd_page_log partition(dt='2020-06-24')
  select
      get_json_object(line,'$.common.ar'),
      get_json_object(line,'$.common.ba'),
      get_json_object(line,'$.common.ch'),
      get_json_object(line,'$.common.md'),
      get_json_object(line,'$.common.mid'),
      get_json_object(line,'$.common.os'),
      get_json_object(line,'$.common.uid'),
      get_json_object(line,'$.common.vc'),
      get_json_object(line,'$.page.during_time'),
      get_json_object(line,'$.page.item'),
      get_json_object(line,'$.page.item_type'),
      get_json_object(line,'$.page.last_page_id'),
      get_json_object(line,'$.page.page_id'),
      get_json_object(line,'$.page.sourceType'),
      get_json_object(line,'$.ts')
  from ods_log
  where dt='2020-06-24'
  and get_json_object(line,'$.page') is not null;
  ```



#### 4.1.4 动作日志表

- 动作日志表需要读取**页面埋点日志**，并提取其中的**公共字段、页面信息、动作数组**数据，由于动作数据使用不定长的json数组存储，因此需要额外定义UDTF函数将数组炸开处理

- 建表语句

  ```mysql
  use gmall;
  drop table if exists dwd_action_log;
  CREATE EXTERNAL TABLE dwd_action_log(
      `area_code` string,
      `brand` string, 
      `channel` string, 
      `model` string, 
      `mid_id` string, 
      `os` string, 
      `user_id` string, 
      `version_code` string, 
      `during_time` bigint, 
      `page_item` string, 
      `page_item_type` string, 
      `last_page_id` string, 
      `page_id` string,
      `source_type` string, 
      `action_id` string,
      `item` string,
      `item_type` string, 
      `ts` bigint
  )
  PARTITIONED BY (dt string)
  stored as parquet
  LOCATION '/warehouse/gmall/dwd/dwd_action_log'
  TBLPROPERTIES('parquet.compression'='lzo');
  ```

- 自定义UDTF函数

  ```java
  //UDTF函数说明
  //参数 json数组字符串
  //返回值 json数组拆分后的多个独立的json对象字符串
  public class ExplodeJsonArray extends GenericUDTF {
  
      @Override
      public StructObjectInspector initialize(StructObjectInspector argOIs) throws UDFArgumentException {
  
          //1.校验参数
          List<? extends StructField> allStructFieldRefs = argOIs.getAllStructFieldRefs();
          if (allStructFieldRefs.size() != 1) {
              throw new UDFArgumentException("explode_json_array函数参数个数为1");
          }
          String typeName = allStructFieldRefs.get(0).getFieldObjectInspector().getTypeName();
          if (!"string".equals(typeName)){
              throw new UDFArgumentException("参数类型不匹配,需要传入string型参数");
          }
  
          //2.规定函数的返回字段的字段名和类型
          ArrayList<String> fieldNames = new ArrayList<>();
          ArrayList<ObjectInspector> fieldOIs = new ArrayList<>();
          fieldNames.add("display");
          fieldOIs.add(PrimitiveObjectInspectorFactory.javaStringObjectInspector);
          //fieldNames中的每一个元素对应函数返回的每个字段的列名
          //fieldOIs中的每一个元素对应函数返回的每个字段的类型检查器对象
          return ObjectInspectorFactory.getStandardStructObjectInspector(fieldNames, fieldOIs);
      }
  
      @Override
      public void process(Object[] args) throws HiveException {
          //1.自定义函数逻辑,对应表中1行参数的处理逻辑
          String jsonStr = args[0].toString();
          JSONArray jsonArray = new JSONArray(jsonStr);
          //2.循环写出返回字段值
          for (int i = 0; i < jsonArray.length(); i++){
              String[] result = new String[1];
              result[0] = jsonArray.getString(0);
              //forward方法写入的参数即为函数最终返回的一行结果,对应initialize方法中的返回字段类型
              //forward方法每执行一次,返回一行执行结果,result集合中的每个元素的类型与fieldOIs中的类型检查器一一对应
              forward(result);
          }
      }
  
      @Override
      public void close() throws HiveException {
      }
  }
  ```

- 导入数据

  ```mysql
  #将自定义函数打包后上传到hdfs,并创建永久函数
  create function explode_json_array 
  as 'com.atguigu.hive.udtf.ExplodeJsonArray' 
  using jar 'hdfs://hadoop201:8020/user/hive/jars/hive-1.0-SNAPSHOT.jar';
  #若需要更新函数功能仅需要替换hdfs中的同名jar包并重启hive即可
  
  set hive.input.format=org.apache.hadoop.hive.ql.io.HiveInputFormat;
  insert overwrite table dwd_action_log partition(dt='2020-06-24')
  select
      get_json_object(line,'$.common.ar'),
      get_json_object(line,'$.common.ba'),
      get_json_object(line,'$.common.ch'),
      get_json_object(line,'$.common.md'),
      get_json_object(line,'$.common.mid'),
      get_json_object(line,'$.common.os'),
      get_json_object(line,'$.common.uid'),
      get_json_object(line,'$.common.vc'),
      get_json_object(line,'$.page.during_time'),
      get_json_object(line,'$.page.item'),
      get_json_object(line,'$.page.item_type'),
      get_json_object(line,'$.page.last_page_id'),
      get_json_object(line,'$.page.page_id'),
      get_json_object(line,'$.page.sourceType'),
      get_json_object(action,'$.action_id'),
      get_json_object(action,'$.item'),
      get_json_object(action,'$.item_type'),
      get_json_object(action,'$.ts')
  from ods_log lateral view explode_json_array(get_json_object(line,'$.actions')) tmp as action
  where dt='2020-06-24'
  and get_json_object(line,'$.actions') is not null;
  ```



#### 4.1.5 曝光日志表

- 曝光日志表需要读取**页面埋点日志**，并提取其中的**公共字段、页面信息、曝光数组**数据，由于曝光数据使用不定长的json数组存储，因此需要额外定义UDTF函数将数组炸开处理

- 建表语句

  ```mysql
  use gmall;
  drop table if exists dwd_display_log;
  CREATE EXTERNAL TABLE dwd_display_log(
      `area_code` string,
      `brand` string, 
      `channel` string, 
      `model` string, 
      `mid_id` string, 
      `os` string, 
      `user_id` string, 
      `version_code` string, 
      `during_time` bigint,
      `page_item` string, 
      `page_item_type` string, 
      `last_page_id` string, 
      `page_id` string,
      `source_type` string, 
      `ts` bigint,
      `display_type` string,
      `item` string,
      `item_type` string, 
      `order` bigint
  )
  PARTITIONED BY (dt string)
  stored as parquet
  LOCATION '/warehouse/gmall/dwd/dwd_display_log'
  TBLPROPERTIES('parquet.compression'='lzo');
  ```

- 使用自定义的udtf函数导入数据

  ```mysql
  insert overwrite table dwd_display_log partition(dt='2020-06-24')
  select
      get_json_object(line,'$.common.ar'),
      get_json_object(line,'$.common.ba'),
      get_json_object(line,'$.common.ch'),
      get_json_object(line,'$.common.md'),
      get_json_object(line,'$.common.mid'),
      get_json_object(line,'$.common.os'),
      get_json_object(line,'$.common.uid'),
      get_json_object(line,'$.common.vc'),
      get_json_object(line,'$.page.during_time'),
      get_json_object(line,'$.page.item'),
      get_json_object(line,'$.page.item_type'),
      get_json_object(line,'$.page.last_page_id'),
      get_json_object(line,'$.page.page_id'),
      get_json_object(line,'$.page.sourceType'),
      get_json_object(line,'$.ts'),
      get_json_object(displays,'$.displayType'),
      get_json_object(displays,'$.item'),
      get_json_object(displays,'$.item_type'),
      get_json_object(displays,'$.order')
  from ods_log lateral view explode_json_array(get_json_object(line,'$.displays')) tmp as displays
  where dt='2020-06-24'
  and get_json_object(line,'$.displays') is not null;
  ```



#### 4.1.6 错误日志表

- 错误日志表需要同时读取页面埋点日志和启动日志的数据，并提取公共字段、页面信息、启动信息、时间戳、错误字段

- 建表语句

  ```mysql
  use gmall;
  drop table if exists dwd_error_log;
  CREATE EXTERNAL TABLE dwd_error_log(
      `area_code` string,
      `brand` string, 
      `channel` string, 
      `model` string, 
      `mid_id` string, 
      `os` string, 
      `user_id` string, 
      `version_code` string, 
      `page_item` string, 
      `page_item_type` string, 
      `last_page_id` string, 
      `page_id` string,
      `source_type` string, 
      `entry` string,
      `loading_time` string,
      `open_ad_id` string,
      `open_ad_ms` string, 
      `open_ad_skip_ms` string,
      `actions` array<struct<action_id:string,item:string,item_type:string,ts:bigint>>,
      `displays` array<struct<display_type:string,item:string,item_type:string,`order`:int>>,
      `ts` string,
      `error_code` string,
      `msg` string
  )
  PARTITIONED BY (dt string)
  stored as parquet
  LOCATION '/warehouse/gmall/dwd/dwd_error_log'
  TBLPROPERTIES('parquet.compression'='lzo');
  ```

- 编写自定义UDF函数

  ```java
  //函数说明
  //功能 将字符串型的json数组转化为array<struct<...>>型的字段
  //参数1 字符串型json数组
  //参数2 需要读取的json数组中json对象的key值
  //参数3 struct中的所有字段名:字段类型
  //返回值 array<struct<...>>型的字段
  public class JsonArrayToStructArray extends GenericUDF {
  
      @Override
      public ObjectInspector initialize(ObjectInspector[] arguments) throws UDFArgumentException {
          //1.校验参数长度
          if (arguments.length != 3){
              throw new UDFArgumentException("参数数量错误,请传入3个参数");
          }
          //2.校验参数类型
          for (int i = 0; i < arguments.length; i++) {
              if (!"string".equals(arguments[i].getTypeName())){
                  throw new UDFArgumentException("参数类型错误,请传入string型参数");
              }
          }
          //3.校验json字段数与返回列数是否一致
          String param1 = 
              ((ConstantObjectInspector) arguments[1]).getWritableConstantValue().toString();
          String param2 = 
              ((ConstantObjectInspector) arguments[2]).getWritableConstantValue().toString();
          String[] jsonKeys = param1.split(" ");
          String[] columns = param2.split(" ");
          if (jsonKeys.length != columns.length){
              throw new UDFArgumentException("返回字段数与类型数不一致");
          }
  
          //4.规定函数返回值类型
          ArrayList<String> fieldNames = new ArrayList<>();
          ArrayList<ObjectInspector> fieldOIs = new ArrayList<>();
          for (int i = 0; i < columns.length; i++) {
              String[] kv = columns[i].split(":");
              fieldNames.add(kv[0]);
              switch (kv[1]){
                  case "string":
                      fieldOIs.add(PrimitiveObjectInspectorFactory.javaStringObjectInspector);
                      break;
                  case "boolean":
                      fieldOIs.add(PrimitiveObjectInspectorFactory.javaBooleanObjectInspector);
                      break;
                  case "tinyint":
                      fieldOIs.add(PrimitiveObjectInspectorFactory.javaByteObjectInspector);
                      break;
                  case "smallint":
                      fieldOIs.add(PrimitiveObjectInspectorFactory.javaShortObjectInspector);
                      break;
                  case "int":
                      fieldOIs.add(PrimitiveObjectInspectorFactory.javaIntObjectInspector);
                      break;
                  case "bigint":
                      fieldOIs.add(PrimitiveObjectInspectorFactory.javaLongObjectInspector);
                      break;
                  case "float":
                      fieldOIs.add(PrimitiveObjectInspectorFactory.javaFloatObjectInspector);
                      break;
                  case "double":
                      fieldOIs.add(PrimitiveObjectInspectorFactory.javaDoubleObjectInspector);
                      break;
                  default:
                      throw new UDFArgumentException("json_array_to_struct_array函数不支持" + kv[1] + "类型");
              }
          }
          return ObjectInspectorFactory.getStandardListObjectInspector(
                  ObjectInspectorFactory.getStandardStructObjectInspector(fieldNames,fieldOIs)
          );
  
      }
  
      @Override
      public Object evaluate(DeferredObject[] arguments) throws HiveException {
          //1.校验参数1是否为空,若为空则不作处理,不返回结果
          if (arguments[0].get() == null) {
              return null;
          }
          //2.获取json数组对象及需要读取的json对象的key值
          String jsonStr = arguments[0].get().toString();
          JSONArray jsonArray = new JSONArray(jsonStr);
          String[] jsonKeys = arguments[1].get().toString().split(" ");
  
          //3.创建用于封装函数结果的集合对象
          //hive-java集合类型映射关系
          //hive-array => java-Array/List
          //hive-map => java-Map
          //hive-struct => java-Array/List(集合中仅需存储struct中的value部分,key已经在initialize方法中进行了封装)
          ArrayList<ArrayList<Object>> array = new ArrayList<>();
  
          //4.遍历json数组,添加元素到返回的集合
          for (int i = 0; i < jsonArray.length(); i++) {
              ArrayList<Object> struct = new ArrayList<>();
              JSONObject jsonObject = jsonArray.getJSONObject(i);
              for (int j = 0; j < jsonKeys.length; j++) {
                  if (jsonObject.has(jsonKeys[j])){
                      struct.add(jsonObject.get(jsonKeys[j]));
                  }else{
                      struct.add(null);
                  }
              }
              array.add(struct);
          }
  
          //5.返回集合,即函数的返回结果
          return array;
      }
  
      @Override
      public String getDisplayString(String[] children) {
          return getStandardDisplayString("json_array_to_struct_array", children);
      }
  
  }
  ```

- 上传自定义函数jar包，导入数据

  ```mysql
  create function json_array_to_struct_array
  as 'com.atguigu.hive.udtf.JsonArrayToStructArray' 
  using jar 'hdfs://hadoop201:8020/user/hive/jars/hive-1.0-SNAPSHOT.jar';
  
  SET hive.input.format=org.apache.hadoop.hive.ql.io.HiveInputFormat;
  
  insert overwrite table dwd_error_log partition(dt='2020-06-24')
  select
      get_json_object(line,'$.common.ar'),
      get_json_object(line,'$.common.ba'),
      get_json_object(line,'$.common.ch'),
      get_json_object(line,'$.common.md'),
      get_json_object(line,'$.common.mid'),
      get_json_object(line,'$.common.os'),
      get_json_object(line,'$.common.uid'),
      get_json_object(line,'$.common.vc'),
      get_json_object(line,'$.page.item'),
      get_json_object(line,'$.page.item_type'),
      get_json_object(line,'$.page.last_page_id'),
      get_json_object(line,'$.page.page_id'),
      get_json_object(line,'$.page.sourceType'),
      get_json_object(line,'$.start.entry'),
      get_json_object(line,'$.start.loading_time'),
      get_json_object(line,'$.start.open_ad_id'),
      get_json_object(line,'$.start.open_ad_ms'),
      get_json_object(line,'$.start.open_ad_skip_ms'),
      json_array_to_struct_array(get_json_object(line,'$.actions'),'action_id item item_type ts','action_id:string item:string item_type:string ts:bigint'),
      json_array_to_struct_array(get_json_object(line,'$.displays'),'displayType item item_type order','display_type:string item:string item_type:string order:int'),
      get_json_object(line,'$.ts'),
      get_json_object(line,'$.err.error_code'),
      get_json_object(line,'$.err.msg')
  from ods_log 
  where dt='2020-06-24'
  and get_json_object(line,'$.err') is not null;
  ```



#### 4.1.7  DWD层日志数据脚本

1. 创建目标表

2. 编写数据加载脚本

   ```shell
   cd /home/atguigu/bin
   vi ods_to_dwd_log.sh
   ```

   ```shell
   #!/bin/bash
   hive=/opt/module/hive-3.1.2/bin/hive
   APP=gmall
   if [ -n "$1" ] ;then
       do_date=$1
   else 
       do_date=`date -d "-1 day" +%F`
   fi
   
   sql="
   SET mapreduce.job.queuename=hive;
   SET hive.input.format=org.apache.hadoop.hive.ql.io.HiveInputFormat;
   insert overwrite table ${APP}.dwd_start_log partition(dt='$do_date')
   select 
       get_json_object(line,'$.common.ar'),
       get_json_object(line,'$.common.ba'),
       get_json_object(line,'$.common.ch'),
       get_json_object(line,'$.common.md'),
       get_json_object(line,'$.common.mid'),
       get_json_object(line,'$.common.os'),
       get_json_object(line,'$.common.uid'),
       get_json_object(line,'$.common.vc'),
       get_json_object(line,'$.start.entry'),
       get_json_object(line,'$.start.loading_time'),
       get_json_object(line,'$.start.open_ad_id'),
       get_json_object(line,'$.start.open_ad_ms'),
       get_json_object(line,'$.start.open_ad_skip_ms'),
       get_json_object(line,'$.ts')
   from ${APP}.ods_log
   where dt='$do_date'
   and get_json_object(line,'$.start') is not null;
   
   
   insert overwrite table ${APP}.dwd_action_log partition(dt='$do_date')
   select
       get_json_object(line,'$.common.ar'),
       get_json_object(line,'$.common.ba'),
       get_json_object(line,'$.common.ch'),
       get_json_object(line,'$.common.md'),
       get_json_object(line,'$.common.mid'),	
       get_json_object(line,'$.common.os'),
       get_json_object(line,'$.common.uid'),
       get_json_object(line,'$.common.vc'),
       get_json_object(line,'$.page.during_time'),
       get_json_object(line,'$.page.item'),
       get_json_object(line,'$.page.item_type'),
       get_json_object(line,'$.page.last_page_id'),
       get_json_object(line,'$.page.page_id'),
       get_json_object(line,'$.page.sourceType'),
       get_json_object(action,'$.action_id'),
       get_json_object(action,'$.item'),
       get_json_object(action,'$.item_type'),
       get_json_object(action,'$.ts')
   from ${APP}.ods_log lateral view ${APP}.explode_json_array(get_json_object(line,'$.actions')) tmp as action
   where dt='$do_date'
   and get_json_object(line,'$.actions') is not null;
   
   
   insert overwrite table ${APP}.dwd_display_log partition(dt='$do_date')
   select
       get_json_object(line,'$.common.ar'),
       get_json_object(line,'$.common.ba'),
       get_json_object(line,'$.common.ch'),
       get_json_object(line,'$.common.md'),
       get_json_object(line,'$.common.mid'),
       get_json_object(line,'$.common.os'),
       get_json_object(line,'$.common.uid'),
       get_json_object(line,'$.common.vc'),
       get_json_object(line,'$.page.during_time'),
       get_json_object(line,'$.page.item'),
       get_json_object(line,'$.page.item_type'),
       get_json_object(line,'$.page.last_page_id'),
       get_json_object(line,'$.page.page_id'),
       get_json_object(line,'$.page.sourceType'),
       get_json_object(line,'$.ts'),
       get_json_object(displays,'$.displayType'),
       get_json_object(displays,'$.item'),
       get_json_object(displays,'$.item_type'),
       get_json_object(displays,'$.order')
   from ${APP}.ods_log lateral view ${APP}.explode_json_array(get_json_object(line,'$.displays')) tmp as displays
   where dt='$do_date'
   and get_json_object(line,'$.displays') is not null;
   
   insert overwrite table ${APP}.dwd_page_log partition(dt='$do_date')
   select
       get_json_object(line,'$.common.ar'),
       get_json_object(line,'$.common.ba'),
       get_json_object(line,'$.common.ch'),
       get_json_object(line,'$.common.md'),
       get_json_object(line,'$.common.mid'),
       get_json_object(line,'$.common.os'),
       get_json_object(line,'$.common.uid'),
       get_json_object(line,'$.common.vc'),
       get_json_object(line,'$.page.during_time'),
       get_json_object(line,'$.page.item'),
       get_json_object(line,'$.page.item_type'),
       get_json_object(line,'$.page.last_page_id'),
       get_json_object(line,'$.page.page_id'),
       get_json_object(line,'$.page.sourceType'),
       get_json_object(line,'$.ts')
   from ${APP}.ods_log
   where dt='$do_date'
   and get_json_object(line,'$.page') is not null;
   
   
   insert overwrite table ${APP}.dwd_error_log partition(dt='$do_date')
   select
       get_json_object(line,'$.common.ar'),
       get_json_object(line,'$.common.ba'),
       get_json_object(line,'$.common.ch'),
       get_json_object(line,'$.common.md'),
       get_json_object(line,'$.common.mid'),
       get_json_object(line,'$.common.os'),
       get_json_object(line,'$.common.uid'),
       get_json_object(line,'$.common.vc'),
       get_json_object(line,'$.page.item'),
       get_json_object(line,'$.page.item_type'),
       get_json_object(line,'$.page.last_page_id'),
       get_json_object(line,'$.page.page_id'),
       get_json_object(line,'$.page.sourceType'),
       get_json_object(line,'$.start.entry'),
       get_json_object(line,'$.start.loading_time'),
       get_json_object(line,'$.start.open_ad_id'),
       get_json_object(line,'$.start.open_ad_ms'),
       get_json_object(line,'$.start.open_ad_skip_ms'),
       ${APP}.json_array_to_struct_array(get_json_object(line,'$.actions'),'action_id item item_type ts','action_id:string item:string item_type:string ts:bigint'),
       ${APP}.json_array_to_struct_array(get_json_object(line,'$.displays'),'displayType item item_type order','display_type:string item:string item_type:string order:int'),
       get_json_object(line,'$.ts'),
       get_json_object(line,'$.err.error_code'),
       get_json_object(line,'$.err.msg')
   from ${APP}.ods_log 
   where dt='$do_date'
   and get_json_object(line,'$.err') is not null;
   "
   
   $hive -e "$sql"
   ```

3. 添加执行权限

   ```shell
   chmod 777 ods_to_dwd_log.sh
   ```

4. 使用脚本

   ```shell
   ods_to_dwd_log.sh 2020-06-24
   ```



### 4.2 业务数据解析

#### 4.2.1 商品维度表（全量）

- 分析

  1. 概述：以日期作为分区字段创建分区表，每个分区中存储当天的全量商品信息
  2. 数据来源：ods层的spu、sku商品表、三个品类分级表及商标表，共6张表

- 建表语句

  ```mysql
  use gmall;
  DROP TABLE IF EXISTS `dwd_dim_sku_info`;
  CREATE EXTERNAL TABLE `dwd_dim_sku_info` (
      `id` string COMMENT '商品id',
      `spu_id` string COMMENT 'spuid',
      `price` decimal(16,2) COMMENT '商品价格',
      `sku_name` string COMMENT '商品名称',
      `sku_desc` string COMMENT '商品描述',
      `weight` decimal(16,2) COMMENT '重量',
      `tm_id` string COMMENT '品牌id',
      `tm_name` string COMMENT '品牌名称',
      `category3_id` string COMMENT '三级分类id',
      `category2_id` string COMMENT '二级分类id',
      `category1_id` string COMMENT '一级分类id',
      `category3_name` string COMMENT '三级分类名称',
      `category2_name` string COMMENT '二级分类名称',
      `category1_name` string COMMENT '一级分类名称',
      `spu_name` string COMMENT 'spu名称',
      `create_time` string COMMENT '创建时间'
  ) 
  COMMENT '商品维度表'
  PARTITIONED BY (`dt` string)
  stored as parquet
  location '/warehouse/gmall/dwd/dwd_dim_sku_info/'
  tblproperties ("parquet.compression"="lzo");
  ```

- 插入数据

  ```mysql
  set hive.input.format=org.apache.hadoop.hive.ql.io.HiveInputFormat;
  insert overwrite table dwd_dim_sku_info partition(dt='2020-06-24')
  select  
      sku.id,
      sku.spu_id,
      sku.price,
      sku.sku_name,
      sku.sku_desc,
      sku.weight,
      sku.tm_id,
      ob.tm_name,
      sku.category3_id,
      c2.id category2_id,
      c1.id category1_id,
      c3.name category3_name,
      c2.name category2_name,
      c1.name category1_name,
      spu.spu_name,
      sku.create_time
  from
  (
      select * from ods_sku_info where dt='2020-06-24'
  )sku
  join
  (
      select * from ods_base_trademark where dt='2020-06-24'
  )ob on sku.tm_id=ob.tm_id
  join
  (
      select * from ods_spu_info where dt='2020-06-24'
  )spu on spu.id = sku.spu_id
  join 
  (
      select * from ods_base_category3 where dt='2020-06-24'
  )c3 on sku.category3_id=c3.id
  join 
  (
      select * from ods_base_category2 where dt='2020-06-24'
  )c2 on c3.category2_id=c2.id 
  join 
  (
      select * from ods_base_category1 where dt='2020-06-24'
  )c1 on c2.category1_id=c1.id;
  ```



#### 4.2.2 优惠券信息表（全量）

- 分析

  1. 概述：以日期作为分区字段创建分区表，每个分区中存储当天的全量优惠券信息
  2. 数据来源：数据全部来自于ods层的优惠券信息表

- 建表语句

  ```mysql
  use gmall;
  drop table if exists dwd_dim_coupon_info;
  create external table dwd_dim_coupon_info(
      `id` string COMMENT '购物券编号',
      `coupon_name` string COMMENT '购物券名称',
      `coupon_type` string COMMENT '购物券类型 1 现金券 2 折扣券 3 满减券 4 满件打折券',
      `condition_amount` decimal(16,2) COMMENT '满额数',
      `condition_num` bigint COMMENT '满件数',
      `activity_id` string COMMENT '活动编号',
      `benefit_amount` decimal(16,2) COMMENT '减金额',
      `benefit_discount` decimal(16,2) COMMENT '折扣',
      `create_time` string COMMENT '创建时间',
      `range_type` string COMMENT '范围类型 1、商品 2、品类 3、品牌',
      `spu_id` string COMMENT '商品id',
      `tm_id` string COMMENT '品牌id',
      `category3_id` string COMMENT '品类id',
      `limit_num` bigint COMMENT '最多领用次数',
      `operate_time`  string COMMENT '修改时间',
      `expire_time`  string COMMENT '过期时间'
  ) COMMENT '优惠券信息表'
  PARTITIONED BY (`dt` string)
  row format delimited fields terminated by '\t'
  stored as parquet
  location '/warehouse/gmall/dwd/dwd_dim_coupon_info/'
  tblproperties ("parquet.compression"="lzo");
  ```

- 插入数据

  ```mysql
  set hive.input.format=org.apache.hadoop.hive.ql.io.HiveInputFormat;
  insert overwrite table dwd_dim_coupon_info partition(dt='2020-06-24')
  select
      id,
      coupon_name,
      coupon_type,
      condition_amount,
      condition_num,
      activity_id,
      benefit_amount,
      benefit_discount,
      create_time,
      range_type,
      spu_id,
      tm_id,
      category3_id,
      limit_num,
      operate_time,
      expire_time
  from ods_coupon_info
  where dt='2020-06-24';
  ```



#### 4.2.3 活动维度表（全量）

- 分析

  1. 概述：以日期作为分区字段创建分区表，每个分区中存储当天的全部活动信息
  2. 数据来源：ods层的活动信息表和活动规则表，共2张表

- 建表语句

  ```mysql
  use gmall;
  drop table if exists dwd_dim_activity_info;
  create external table dwd_dim_activity_info(
      `id` string COMMENT '编号',
      `activity_name` string  COMMENT '活动名称',
      `activity_type` string  COMMENT '活动类型',
      `start_time` string  COMMENT '开始时间',
      `end_time` string  COMMENT '结束时间',
      `create_time` string  COMMENT '创建时间'
  ) COMMENT '活动信息表'
  PARTITIONED BY (`dt` string)
  row format delimited fields terminated by '\t'
  stored as parquet
  location '/warehouse/gmall/dwd/dwd_dim_activity_info/'
  tblproperties ("parquet.compression"="lzo");
  ```
  
- 插入数据

  ```mysql
  set hive.input.format=org.apache.hadoop.hive.ql.io.HiveInputFormat;
  insert overwrite table dwd_dim_activity_info partition(dt='2020-06-24')
  select
      info.id,
      info.activity_name,
      info.activity_type,
      info.start_time,
      info.end_time,
      info.create_time
  from ods_activity_info info
  where dt='2020-06-24';
  ```



#### 4.2.4 地区维度表（特殊）

- 分析

  1. 概述：不需要分区，且不需要更新，仅在建表后插入一次数据即可
  2. 数据来源：ods层的省份表和地区表，共2张表

- 建表语句

  ```mysql
  use gmall;
  DROP TABLE IF EXISTS `dwd_dim_base_province`;
  CREATE EXTERNAL TABLE `dwd_dim_base_province` (
      `id` string COMMENT 'id',
      `province_name` string COMMENT '省市名称',
      `area_code` string COMMENT '地区编码',
      `iso_code` string COMMENT 'ISO编码',
      `region_id` string COMMENT '地区id',
      `region_name` string COMMENT '地区名称'
  ) 
  COMMENT '地区省市表'
  stored as parquet
  location '/warehouse/gmall/dwd/dwd_dim_base_province/'
  tblproperties ("parquet.compression"="lzo");
  ```

- 插入数据

  ```mysql
  set hive.input.format=org.apache.hadoop.hive.ql.io.HiveInputFormat;
  insert overwrite table dwd_dim_base_province
  select 
      bp.id,
      bp.name,
      bp.area_code,
      bp.iso_code,
      bp.region_id,
      br.region_name
  from ods_base_province bp
  join ods_base_region br
  on bp.region_id=br.id;
  
  ```



#### 4.2.5 时间维度表（特殊）

- 分析

  1. 概述：不需要分区，直接装载某段时间内的日期数据，有需要时手动插入数据
  2. 数据来源：直接从日期数据文件中读取数据

- 建表语句

  ```mysql
  use gmall;
  DROP TABLE IF EXISTS `dwd_dim_date_info`;
  CREATE EXTERNAL TABLE `dwd_dim_date_info`(
      `date_id` string COMMENT '日',
      `week_id` string COMMENT '周',
      `week_day` string COMMENT '周的第几天',
      `day` string COMMENT '每月的第几天',
      `month` string COMMENT '第几月',
      `quarter` string COMMENT '第几季度',
      `year` string COMMENT '年',
      `is_workday` string COMMENT '是否是周末',
      `holiday_id` string COMMENT '是否是节假日'
  )
  row format delimited fields terminated by '\t'
  stored as parquet
  location '/warehouse/gmall/dwd/dwd_dim_date_info/'
  tblproperties ("parquet.compression"="lzo");
  ```

- 插入数据

  ```mysql
  -- 由于数据文件采用行式存储格式,因此需要通过行式存储中间表导入日期数据
  -- 1.创建临时表
  use gmall;
  DROP TABLE IF EXISTS `dwd_dim_date_info_tmp`;
  CREATE EXTERNAL TABLE `dwd_dim_date_info_tmp`(
      `date_id` string COMMENT '日',
      `week_id` string COMMENT '周',
      `week_day` string COMMENT '周的第几天',
      `day` string COMMENT '每月的第几天',
      `month` string COMMENT '第几月',
      `quarter` string COMMENT '第几季度',
      `year` string COMMENT '年',
      `is_workday` string COMMENT '是否是周末',
      `holiday_id` string COMMENT '是否是节假日'
  )
  row format delimited fields terminated by '\t'
  location '/warehouse/gmall/dwd/dwd_dim_date_info_tmp/';
  
  -- 2.从本地数据文件导入数据到临时表
  load data local inpath '/opt/module/db_log/date_info.txt' into table dwd_dim_date_info_tmp;
  
  -- 3.从临时表导入数据到时间维度表
  insert overwrite table dwd_dim_date_info select * from dwd_dim_date_info_tmp;
  ```



#### 4.2.6 订单明细事实表（事务型）

- 分析

  1. 概述：以日期作为分区字段创建分区表，每个分区中存储当天创建的订单的明细数据，表中一行对应订单中的一个商品项
  2. 数据来源：ods层的订单信息表及订单明细表，共2张表
  3. 详述：表中存在多个价格分摊字段，该分摊数据取值应根据商品与订单的原始价格比计算，当同一订单中分摊字段数据总和不等于分摊前的总价格时，应将差额部分追加到订单中的某个商品项中。

- 建表语句

  ```mysql
  use gmall;
  drop table if exists dwd_fact_order_detail;
  create external table dwd_fact_order_detail (
      `id` string COMMENT '订单编号',
      `order_id` string COMMENT '订单号',
      `user_id` string COMMENT '用户id',
      `sku_id` string COMMENT 'sku商品id',
      `sku_name` string COMMENT '商品名称',
      `order_price` decimal(16,2) COMMENT '商品价格',
      `sku_num` bigint COMMENT '商品数量',
      `create_time` string COMMENT '创建时间',
      `province_id` string COMMENT '省份ID',
      `source_type` string COMMENT '来源类型',
      `source_id` string COMMENT '来源编号',
      `original_amount_d` decimal(20,2) COMMENT '原始价格分摊',
      `final_amount_d` decimal(20,2) COMMENT '购买价格分摊',
      `feight_fee_d` decimal(20,2) COMMENT '分摊运费',
      `benefit_reduce_amount_d` decimal(20,2) COMMENT '分摊优惠'
  ) 
  PARTITIONED BY (`dt` string)
  stored as parquet
  location '/warehouse/gmall/dwd/dwd_fact_order_detail/'
  tblproperties ("parquet.compression"="lzo");
  ```

- 插入数据

  ```mysql
  set hive.input.format=org.apache.hadoop.hive.ql.io.HiveInputFormat;
  insert overwrite table dwd_fact_order_detail partition(dt='2020-06-24')
  select
      id,
      order_id,
      user_id,
      sku_id,
      sku_num,
      order_price,
      sku_num,
      create_time,
      province_id,
      source_type,
      source_id,
      original_amount_d,
      if(rn=1,final_total_amount-(sum_div_final_amount-final_amount_d),final_amount_d),
      if(rn=1,feight_fee-(sum_div_feight_fee-feight_fee_d),feight_fee_d),
      if(rn=1,benefit_reduce_amount-(sum_div_benefit_reduce_amount-benefit_reduce_amount_d),benefit_reduce_amount_d)
  from
  (
      select
          od.id,
          od.order_id,
          od.user_id,
          od.sku_id,
          od.sku_name,
          od.order_price,
          od.sku_num,
          od.create_time,
          oi.province_id,
          od.source_type,
          od.source_id,
          round(od.order_price*od.sku_num,2) original_amount_d,
          round(od.order_price*od.sku_num/oi.original_total_amount*oi.final_total_amount,2) final_amount_d,
          round(od.order_price*od.sku_num/oi.original_total_amount*oi.feight_fee,2) feight_fee_d,
          round(od.order_price*od.sku_num/oi.original_total_amount*oi.benefit_reduce_amount,2) benefit_reduce_amount_d,
          row_number() over(partition by od.order_id order by od.id desc) rn,
          oi.final_total_amount,
          oi.feight_fee,
          oi.benefit_reduce_amount,
          sum(round(od.order_price*od.sku_num/oi.original_total_amount*oi.final_total_amount,2)) over(partition by od.order_id) sum_div_final_amount,
          sum(round(od.order_price*od.sku_num/oi.original_total_amount*oi.feight_fee,2)) over(partition by od.order_id) sum_div_feight_fee,
          sum(round(od.order_price*od.sku_num/oi.original_total_amount*oi.benefit_reduce_amount,2)) over(partition by od.order_id) sum_div_benefit_reduce_amount
      from 
      (
          select * from ods_order_detail where dt='2020-06-24'
      ) od
      join 
      (
          select * from ods_order_info where dt='2020-06-24'
      ) oi
      on od.order_id=oi.id
  ) t1;
  ```



#### 4.2.7 支付事实表（事务型）

- 分析

  - 概述：以日期作为分区字段创建分区表，每个分区中存储当日的支付流水数据
  - 数据来源：ods层的支付信息表和订单表，共2张表

- 建表语句

  ```mysql
  use gmall;
  drop table if exists dwd_fact_payment_info;
  create external table dwd_fact_payment_info (
      `id` string COMMENT '',
      `out_trade_no` string COMMENT '对外业务编号',
      `order_id` string COMMENT '订单编号',
      `user_id` string COMMENT '用户编号',
      `alipay_trade_no` string COMMENT '支付宝交易流水编号',
      `payment_amount`    decimal(16,2) COMMENT '支付金额',
      `subject`         string COMMENT '交易内容',
      `payment_type` string COMMENT '支付类型',
      `payment_time` string COMMENT '支付时间',
      `province_id` string COMMENT '省份ID'
  ) 
  PARTITIONED BY (`dt` string)
  stored as parquet
  location '/warehouse/gmall/dwd/dwd_fact_payment_info/'
  tblproperties ("parquet.compression"="lzo");
  ```

- 插入数据

  ```mysql
  set hive.input.format=org.apache.hadoop.hive.ql.io.HiveInputFormat;
  insert overwrite table dwd_fact_payment_info partition(dt='2020-06-24')
  select
      pi.id,
      pi.out_trade_no,
      pi.order_id,
      pi.user_id,
      pi.alipay_trade_no,
      pi.total_amount,
      pi.subject,
      pi.payment_type,
      pi.payment_time,          
      oi.province_id
  from
  (
      select * from ods_payment_info where dt='2020-06-24'
  )pi
  join
  (
      select id, province_id from ods_order_info where dt='2020-06-24'
  )oi
  on pi.order_id = oi.id;
  ```



#### 4.2.8 退款事实表（事务型）

- 分析

  - 概述：以日期作为分区字段创建分区表，每个分区中存储当日的退款流水数据
  - 数据来源：数据全部来源于ods层的退款信息表

- 建表语句

  ```mysql
  use gmall;
  drop table if exists dwd_fact_order_refund_info;
  create external table dwd_fact_order_refund_info(
      `id` string COMMENT '编号',
      `user_id` string COMMENT '用户ID',
      `order_id` string COMMENT '订单ID',
      `sku_id` string COMMENT '商品ID',
      `refund_type` string COMMENT '退款类型',
      `refund_num` bigint COMMENT '退款件数',
      `refund_amount` decimal(16,2) COMMENT '退款金额',
      `refund_reason_type` string COMMENT '退款原因类型',
      `create_time` string COMMENT '退款时间'
  ) COMMENT '退款事实表'
  PARTITIONED BY (`dt` string)
  row format delimited fields terminated by '\t'
  location '/warehouse/gmall/dwd/dwd_fact_order_refund_info/';
  ```

- 插入数据

  ```mysql
  set hive.input.format=org.apache.hadoop.hive.ql.io.HiveInputFormat;
  insert overwrite table dwd_fact_order_refund_info partition(dt='2020-06-24')
  select
      id,
      user_id,
      order_id,
      sku_id,
      refund_type,
      refund_num,
      refund_amount,
      refund_reason_type,
      create_time
  from ods_order_refund_info
  where dt='2020-06-24';
  ```



#### 4.2.9 评价事实表（事务型）

- 分析

  1. 概述：以日期作为分区字段创建分区表，每个分区中存储当日新增的所有评价
  2. 数据来源：数据全部来自ods层的评价信息表

- 建表语句

  ```mysql
  use gmall;
  drop table if exists dwd_fact_comment_info;
  create external table dwd_fact_comment_info(
      `id` string COMMENT '编号',
      `user_id` string COMMENT '用户ID',
      `sku_id` string COMMENT '商品sku',
      `spu_id` string COMMENT '商品spu',
      `order_id` string COMMENT '订单ID',
      `appraise` string COMMENT '评价',
      `create_time` string COMMENT '评价时间'
  ) COMMENT '评价事实表'
  PARTITIONED BY (`dt` string)
  row format delimited fields terminated by '\t'
  location '/warehouse/gmall/dwd/dwd_fact_comment_info/';
  ```

- 插入数据

  ```mysql
  set hive.input.format=org.apache.hadoop.hive.ql.io.HiveInputFormat;
  insert overwrite table dwd_fact_comment_info partition(dt='2020-06-24')
  select
      id,
      user_id,
      sku_id,
      spu_id,
      order_id,
      appraise,
      create_time
  from ods_comment_info
  where dt='2020-06-24';
  ```



#### 4.2.10 加购事实表（周期型快照）

- 分析

  1. 概述：以日期作为分区字段创建分区表，每个分区中存储当日所有用户的购物车信息
  2. 数据来源：数据全部来源于ods层的购物车信息表

- 建表语句

  ```mysql
  use gmall;
  drop table if exists dwd_fact_cart_info;
  create external table dwd_fact_cart_info(
      `id` string COMMENT '编号',
      `user_id` string  COMMENT '用户id',
      `sku_id` string  COMMENT 'skuid',
      `cart_price` string  COMMENT '放入购物车时价格',
      `sku_num` string  COMMENT '数量',
      `sku_name` string  COMMENT 'sku名称 (冗余)',
      `create_time` string  COMMENT '创建时间',
      `operate_time` string COMMENT '修改时间',
      `is_ordered` string COMMENT '是否已经下单。1为已下单;0为未下单',
  `order_time` string  COMMENT '下单时间',
  `source_type` string COMMENT '来源类型',
  `srouce_id` string COMMENT '来源编号'
  ) COMMENT '加购事实表'
  PARTITIONED BY (`dt` string)
  row format delimited fields terminated by '\t'
  location '/warehouse/gmall/dwd/dwd_fact_cart_info/';
  ```

- 插入数据

  ```mysql
  set hive.input.format=org.apache.hadoop.hive.ql.io.HiveInputFormat;
  insert overwrite table dwd_fact_cart_info partition(dt='2020-06-24')
  select
      id,
      user_id,
      sku_id,
      cart_price,
      sku_num,
      sku_name,
      create_time,
      operate_time,
      is_ordered,
  order_time,
  source_type,
  source_id
  from ods_cart_info
  where dt='2020-06-24';
  ```



#### 4.2.11 收藏事实表（周期型快照）

- 分析

  1. 概述：以日期作为分区字段创建分区表，每个分区中存储当日所有用户的收藏列表信息
  2. 数据来源：数据全部来源于ods层的收藏信息表

- 建表语句

  ```mysql
  use gmall;
  drop table if exists dwd_fact_favor_info;
  create external table dwd_fact_favor_info(
      `id` string COMMENT '编号',
      `user_id` string  COMMENT '用户id',
      `sku_id` string  COMMENT 'skuid',
      `spu_id` string  COMMENT 'spuid',
      `is_cancel` string  COMMENT '是否取消',
      `create_time` string  COMMENT '收藏时间',
      `cancel_time` string  COMMENT '取消时间'
  ) COMMENT '收藏事实表'
  PARTITIONED BY (`dt` string)
  row format delimited fields terminated by '\t'
  location '/warehouse/gmall/dwd/dwd_fact_favor_info/';
  ```

- 插入数据

  ```mysql
  set hive.input.format=org.apache.hadoop.hive.ql.io.HiveInputFormat;
  insert overwrite table dwd_fact_favor_info partition(dt='2020-06-24')
  select
      id,
      user_id,
      sku_id,
      spu_id,
      is_cancel,
      create_time,
      cancel_time
  from ods_favor_info
  where dt='2020-06-24';
  ```



#### 4.2.12 优惠券领用事实表（累积型快照）

- 分析

  1. 概述：以日期作为分区字段创建分区表，每个分区中存储优惠券领用日期为分区日期的所有优惠券信息
  2. 数据来源：ods层的优惠券领用信息表和dwd层的优惠券领用事实表（自身），共2张表
  3. 详述：每日更新领用事实表数据时，既需要将当日新领用的优惠券数据插入当日新分区中，还需要将已领用的优惠券的新使用记录更新到对应的旧分区中。更新旧数据的方式为整体拉取对应分区的全部数据，对目标字段进行更新，再使用动态分区将多个个分区数据分别覆盖写回相应分区。

- 建表语句

  ```mysql
  use gmall;
  drop table if exists dwd_fact_coupon_use;
  create external table dwd_fact_coupon_use(
      `id` string COMMENT '编号',
      `coupon_id` string  COMMENT '优惠券ID',
      `user_id` string  COMMENT 'userid',
      `order_id` string  COMMENT '订单id',
      `coupon_status` string  COMMENT '优惠券状态',
      `get_time` string  COMMENT '领取时间',
      `using_time` string  COMMENT '使用时间(下单)',
      `used_time` string  COMMENT '使用时间(支付)'
  ) COMMENT '优惠券领用事实表'
  PARTITIONED BY (`dt` string)
  row format delimited fields terminated by '\t'
  location '/warehouse/gmall/dwd/dwd_fact_coupon_use/';
  ```

- 插入数据

  ```mysql
  set hive.exec.dynamic.partition.mode=nonstrict;
  set hive.input.format=org.apache.hadoop.hive.ql.io.HiveInputFormat;
  insert overwrite table dwd_fact_coupon_use partition(dt)
  select
      if(new.id is null,old.id,new.id),
      if(new.coupon_id is null,old.coupon_id,new.coupon_id),
      if(new.user_id is null,old.user_id,new.user_id),
      if(new.order_id is null,old.order_id,new.order_id),
      if(new.coupon_status is null,old.coupon_status,new.coupon_status),
      if(new.get_time is null,old.get_time,new.get_time),
      if(new.using_time is null,old.using_time,new.using_time),
      if(new.used_time is null,old.used_time,new.used_time),
      date_format(if(new.get_time is null,old.get_time,new.get_time),'yyyy-MM-dd')
  from
  (
      select
          id,
          coupon_id,
          user_id,
          order_id,
          coupon_status,
          get_time,
          using_time,
          used_time
      from dwd_fact_coupon_use
      where dt in
      (
          select
              date_format(get_time,'yyyy-MM-dd')
          from ods_coupon_use
          where dt='2020-06-24'
      )
  )old
  full outer join
  (
      select
          id,
          coupon_id,
          user_id,
          order_id,
          coupon_status,
          get_time,
          using_time,
          used_time
      from ods_coupon_use
      where dt='2020-06-24'
  )new
  on old.id=new.id;
  ```



#### 4.2.13 订单事实表（累积型快照）

- 分析

  1. 概述：以日期作为分区字段创建分区表，每个分区中存储创建时间与分区日期一致的订单信息
  2. 数据来源：ods层的订单信息表、订单状态表、活动订单表和dwd层的订单事实表（自身），共4张表
  3. 详述：
     - 更新订单事实表时，既需要将当日的新增订单数据插入当日分区中，还需要修改旧分区中旧订单的状态。
     - 更新旧分区数据的方式为，拉取旧分区的全部数据，将订单状态信息修改后，通过动态分区将数据覆盖写回原分区。
     - 在处理ods层的订单状态表数据时，可按照订单id分组，将当天该订单的所有【订单状态-修改时间】插入map集合，再与订单信息表通过订单id关联。

- 建表语句

  ```mysql
  use gmall;
  drop table if exists dwd_fact_order_info;
  create external table dwd_fact_order_info (
      `id` string COMMENT '订单编号',
      `order_status` string COMMENT '订单状态',
      `user_id` string COMMENT '用户id',
      `out_trade_no` string COMMENT '支付流水号',
      `create_time` string COMMENT '创建时间(未支付状态)',
      `payment_time` string COMMENT '支付时间(已支付状态)',
      `cancel_time` string COMMENT '取消时间(已取消状态)',
      `finish_time` string COMMENT '完成时间(已完成状态)',
      `refund_time` string COMMENT '退款时间(退款中状态)',
      `refund_finish_time` string COMMENT '退款完成时间(退款完成状态)',
      `province_id` string COMMENT '省份ID',
      `activity_id` string COMMENT '活动ID',
      `original_total_amount` decimal(16,2) COMMENT '原价金额',
      `benefit_reduce_amount` decimal(16,2) COMMENT '优惠金额',
      `feight_fee` decimal(16,2) COMMENT '运费',
      `final_total_amount` decimal(16,2) COMMENT '订单金额'
  ) 
  PARTITIONED BY (`dt` string)
  stored as parquet
  location '/warehouse/gmall/dwd/dwd_fact_order_info/'
  tblproperties ("parquet.compression"="lzo");
  ```

- 插入数据

  ```mysql
  set hive.exec.dynamic.partition.mode=nonstrict;
  set hive.input.format=org.apache.hadoop.hive.ql.io.HiveInputFormat;
  insert overwrite table dwd_fact_order_info partition(dt)
  select
      if(new.id is null,old.id,new.id),
      if(new.order_status is null,old.order_status,new.order_status),
      if(new.user_id is null,old.user_id,new.user_id),
      if(new.out_trade_no is null,old.out_trade_no,new.out_trade_no),
      if(new.tms['1001'] is null,old.create_time,new.tms['1001']),--1001对应未支付状态
      if(new.tms['1002'] is null,old.payment_time,new.tms['1002']),
      if(new.tms['1003'] is null,old.cancel_time,new.tms['1003']),
      if(new.tms['1004'] is null,old.finish_time,new.tms['1004']),
      if(new.tms['1005'] is null,old.refund_time,new.tms['1005']),
      if(new.tms['1006'] is null,old.refund_finish_time,new.tms['1006']),
      if(new.province_id is null,old.province_id,new.province_id),
      if(new.activity_id is null,old.activity_id,new.activity_id),
      if(new.original_total_amount is null,old.original_total_amount,new.original_total_amount),
      if(new.benefit_reduce_amount is null,old.benefit_reduce_amount,new.benefit_reduce_amount),
      if(new.feight_fee is null,old.feight_fee,new.feight_fee),
      if(new.final_total_amount is null,old.final_total_amount,new.final_total_amount),
      date_format(if(new.tms['1001'] is null,old.create_time,new.tms['1001']),'yyyy-MM-dd')
  from
  (
      select
          id,
          order_status,
          user_id,
          out_trade_no,
          create_time,
          payment_time,
          cancel_time,
          finish_time,
          refund_time,
          refund_finish_time,
          province_id,
          activity_id,
          original_total_amount,
          benefit_reduce_amount,
          feight_fee,
          final_total_amount
      from dwd_fact_order_info
      where dt
      in
      (
          select
            date_format(create_time,'yyyy-MM-dd')
          from ods_order_info
          where dt='2020-06-24'
      )
  )old
  full outer join
  (
      select
          info.id,
          info.order_status,
          info.user_id,
          info.out_trade_no,
          info.province_id,
          act.activity_id,
          log.tms,
          info.original_total_amount,
          info.benefit_reduce_amount,
          info.feight_fee,
          info.final_total_amount
      from
      (
          select
              order_id,
              str_to_map(concat_ws(',',collect_set(concat(order_status,'=',operate_time))),',','=') tms
          from ods_order_status_log
          where dt='2020-06-24'
          group by order_id
      )log
      join
      (
          select * from ods_order_info where dt='2020-06-24'
      )info
      on log.order_id=info.id
      left join
      (
          select * from ods_activity_order where dt='2020-06-24'
      )act
      on log.order_id=act.order_id
  )new
  on old.id=new.id;
  ```



#### 4.2.14 用户维度表（拉链）

- 拉链表

  - 定义：数据不分区，除基本的指标字段外，额外增加两个日期字段（开始、结束日期）记录该条数据的**生效范围**
  - 应用场景通常满足以下条件
    1. 需要保留历史数据记录
    2. 修改量不大（若使用分区表每日全量更新会导致大量数据冗余）

- 分析

  1. 概述：使用拉链表存储用户信息，数据不分区，每日更新
  2. 数据来源：全部来源于ods的用户信息表
  3. 详述
     - 创建表后需要手动导入初始数据，后续每天导入新增及需要修改信息的客户数据
     - 对于当日的新数据，直接手动为开始、结束日期字段赋值
     - 对于旧数据，需要修改结束日期为'9999-99-99'的数据内容为当日的前一天
     - 最后对两表进行union合并，将数据插入临时表，再导回维度表

- 建表语句

  ```mysql
  use gmall;
  drop table if exists dwd_dim_user_info_his;
  create external table dwd_dim_user_info_his(
      `id` string COMMENT '用户id',
      `name` string COMMENT '姓名', 
      `birthday` string COMMENT '生日',
      `gender` string COMMENT '性别',
      `email` string COMMENT '邮箱',
      `user_level` string COMMENT '用户等级',
      `create_time` string COMMENT '创建时间',
      `operate_time` string COMMENT '操作时间',
      `start_date`  string COMMENT '有效开始日期',
      `end_date`  string COMMENT '有效结束日期'
  ) COMMENT '订单拉链表'
  stored as parquet
  location '/warehouse/gmall/dwd/dwd_dim_user_info_his/'
  tblproperties ("parquet.compression"="lzo");
  
  #为确保数据安全,需要额外创建临时表用于临时导入每日的拉链表数据
  drop table if exists dwd_dim_user_info_his_tmp;
  create external table dwd_dim_user_info_his_tmp(
      `id` string COMMENT '用户id',
      `name` string COMMENT '姓名', 
      `birthday` string COMMENT '生日',
      `gender` string COMMENT '性别',
      `email` string COMMENT '邮箱',
      `user_level` string COMMENT '用户等级',
      `create_time` string COMMENT '创建时间',
      `operate_time` string COMMENT '操作时间',
      `start_date`  string COMMENT '有效开始日期',
      `end_date`  string COMMENT '有效结束日期'
  ) COMMENT '订单拉链临时表'
  stored as parquet
  location '/warehouse/gmall/dwd/dwd_dim_user_info_his_tmp/'
  tblproperties ("parquet.compression"="lzo");
  ```

- 初始化数据

  ```mysql
  set hive.input.format=org.apache.hadoop.hive.ql.io.HiveInputFormat;
  insert overwrite table dwd_dim_user_info_his
  select
      id,
      name,
      birthday,
      gender,
      email,
      user_level,
      create_time,
      operate_time,
      '2020-06-24',
      '9999-99-99'
  from ods_user_info oi
  where oi.dt='2020-06-24';
  ```
  
- 插入数据

  ```mysql
  -- 1.将数据插入临时表
  set hive.input.format=org.apache.hadoop.hive.ql.io.HiveInputFormat;
  insert overwrite table dwd_dim_user_info_his_tmp
  select * from 
  (
      select 
          id,
          name,
          birthday,
          gender,
          email,
          user_level,
          create_time,
          operate_time,
          '2020-06-24' start_date,
          '9999-99-99' end_date
      from ods_user_info where dt='2020-06-24'
  
      union all 
      
      select 
          uh.id,
          uh.name,
          uh.birthday,
          uh.gender,
          uh.email,
          uh.user_level,
          uh.create_time,
          uh.operate_time,
          uh.start_date,
          if(ui.id is not null and uh.end_date='9999-99-99', date_add(ui.dt,-1), uh.end_date) end_date
      from dwd_dim_user_info_his uh left join 
      (
          select
              *
          from ods_user_info
          where dt='2020-06-24'
      ) ui on uh.id=ui.id
  )his 
  order by his.id, start_date;
  
  -- 2.将数据导入正式表
  insert overwrite table dwd_dim_user_info_his 
  select * from dwd_dim_user_info_his_tmp;
  ```



#### 4.2.15 DWD层业务数据脚本

1. 创建dwd层业务表

2. 手动插入时间维度表数据

3. 手动初始化用户维度表数据

4. 编写dwd层业务数据脚本

   ```shell
   cd bin/
   vi ods_to_dwd_db.sh
   ```

   ```shell
   #!/bin/bash
   
   APP=gmall
   hive=/opt/module/hive-3.1.2/bin/hive
   
   if [ -n "$2" ] ;then
       do_date=$2
   else 
       do_date=`date -d "-1 day" +%F`
   fi
   
   sql1="
   set mapreduce.job.queuename=hive;
   set hive.exec.dynamic.partition.mode=nonstrict;
   SET hive.input.format=org.apache.hadoop.hive.ql.io.HiveInputFormat;
   
   insert overwrite table ${APP}.dwd_dim_sku_info partition(dt='$do_date')
   select  
       sku.id,
       sku.spu_id,
       sku.price,
       sku.sku_name,
       sku.sku_desc,
       sku.weight,
       sku.tm_id,
       ob.tm_name,
       sku.category3_id,
       c2.id category2_id,
       c1.id category1_id,
       c3.name category3_name,
       c2.name category2_name,
       c1.name category1_name,
       spu.spu_name,
       sku.create_time
   from
   (
       select * from ${APP}.ods_sku_info where dt='$do_date'
   )sku
   join
   (
       select * from ${APP}.ods_base_trademark where dt='$do_date'
   )ob on sku.tm_id=ob.tm_id
   join
   (
       select * from ${APP}.ods_spu_info where dt='$do_date'
   )spu on spu.id = sku.spu_id
   join 
   (
       select * from ${APP}.ods_base_category3 where dt='$do_date'
   )c3 on sku.category3_id=c3.id
   join 
   (
       select * from ${APP}.ods_base_category2 where dt='$do_date'
   )c2 on c3.category2_id=c2.id 
   join 
   (
       select * from ${APP}.ods_base_category1 where dt='$do_date'
   )c1 on c2.category1_id=c1.id;
   
   
   insert overwrite table ${APP}.dwd_dim_coupon_info partition(dt='$do_date')
   select
       id,
       coupon_name,
       coupon_type,
       condition_amount,
       condition_num,
       activity_id,
       benefit_amount,
       benefit_discount,
       create_time,
       range_type,
       spu_id,
       tm_id,
       category3_id,
       limit_num,
       operate_time,
       expire_time
   from ${APP}.ods_coupon_info
   where dt='$do_date';
   
   
   insert overwrite table ${APP}.dwd_dim_activity_info partition(dt='$do_date')
   select
       info.id,
       info.activity_name,
       info.activity_type,
       info.start_time,
       info.end_time,
       info.create_time
   from ${APP}.ods_activity_info info
   where dt='$do_date';
   
   
   insert overwrite table ${APP}.dwd_fact_order_detail partition(dt='$do_date')
   select
       id,
       order_id,
       user_id,
       sku_id,
       sku_num,
       order_price,
       sku_num,
       create_time,
       province_id,
       source_type,
       source_id,
       original_amount_d,
       if(rn=1,final_total_amount-(sum_div_final_amount-final_amount_d),final_amount_d),
       if(rn=1,feight_fee-(sum_div_feight_fee-feight_fee_d),feight_fee_d),
       if(rn=1,benefit_reduce_amount-(sum_div_benefit_reduce_amount-benefit_reduce_amount_d),benefit_reduce_amount_d)
   from
   (
       select
           od.id,
           od.order_id,
           od.user_id,
           od.sku_id,
           od.sku_name,
           od.order_price,
           od.sku_num,
           od.create_time,
           oi.province_id,
           od.source_type,
           od.source_id,
           round(od.order_price*od.sku_num,2) original_amount_d,
           round(od.order_price*od.sku_num/oi.original_total_amount*oi.final_total_amount,2) final_amount_d,
           round(od.order_price*od.sku_num/oi.original_total_amount*oi.feight_fee,2) feight_fee_d,
           round(od.order_price*od.sku_num/oi.original_total_amount*oi.benefit_reduce_amount,2) benefit_reduce_amount_d,
           row_number() over(partition by od.order_id order by od.id desc) rn,
           oi.final_total_amount,
           oi.feight_fee,
           oi.benefit_reduce_amount,
           sum(round(od.order_price*od.sku_num/oi.original_total_amount*oi.final_total_amount,2)) over(partition by od.order_id) sum_div_final_amount,
           sum(round(od.order_price*od.sku_num/oi.original_total_amount*oi.feight_fee,2)) over(partition by od.order_id) sum_div_feight_fee,
           sum(round(od.order_price*od.sku_num/oi.original_total_amount*oi.benefit_reduce_amount,2)) over(partition by od.order_id) sum_div_benefit_reduce_amount
       from 
       (
           select * from ${APP}.ods_order_detail where dt='$do_date'
       ) od
       join 
       (
           select * from ${APP}.ods_order_info where dt='$do_date'
       ) oi
       on od.order_id=oi.id
   )t1;
   
   insert overwrite table ${APP}.dwd_fact_payment_info partition(dt='$do_date')
   select
       pi.id,
       pi.out_trade_no,
       pi.order_id,
       pi.user_id,
       pi.alipay_trade_no,
       pi.total_amount,
       pi.subject,
       pi.payment_type,
       pi.payment_time,          
       oi.province_id
   from
   (
       select * from ${APP}.ods_payment_info where dt='$do_date'
   )pi
   join
   (
       select id, province_id from ${APP}.ods_order_info where dt='$do_date'
   )oi
   on pi.order_id = oi.id;
   
   
   insert overwrite table ${APP}.dwd_fact_order_refund_info partition(dt='$do_date')
   select
       id,
       user_id,
       order_id,
       sku_id,
       refund_type,
       refund_num,
       refund_amount,
       refund_reason_type,
       create_time
   from ${APP}.ods_order_refund_info
   where dt='$do_date';
   
   
   insert overwrite table ${APP}.dwd_fact_comment_info partition(dt='$do_date')
   select
       id,
       user_id,
       sku_id,
       spu_id,
       order_id,
       appraise,
       create_time
   from ${APP}.ods_comment_info
   where dt='$do_date';
   
   
   insert overwrite table ${APP}.dwd_fact_cart_info partition(dt='$do_date')
   select
       id,
       user_id,
       sku_id,
       cart_price,
       sku_num,
       sku_name,
       create_time,
       operate_time,
       is_ordered,
       order_time,
       source_type,
       source_id
   from ${APP}.ods_cart_info
   where dt='$do_date';
   
   
   insert overwrite table ${APP}.dwd_fact_favor_info partition(dt='$do_date')
   select
       id,
       user_id,
       sku_id,
       spu_id,
       is_cancel,
       create_time,
       cancel_time
   from ${APP}.ods_favor_info
   where dt='$do_date';
   
   insert overwrite table ${APP}.dwd_fact_coupon_use partition(dt)
   select
       if(new.id is null,old.id,new.id),
       if(new.coupon_id is null,old.coupon_id,new.coupon_id),
       if(new.user_id is null,old.user_id,new.user_id),
       if(new.order_id is null,old.order_id,new.order_id),
       if(new.coupon_status is null,old.coupon_status,new.coupon_status),
       if(new.get_time is null,old.get_time,new.get_time),
       if(new.using_time is null,old.using_time,new.using_time),
       if(new.used_time is null,old.used_time,new.used_time),
       date_format(if(new.get_time is null,old.get_time,new.get_time),'yyyy-MM-dd')
   from
   (
       select
           id,
           coupon_id,
           user_id,
           order_id,
           coupon_status,
           get_time,
           using_time,
           used_time
       from ${APP}.dwd_fact_coupon_use
       where dt in
       (
           select
               date_format(get_time,'yyyy-MM-dd')
           from ${APP}.ods_coupon_use
           where dt='$do_date'
       )
   )old
   full outer join
   (
       select
           id,
           coupon_id,
           user_id,
           order_id,
           coupon_status,
           get_time,
           using_time,
           used_time
       from ${APP}.ods_coupon_use
       where dt='$do_date'
   )new
   on old.id=new.id;
   
   
   insert overwrite table ${APP}.dwd_fact_order_info partition(dt)
   select
       if(new.id is null,old.id,new.id),
       if(new.order_status is null,old.order_status,new.order_status),
       if(new.user_id is null,old.user_id,new.user_id),
       if(new.out_trade_no is null,old.out_trade_no,new.out_trade_no),
       if(new.tms['1001'] is null,old.create_time,new.tms['1001']),--1001对应未支付状态
       if(new.tms['1002'] is null,old.payment_time,new.tms['1002']),
       if(new.tms['1003'] is null,old.cancel_time,new.tms['1003']),
       if(new.tms['1004'] is null,old.finish_time,new.tms['1004']),
       if(new.tms['1005'] is null,old.refund_time,new.tms['1005']),
       if(new.tms['1006'] is null,old.refund_finish_time,new.tms['1006']),
       if(new.province_id is null,old.province_id,new.province_id),
       if(new.activity_id is null,old.activity_id,new.activity_id),
       if(new.original_total_amount is null,old.original_total_amount,new.original_total_amount),
       if(new.benefit_reduce_amount is null,old.benefit_reduce_amount,new.benefit_reduce_amount),
       if(new.feight_fee is null,old.feight_fee,new.feight_fee),
       if(new.final_total_amount is null,old.final_total_amount,new.final_total_amount),
       date_format(if(new.tms['1001'] is null,old.create_time,new.tms['1001']),'yyyy-MM-dd')
   from
   (
       select
           id,
           order_status,
           user_id,
           out_trade_no,
           create_time,
           payment_time,
           cancel_time,
           finish_time,
           refund_time,
           refund_finish_time,
           province_id,
           activity_id,
           original_total_amount,
           benefit_reduce_amount,
           feight_fee,
           final_total_amount
       from ${APP}.dwd_fact_order_info
       where dt
       in
       (
           select
             date_format(create_time,'yyyy-MM-dd')
           from ${APP}.ods_order_info
           where dt='$do_date'
       )
   )old
   full outer join
   (
       select
           info.id,
           info.order_status,
           info.user_id,
           info.out_trade_no,
           info.province_id,
           act.activity_id,
           log.tms,
           info.original_total_amount,
           info.benefit_reduce_amount,
           info.feight_fee,
           info.final_total_amount
       from
       (
           select
               order_id,
               str_to_map(concat_ws(',',collect_set(concat(order_status,'=',operate_time))),',','=') tms
           from ${APP}.ods_order_status_log
           where dt='$do_date'
           group by order_id
       )log
       join
       (
           select * from ${APP}.ods_order_info where dt='$do_date'
       )info
       on log.order_id=info.id
       left join
       (
           select * from ${APP}.ods_activity_order where dt='$do_date'
       )act
       on log.order_id=act.order_id
   )new
   on old.id=new.id;
   "
   
   sql2="
   insert overwrite table ${APP}.dwd_dim_base_province
   select 
       bp.id,
       bp.name,
       bp.area_code,
       bp.iso_code,
       bp.region_id,
       br.region_name
   from ${APP}.ods_base_province bp
   join ${APP}.ods_base_region br
   on bp.region_id=br.id;
   "
   
   sql3="
   insert overwrite table ${APP}.dwd_dim_user_info_his_tmp
   select * from 
   (
       select 
           id,
           name,
           birthday,
           gender,
           email,
           user_level,
           create_time,
           operate_time,
           '$do_date' start_date,
           '9999-99-99' end_date
       from ${APP}.ods_user_info where dt='$do_date'
   
       union all 
       select 
           uh.id,
           uh.name,
           uh.birthday,
           uh.gender,
           uh.email,
           uh.user_level,
           uh.create_time,
           uh.operate_time,
           uh.start_date,
           if(ui.id is not null  and uh.end_date='9999-99-99', date_add(ui.dt,-1), uh.end_date) end_date
       from ${APP}.dwd_dim_user_info_his uh left join 
       (
           select
               *
           from ${APP}.ods_user_info
           where dt='$do_date'
       ) ui on uh.id=ui.id
   )his 
   order by his.id, start_date;
   
   insert overwrite table ${APP}.dwd_dim_user_info_his 
   select * from ${APP}.dwd_dim_user_info_his_tmp;
   "
   
   case $1 in
   "first"){
       $hive -e "$sql1$sql2"
   };;
   "all"){
       $hive -e "$sql1$sql3"
   };;
   esac
   ```
   
5. 赋予脚本权限

   ```shell
   chmod 777 ods_to_dwd_db.sh
   ```

6. 执行脚本

   ```shell
   #初次导入时,不导入用户维度表数据
   ods_to_dwd_db.sh first
   #每日导入时,不导入地区维度表数据
   ods_to_dwd_db.sh all
   ```



## 第五章 DWS层搭建

### 5.1 业务术语

1. 用户：在移动端统计中，通常认为每个设备是一个独立的用户
2. 新增用户：联网状态下首次打开应用即为新增用户，卸载应用后再重新安装不计入新增
3. 活跃用户：在规定时间段启动过应用的用户即为活跃用户
4. 周、月活跃用户：某周、月中启动过应用的用户
5. 月活跃率：月活跃用户数量与总用户数量之比
6. 沉默用户：仅在安装应用当天启动过一次的用户
7. 版本分布：某段时间内用户启动应用的版本
8. 本周回流用户：上周未启动应用，本周启动过应用的用户
9. 连续n周活跃用户：连续n周均有启动应用记录的用户
10. 忠诚用户：连续5周及以上活跃的用户
11. 连续活跃用户：连续2周及以上活跃的用户
12. 近期流失用户：连续2-4周没有启动应用的用户
13. 留存用户：初次启动应用一定时间后，仍有启动应用操作的用户
14. 用户新鲜度：每天启动应用的新老用户比
15. 单次使用时长：每次启动应用后使用的时长
16. 日使用时长：一天内使用应用的总时长
17. 启动次数计算标准：IOS--应用退到后台再打开算1次独立启动，Android--距上次启动间隔大于30s算1次独立启动



### 5.2 系统函数（重要）

- concat 连接字符串、数组元素

  ```mysql
  -- 1.concat 连接多个字符串
  -- 参数 任意个string型数据
  concat('a','b') -- ab
  concat('a','=','b') -- a=b
  
  -- 2.concat_ws 使用指定分隔符连接多个字符串,或一个数组中的元素
  -- 参数1 分隔符
  -- 参数2 数组,或字符串
  concat_ws(',','a','b') -- a,b
  concat_ws(',',array)
  ```

- collect 列转行

  ```mysql
  -- 1.collect_set函数 配合group by使用,将同一分组下的指定字段数据封装为数组,并去重
  select class_id,collect_set(stu_name)
  from students
  group by class_id
  
  -- 2.collect_list函数 与collect_set相同,但不对数组元素去重
  ```

- 条件判断

  ```mysql
  -- 1.nvl() 若表达式1非空则取表达式1,若表达式1为空则取表达式2
  nvl(salary,0)
  nvl(new.id,old.id)
  
  -- 2.if() 若表达式1为true则取表达式2,若表达式1为false则取表达式3
  if(new.id is not null,new.id,old.id)
  
  -- 3.coalesce() 若表达式1为空则取表达式2,若表达式2为空则取表达式3,以此类推
  a
  full outer join b on a.id=b.id
  full outer join c on nvl(a.id=b.id)
  full outer join d on coalesce(a.id,b.id,c.id)=d.id
  
  -- 4.array_contains() 判断数组中是否包含指定元素,返回布尔型结果
  array_contains('home')
  ```

- 日期处理

  ```mysql
  -- 1.date_format 将日期字段数据转换为指定格式的数据
  date_format('2020-01-01','yyyy-MM') -- 2020-01
  
  -- 2.date_add 加减指定天数
  date_add('2020-01-01',1) -- 2020-01-02
  date_add('2020-01-01',-1) -- 2019-12-31
  
  -- 3.date_sub 减指定天数
  date_sub('2020-01-01',1) -- 2019-12-31
  
  -- 4.next_day 取下一个星期几的日期 星期可传入头两位、三位、完整单词
  next_day('2020-07-01','Monday') -- 2020-07-06
  
  -- 5.last_day 取当月的最后一天的日期
  last_day('2020-07-01') -- 2020-07-31
  
  -- 6.regexp_replace 将字符串中的指定字符替换为另一字符(配合date_fromat使用)
  date_format(regexp_replace('2020/01/02','/','-'),'yyyy-MM')
  ```

- 获取map、struct类型数据

  ```mysql
  -- 1.str_to_map 将结构化字符串转换为map集合
  -- 参数1 具有分隔符的字符串
  -- 参数2 每个元素的分隔符
  -- 参数3 key与value的分隔符
  str_to_map('1001=2020-06-01,1002=2020-07-01',',','=') -- {"1001":"2020-06-01","1002"="2020-07-01"}
  
  -- 2.named_struct 将多个字符串转换为struct集合
  -- 参数(1、3、5...)为struct中的key
  -- 参数(2、4、6...)为struct中的value
  named_struct('name','zhangsan','age',20)
  
  -- 依次对应array,map,struct类型数据的调用方式
  select friends[1],children['xiao song'],address.city from test
  ```



### 5.3 用户行为日表

#### 5.3.1 每日设备行为

- 分析

  - 概述：以日期为分区字段创建分区表，每个分区中存储以用户维度统计的启动次数和访问页面信息
  - 数据来源：dwd层的启动日志表和页面日志表
  - 详述：将一个用户一天的多个页面访问记录以map集合类型字段存储

- 建表语句

  ```mysql
  use gmall;
  drop table if exists dws_uv_detail_daycount;
  create external table dws_uv_detail_daycount
  (
      `mid_id` string, 
      `brand` string, 
      `model` string, 
      `login_count` bigint COMMENT '活跃次数',
      `page_stats` array<struct<page_id:string,page_count:bigint>> COMMENT '页面访问统计'
  )
  partitioned by(dt string)
  stored as parquet
  location '/warehouse/gmall/dws/dws_uv_detail_daycount';
  ```

- 数据插入

  ```mysql
  with
  tmp_start as
  (
      select  
          mid_id,
          brand,
          model,
          count(*) login_count
      from dwd_start_log
      where dt='2020-06-24'
      group by mid_id,brand,model
  ),
  tmp_page as
  (
      select
          mid_id,
          brand,
          model,        str_to_map(concat_ws(',',collect_set(concat(page_id,':',page_count))),',',':') page_stats
      from
      (
          select
              mid_id,
              brand,
              model,
              page_id,
              count(*) page_count
          from dwd_page_log
          where dt='2020-06-24'
          group by mid_id,brand,model,page_id
      )tmp
      group by mid_id,brand,model
  )
  insert overwrite table dws_uv_detail_daycount partition(dt='2020-06-24')
  select
      nvl(tmp_start.mid_id,tmp_page.mid_id),
      nvl(tmp_start.brand,tmp_page.brand),
      nvl(tmp_start.model,tmp_page.model),
      tmp_start.login_count,
      tmp_page.page_stats
  from tmp_start 
  full outer join tmp_page
  on tmp_start.mid_id=tmp_page.mid_id
  and tmp_start.brand=tmp_page.brand
  and tmp_start.model=tmp_page.model;
  ```



### 5.4 业务日表

#### 5.4.1 每日会员行为

- 分析

  1. 概述：以日期作为分区字段创建分区表，每个分区中存储会员的当日行为记录
  2. 数据来源：dwd层的启动日志表、动作日志表、订单事实表、支付事实表、订单明细表，共5张表
  3. 详述：注意读取日志表数据时，需要过滤未登录即用户id为null的记录

- 建表语句

  ```mysql
  use gmall;
  drop table if exists dws_user_action_daycount;
  create external table dws_user_action_daycount
  (   
      user_id string comment '用户id',
      login_count bigint comment '登录次数',
      cart_count bigint comment '加入购物车次数',
      order_count bigint comment '下单次数',
      order_amount decimal(16,2) comment '下单金额',
      payment_count bigint comment '支付次数',
      payment_amount decimal(16,2) comment '支付金额',
      order_detail_stats array<struct<sku_id:string,sku_num:bigint,order_count:bigint,order_amount:decimal(20,2)>> comment '下单明细统计'
  ) COMMENT '每日用户行为'
  PARTITIONED BY (`dt` string)
  stored as parquet
  location '/warehouse/gmall/dws/dws_user_action_daycount/'
  tblproperties ("parquet.compression"="lzo");
  ```

- 数据插入

  ```mysql
  with
  tmp_login as
  (
      select
          user_id,
          count(*) login_count
      from dwd_start_log
      where dt='2020-06-24'
      and user_id is not null
      group by user_id
  ),
  tmp_cart as
  (
      select
          user_id,
          count(*) cart_count
      from dwd_action_log
      where dt='2020-06-24'
      and user_id is not null
      and action_id='cart_add'
      group by user_id
  ),tmp_order as
  (
      select
          user_id,
          count(*) order_count,
          sum(final_total_amount) order_amount
      from dwd_fact_order_info
      where dt='2020-06-24'
      group by user_id
  ) ,
  tmp_payment as
  (
      select
          user_id,
          count(*) payment_count,
          sum(payment_amount) payment_amount
      from dwd_fact_payment_info
      where dt='2020-06-24'
      group by user_id
  ),
  tmp_order_detail as
  (
      select
          user_id,
          collect_set(named_struct('sku_id',sku_id,'sku_num',sku_num,'order_count',order_count,'order_amount',order_amount)) order_stats
      from
      (
          select
              user_id,
              sku_id,
              sum(sku_num) sku_num,
              count(*) order_count,
              cast(sum(final_amount_d) as decimal(20,2)) order_amount
          from dwd_fact_order_detail
          where dt='2020-06-24'
          group by user_id,sku_id
      )tmp
      group by user_id
  )
  insert overwrite table dws_user_action_daycount partition(dt='2020-06-24')
  select
      tmp_login.user_id,
      login_count,
      nvl(cart_count,0),
      nvl(order_count,0),
      nvl(order_amount,0.0),
      nvl(payment_count,0),
      nvl(payment_amount,0.0),
      order_stats
  from tmp_login
  left join tmp_cart on tmp_login.user_id=tmp_cart.user_id
  left join tmp_order on tmp_login.user_id=tmp_order.user_id
  left join tmp_payment on tmp_login.user_id=tmp_payment.user_id
  left join tmp_order_detail on tmp_login.user_id=tmp_order_detail.user_id;
  ```



#### 5.4.2 每日商品行为

- 分析

  1. 概述：以日期作为分区字段创建分区表，每个分区中存储当日每种商品的各项指标数据
  2. 数据来源：dwd层的订单明细表、订单事实表、退款事实表、加购事实表、收藏事实表、评价事实表，共6张表
  3. 详述：由于所有指标均为数值类型，可以将多个查询结果直接做union后，根据sku_id分组聚合结果，减少join

- 建表语句

  ```mysql
  use gmall;
  drop table if exists dws_sku_action_daycount;
  create external table dws_sku_action_daycount 
  (   
      sku_id string comment 'sku_id',
      order_count bigint comment '被下单次数',
      order_num bigint comment '被下单件数',
      order_amount decimal(16,2) comment '被下单金额',
      payment_count bigint  comment '被支付次数',
      payment_num bigint comment '被支付件数',
      payment_amount decimal(16,2) comment '被支付金额',
      refund_count bigint  comment '被退款次数',
      refund_num bigint comment '被退款件数',
      refund_amount  decimal(16,2) comment '被退款金额',
      cart_count bigint comment '被加入购物车次数',
      favor_count bigint comment '被收藏次数',
      appraise_good_count bigint comment '好评数',
      appraise_mid_count bigint comment '中评数',
      appraise_bad_count bigint comment '差评数',
      appraise_default_count bigint comment '默认评价数'
  ) COMMENT '每日商品行为'
  PARTITIONED BY (`dt` string)
  stored as parquet
  location '/warehouse/gmall/dws/dws_sku_action_daycount/'
  tblproperties ("parquet.compression"="lzo");
  ```

- 数据插入

  ```mysql
  with 
  tmp_order as
  (
      select
          sku_id,
          count(*) order_count,
          sum(sku_num) order_num,
          sum(final_amount_d) order_amount
      from dwd_fact_order_detail
      where dt='2020-06-24'
      group by sku_id
  ),
  tmp_payment as
  (
      select
          sku_id,
          count(*) payment_count,
          sum(sku_num) payment_num,
          sum(final_amount_d) payment_amount
      from dwd_fact_order_detail
      where dt='2020-06-24'
      and order_id in
      (
          select
              id
          from dwd_fact_order_info
          where (dt='2020-06-24'
          or dt=date_add('2020-06-24',-1))
          and date_format(payment_time,'yyyy-MM-dd')='2020-06-24'
      )
      group by sku_id
  ),
  tmp_refund as
  (
      select
          sku_id,
          count(*) refund_count,
          sum(refund_num) refund_num,
          sum(refund_amount) refund_amount
      from dwd_fact_order_refund_info
      where dt='2020-06-24'
      group by sku_id
  ),
  tmp_cart as
  (
      select
          item sku_id,
          count(*) cart_count
      from dwd_action_log
      where dt='2020-06-24'
      and user_id is not null
      and action_id='cart_add'
      group by item 
  ),tmp_favor as
  (
      select
          item sku_id,
          count(*) favor_count
      from dwd_action_log
      where dt='2020-06-24'
      and user_id is not null
      and action_id='favor_add'
      group by item 
  ),
  tmp_appraise as
  (
  select
      sku_id,
      sum(if(appraise='1201',1,0)) appraise_good_count,
      sum(if(appraise='1202',1,0)) appraise_mid_count,
      sum(if(appraise='1203',1,0)) appraise_bad_count,
      sum(if(appraise='1204',1,0)) appraise_default_count
  from dwd_fact_comment_info
  where dt='2020-06-24'
  group by sku_id
  )
  
  insert overwrite table dws_sku_action_daycount partition(dt='2020-06-24')
  select
      sku_id,
      sum(order_count),
      sum(order_num),
      sum(order_amount),
      sum(payment_count),
      sum(payment_num),
      sum(payment_amount),
      sum(refund_count),
      sum(refund_num),
      sum(refund_amount),
      sum(cart_count),
      sum(favor_count),
      sum(appraise_good_count),
      sum(appraise_mid_count),
      sum(appraise_bad_count),
      sum(appraise_default_count)
  from
  (
      select
          sku_id,
          order_count,
          order_num,
          order_amount,
          0 payment_count,
          0 payment_num,
          0 payment_amount,
          0 refund_count,
          0 refund_num,
          0 refund_amount,
          0 cart_count,
          0 favor_count,
          0 appraise_good_count,
          0 appraise_mid_count,
          0 appraise_bad_count,
          0 appraise_default_count
      from tmp_order
      union all
      select
          sku_id,
          0 order_count,
          0 order_num,
          0 order_amount,
          payment_count,
          payment_num,
          payment_amount,
          0 refund_count,
          0 refund_num,
          0 refund_amount,
          0 cart_count,
          0 favor_count,
          0 appraise_good_count,
          0 appraise_mid_count,
          0 appraise_bad_count,
          0 appraise_default_count
      from tmp_payment
      union all
      select
          sku_id,
          0 order_count,
          0 order_num,
          0 order_amount,
          0 payment_count,
          0 payment_num,
          0 payment_amount,
          refund_count,
          refund_num,
          refund_amount,
          0 cart_count,
          0 favor_count,
          0 appraise_good_count,
          0 appraise_mid_count,
          0 appraise_bad_count,
          0 appraise_default_count        
      from tmp_refund
      union all
      select
          sku_id,
          0 order_count,
          0 order_num,
          0 order_amount,
          0 payment_count,
          0 payment_num,
          0 payment_amount,
          0 refund_count,
          0 refund_num,
          0 refund_amount,
          cart_count,
          0 favor_count,
          0 appraise_good_count,
          0 appraise_mid_count,
          0 appraise_bad_count,
          0 appraise_default_count
      from tmp_cart
      union all
      select
          sku_id,
          0 order_count,
          0 order_num,
          0 order_amount,
          0 payment_count,
          0 payment_num,
          0 payment_amount,
          0 refund_count,
          0 refund_num,
          0 refund_amount,
          0 cart_count,
          favor_count,
          0 appraise_good_count,
          0 appraise_mid_count,
          0 appraise_bad_count,
          0 appraise_default_count
      from tmp_favor
      union all
      select
          sku_id,
          0 order_count,
          0 order_num,
          0 order_amount,
          0 payment_count,
          0 payment_num,
          0 payment_amount,
          0 refund_count,
          0 refund_num,
          0 refund_amount,
          0 cart_count,
          0 favor_count,
          appraise_good_count,
          appraise_mid_count,
          appraise_bad_count,
          appraise_default_count
      from tmp_appraise
  )tmp
  group by sku_id;
  ```



#### 5.4.3 每日活动统计

- 分析

  1. 概述：以日期为分区字段创建分区表，每个分区中存储当日被曝光或被参与的活动的指标数据统计
  2. 数据来源：dwd层的曝光日志表、活动维度表、订单事实表，共3张表
  3. 详述：当日有效但未被曝光或没有订单参与的活动不列入统计

- 建表语句

  ```mysql
  drop table if exists dws_activity_info_daycount;
  create external table dws_activity_info_daycount(
      `id` string COMMENT '编号',
      `activity_name` string  COMMENT '活动名称',
      `activity_type` string  COMMENT '活动类型',
      `start_time` string  COMMENT '开始时间',
      `end_time` string  COMMENT '结束时间',
      `create_time` string  COMMENT '创建时间',
      `display_count` bigint COMMENT '曝光次数',
      `order_count` bigint COMMENT '下单次数',
      `order_amount` decimal(20,2) COMMENT '下单金额',
      `payment_count` bigint COMMENT '支付次数',
      `payment_amount` decimal(20,2) COMMENT '支付金额'
  ) COMMENT '每日活动信息表'
  PARTITIONED BY (`dt` string)
  stored as parquet
  location '/warehouse/gmall/dws/dws_activity_info_daycount/'
  tblproperties ("parquet.compression"="lzo");
  ```

- 数据插入

  ```mysql
  with
  tmp_op as
  (
      select
          activity_id,
          sum(if(date_format(create_time,'yyyy-MM-dd')='2020-06-24',1,0)) order_count,
          sum(if(date_format(create_time,'yyyy-MM-dd')='2020-06-24',final_total_amount,0)) order_amount,
          sum(if(date_format(payment_time,'yyyy-MM-dd')='2020-06-24',1,0)) payment_count,
          sum(if(date_format(payment_time,'yyyy-MM-dd')='2020-06-24',final_total_amount,0)) payment_amount
      from dwd_fact_order_info
      where (dt='2020-06-24' or dt=date_add('2020-06-24',-1))
      and activity_id is not null
      group by activity_id
  ),
  tmp_display as
  (
      select
          item activity_id,
          count(*) display_count
      from dwd_display_log
      where dt='2020-06-24'
      and item_type='activity_id'
      group by item
  ),
  tmp_activity as
  (
      select
          *
      from dwd_dim_activity_info
      where dt='2020-06-24'
  )
  insert overwrite table dws_activity_info_daycount partition(dt='2020-06-24')
  select
      nvl(tmp_op.activity_id,tmp_display.activity_id),
      tmp_activity.activity_name,
      tmp_activity.activity_type,
      tmp_activity.start_time,
      tmp_activity.end_time,
      tmp_activity.create_time,
      tmp_display.display_count,
      tmp_op.order_count,
      tmp_op.order_amount,
      tmp_op.payment_count,
      tmp_op.payment_amount
  from tmp_op
  full outer join tmp_display on tmp_op.activity_id=tmp_display.activity_id
  left join tmp_activity on nvl(tmp_op.activity_id,tmp_display.activity_id)=tmp_activity.id;
  ```



#### 5.4.4 每日地区统计

- 分析

  1. 概述：以日期作为分区字段创建分区表，每个分区中存储当日所有省份维度的指标数据
  2. 数据来源：dwd层的启动日志表、地区维度表、订单事实表，共3张表
  3. 详述：对于当日没有活跃数据的省份也要呈现在结果表中

- 建表语句

  ```mysql
  use gmall;
  drop table if exists dws_area_stats_daycount;
  create external table dws_area_stats_daycount(
      `id` bigint COMMENT '编号',
      `province_name` string COMMENT '省份名称',
      `area_code` string COMMENT '地区编码',
      `iso_code` string COMMENT 'iso编码',
      `region_id` string COMMENT '地区ID',
      `region_name` string COMMENT '地区名称',
      `login_count` string COMMENT '地区启动次数',
      `order_count` bigint COMMENT '下单次数',
      `order_amount` decimal(20,2) COMMENT '下单金额',
      `payment_count` bigint COMMENT '支付次数',
      `payment_amount` decimal(20,2) COMMENT '支付金额'
  ) COMMENT '每日地区信息表'
  PARTITIONED BY (`dt` string)
  stored as parquet
  location '/warehouse/gmall/dws/dws_area_stats_daycount/'
  tblproperties ("parquet.compression"="lzo");
  ```

- 数据插入

  ```mysql
  with 
  tmp_login as
  (
      select
          area_code,
          count(*) login_count
      from dwd_start_log
      where dt='2020-06-24'
      group by area_code
  ),
  tmp_op as
  (
      select
          province_id,
          sum(if(date_format(create_time,'yyyy-MM-dd')='2020-06-24',1,0)) order_count,
          sum(if(date_format(create_time,'yyyy-MM-dd')='2020-06-24',final_total_amount,0)) order_amount,
          sum(if(date_format(payment_time,'yyyy-MM-dd')='2020-06-24',1,0)) payment_count,
          sum(if(date_format(payment_time,'yyyy-MM-dd')='2020-06-24',final_total_amount,0)) payment_amount
      from dwd_fact_order_info
      where (dt='2020-06-24' or dt=date_add('2020-06-24',-1))
      group by province_id
  )
  insert overwrite table dws_area_stats_daycount partition(dt='2020-06-24')
  select
      pro.id,
      pro.province_name,
      pro.area_code,
      pro.iso_code,
      pro.region_id,
      pro.region_name,
      nvl(tmp_login.login_count,0),
      nvl(tmp_op.order_count,0),
      nvl(tmp_op.order_amount,0.0),
      nvl(tmp_op.payment_count,0),
      nvl(tmp_op.payment_amount,0.0)
  from dwd_dim_base_province pro
  left join tmp_login on pro.area_code=tmp_login.area_code
  left join tmp_op on pro.id=tmp_op.province_id;
  ```



### 5.5 DWS层数据脚本

1. 创建脚本

   ```shell
   cd bin/
   vi dwd_to_dws.sh
   ```

   ```shell
   #!/bin/bash
   
   APP=gmall
   hive=/opt/module/hive-3.1.2/bin/hive
   
   if [ -n "$1" ] ;then
       do_date=$1
   else
       do_date=`date -d "-1 day" +%F`
   fi
   
   sql="
   set mapreduce.job.queuename=hive;
   with
   tmp_start as
   (
       select  
           mid_id,
           brand,
           model,
           count(*) login_count
       from ${APP}.dwd_start_log
       where dt='$do_date'
       group by mid_id,brand,model
   ),
   tmp_page as
   (
       select
           mid_id,
           brand,
           model,        
           collect_set(named_struct('page_id',page_id,'page_count',page_count)) page_stats
       from
       (
           select
               mid_id,
               brand,
               model,
               page_id,
               count(*) page_count
           from ${APP}.dwd_page_log
           where dt='$do_date'
           group by mid_id,brand,model,page_id
       )tmp
       group by mid_id,brand,model
   )
   insert overwrite table ${APP}.dws_uv_detail_daycount partition(dt='$do_date')
   select
       nvl(tmp_start.mid_id,tmp_page.mid_id),
       nvl(tmp_start.brand,tmp_page.brand),
       nvl(tmp_start.model,tmp_page.model),
       tmp_start.login_count,
       tmp_page.page_stats
   from tmp_start 
   full outer join tmp_page
   on tmp_start.mid_id=tmp_page.mid_id
   and tmp_start.brand=tmp_page.brand
   and tmp_start.model=tmp_page.model;
   
   
   with
   tmp_login as
   (
       select
           user_id,
           count(*) login_count
       from ${APP}.dwd_start_log
       where dt='$do_date'
       and user_id is not null
       group by user_id
   ),
   tmp_cart as
   (
       select
           user_id,
           count(*) cart_count
       from ${APP}.dwd_action_log
       where dt='$do_date'
       and user_id is not null
       and action_id='cart_add'
       group by user_id
   ),tmp_order as
   (
       select
           user_id,
           count(*) order_count,
           sum(final_total_amount) order_amount
       from ${APP}.dwd_fact_order_info
       where dt='$do_date'
       group by user_id
   ) ,
   tmp_payment as
   (
       select
           user_id,
           count(*) payment_count,
           sum(payment_amount) payment_amount
       from ${APP}.dwd_fact_payment_info
       where dt='$do_date'
       group by user_id
   ),
   tmp_order_detail as
   (
       select
           user_id,
           collect_set(named_struct('sku_id',sku_id,'sku_num',sku_num,'order_count',order_count,'order_amount',order_amount)) order_stats
       from
       (
           select
               user_id,
               sku_id,
               sum(sku_num) sku_num,
               count(*) order_count,
               cast(sum(final_amount_d) as decimal(20,2)) order_amount
           from ${APP}.dwd_fact_order_detail
           where dt='$do_date'
           group by user_id,sku_id
       )tmp
       group by user_id
   )
   
   insert overwrite table ${APP}.dws_user_action_daycount partition(dt='$do_date')
   select
       tmp_login.user_id,
       login_count,
       nvl(cart_count,0),
       nvl(order_count,0),
       nvl(order_amount,0.0),
       nvl(payment_count,0),
       nvl(payment_amount,0.0),
       order_stats
   from tmp_login
   left outer join tmp_cart on tmp_login.user_id=tmp_cart.user_id
   left outer join tmp_order on tmp_login.user_id=tmp_order.user_id
   left outer join tmp_payment on tmp_login.user_id=tmp_payment.user_id
   left outer join tmp_order_detail on tmp_login.user_id=tmp_order_detail.user_id;
   
   with 
   tmp_order as
   (
       select
           sku_id,
           count(*) order_count,
           sum(sku_num) order_num,
           sum(final_amount_d) order_amount
       from ${APP}.dwd_fact_order_detail
       where dt='$do_date'
       group by sku_id
   ),
   tmp_payment as
   (
       select
           sku_id,
           count(*) payment_count,
           sum(sku_num) payment_num,
           sum(final_amount_d) payment_amount
       from ${APP}.dwd_fact_order_detail
       where dt='$do_date'
       and order_id in
       (
           select
               id
           from ${APP}.dwd_fact_order_info
           where (dt='$do_date'
           or dt=date_add('$do_date',-1))
           and date_format(payment_time,'yyyy-MM-dd')='$do_date'
       )
       group by sku_id
   ),
   tmp_refund as
   (
       select
           sku_id,
           count(*) refund_count,
           sum(refund_num) refund_num,
           sum(refund_amount) refund_amount
       from ${APP}.dwd_fact_order_refund_info
       where dt='$do_date'
       group by sku_id
   ),
   tmp_cart as
   (
       select
           item sku_id,
           count(*) cart_count
       from ${APP}.dwd_action_log
       where dt='$do_date'
       and user_id is not null
       and action_id='cart_add'
       group by item 
   ),tmp_favor as
   (
       select
           item sku_id,
           count(*) favor_count
       from ${APP}.dwd_action_log
       where dt='$do_date'
       and user_id is not null
       and action_id='favor_add'
       group by item 
   ),
   tmp_appraise as
   (
   select
       sku_id,
       sum(if(appraise='1201',1,0)) appraise_good_count,
       sum(if(appraise='1202',1,0)) appraise_mid_count,
       sum(if(appraise='1203',1,0)) appraise_bad_count,
       sum(if(appraise='1204',1,0)) appraise_default_count
   from ${APP}.dwd_fact_comment_info
   where dt='$do_date'
   group by sku_id
   )
   
   insert overwrite table ${APP}.dws_sku_action_daycount partition(dt='$do_date')
   select
       sku_id,
       sum(order_count),
       sum(order_num),
       sum(order_amount),
       sum(payment_count),
       sum(payment_num),
       sum(payment_amount),
       sum(refund_count),
       sum(refund_num),
       sum(refund_amount),
       sum(cart_count),
       sum(favor_count),
       sum(appraise_good_count),
       sum(appraise_mid_count),
       sum(appraise_bad_count),
       sum(appraise_default_count)
   from
   (
       select
           sku_id,
           order_count,
           order_num,
           order_amount,
           0 payment_count,
           0 payment_num,
           0 payment_amount,
           0 refund_count,
           0 refund_num,
           0 refund_amount,
           0 cart_count,
           0 favor_count,
           0 appraise_good_count,
           0 appraise_mid_count,
           0 appraise_bad_count,
           0 appraise_default_count
       from tmp_order
       union all
       select
           sku_id,
           0 order_count,
           0 order_num,
           0 order_amount,
           payment_count,
           payment_num,
           payment_amount,
           0 refund_count,
           0 refund_num,
           0 refund_amount,
           0 cart_count,
           0 favor_count,
           0 appraise_good_count,
           0 appraise_mid_count,
           0 appraise_bad_count,
           0 appraise_default_count
       from tmp_payment
       union all
       select
           sku_id,
           0 order_count,
           0 order_num,
           0 order_amount,
           0 payment_count,
           0 payment_num,
           0 payment_amount,
           refund_count,
           refund_num,
           refund_amount,
           0 cart_count,
           0 favor_count,
           0 appraise_good_count,
           0 appraise_mid_count,
           0 appraise_bad_count,
           0 appraise_default_count        
       from tmp_refund
       union all
       select
           sku_id,
           0 order_count,
           0 order_num,
           0 order_amount,
           0 payment_count,
           0 payment_num,
           0 payment_amount,
           0 refund_count,
           0 refund_num,
           0 refund_amount,
           cart_count,
           0 favor_count,
           0 appraise_good_count,
           0 appraise_mid_count,
           0 appraise_bad_count,
           0 appraise_default_count
       from tmp_cart
       union all
       select
           sku_id,
           0 order_count,
           0 order_num,
           0 order_amount,
           0 payment_count,
           0 payment_num,
           0 payment_amount,
           0 refund_count,
           0 refund_num,
           0 refund_amount,
           0 cart_count,
           favor_count,
           0 appraise_good_count,
           0 appraise_mid_count,
           0 appraise_bad_count,
           0 appraise_default_count
       from tmp_favor
       union all
       select
           sku_id,
           0 order_count,
           0 order_num,
           0 order_amount,
           0 payment_count,
           0 payment_num,
           0 payment_amount,
           0 refund_count,
           0 refund_num,
           0 refund_amount,
           0 cart_count,
           0 favor_count,
           appraise_good_count,
           appraise_mid_count,
           appraise_bad_count,
           appraise_default_count
       from tmp_appraise
   )tmp
   group by sku_id;
   
   with 
   tmp_login as
   (
       select
           area_code,
           count(*) login_count
       from ${APP}.dwd_start_log
       where dt='$do_date'
       group by area_code
   ),
   tmp_op as
   (
       select
           province_id,
           sum(if(date_format(create_time,'yyyy-MM-dd')='$do_date',1,0)) order_count,
           sum(if(date_format(create_time,'yyyy-MM-dd')='$do_date',final_total_amount,0)) order_amount,
           sum(if(date_format(payment_time,'yyyy-MM-dd')='$do_date',1,0)) payment_count,
           sum(if(date_format(payment_time,'yyyy-MM-dd')='$do_date',final_total_amount,0)) payment_amount
       from ${APP}.dwd_fact_order_info
       where (dt='$do_date' or dt=date_add('$do_date',-1))
       group by province_id
   )
   insert overwrite table ${APP}.dws_area_stats_daycount partition(dt='$do_date')
   select
       pro.id,
       pro.province_name,
       pro.area_code,
       pro.iso_code,
       pro.region_id,
       pro.region_name,
       nvl(tmp_login.login_count,0),
       nvl(tmp_op.order_count,0),
       nvl(tmp_op.order_amount,0.0),
       nvl(tmp_op.payment_count,0),
       nvl(tmp_op.payment_amount,0.0)
   from ${APP}.dwd_dim_base_province pro
   left join tmp_login on pro.area_code=tmp_login.area_code
   left join tmp_op on pro.id=tmp_op.province_id;
   
   
   with
   tmp_op as
   (
       select
           activity_id,
           sum(if(date_format(create_time,'yyyy-MM-dd')='$do_date',1,0)) order_count,
           sum(if(date_format(create_time,'yyyy-MM-dd')='$do_date',final_total_amount,0)) order_amount,
           sum(if(date_format(payment_time,'yyyy-MM-dd')='$do_date',1,0)) payment_count,
           sum(if(date_format(payment_time,'yyyy-MM-dd')='$do_date',final_total_amount,0)) payment_amount
       from ${APP}.dwd_fact_order_info
       where (dt='$do_date' or dt=date_add('$do_date',-1))
       and activity_id is not null
       group by activity_id
   ),
   tmp_display as
   (
       select
           item activity_id,
           count(*) display_count
       from ${APP}.dwd_display_log
       where dt='$do_date'
       and item_type='activity_id'
       group by item
   ),
   tmp_activity as
   (
       select
           *
       from ${APP}.dwd_dim_activity_info
       where dt='$do_date'
   )
   insert overwrite table ${APP}.dws_activity_info_daycount partition(dt='$do_date')
   select
       nvl(tmp_op.activity_id,tmp_display.activity_id),
       tmp_activity.activity_name,
       tmp_activity.activity_type,
       tmp_activity.start_time,
       tmp_activity.end_time,
       tmp_activity.create_time,
       tmp_display.display_count,
       tmp_op.order_count,
       tmp_op.order_amount,
       tmp_op.payment_count,
       tmp_op.payment_amount
   from tmp_op
   full outer join tmp_display on tmp_op.activity_id=tmp_display.activity_id
   left join tmp_activity on nvl(tmp_op.activity_id,tmp_display.activity_id)=tmp_activity.id;
   "
   
   $hive -e "$sql"
   ```

2. 添加执行权限

   ```shell
   chmod 777 dwd_to_dws.sh
   ```

3. 执行脚本

   ```shell
   dwd_to_dws.sh
   ```



## 第六章 DWT层搭建

### 6.1 设备主题宽表

- 分析

  1. 概述：以用户（设备id）维度统计用户的活跃记录
  2. 数据来源：dws层的每日设备行为表和宽表自身
  3. 详述：每天从旧表中拉取全部数据，与当日数据join后更新数据，再整体插回宽表

- 建表语句

  ```mysql
  use gmall;
  drop table if exists dwt_uv_topic;
  create external table dwt_uv_topic
  (
      `mid_id` string,
      `brand` string,
      `model` string,
      `login_date_first` string  comment '首次活跃时间',
      `login_date_last` string  comment '末次活跃时间',
      `login_day_count` bigint comment '当日活跃次数',
      `login_count` bigint comment '累积活跃天数'
  )
  stored as parquet
  location '/warehouse/gmall/dwt/dwt_uv_topic';
  ```

- 数据插入

  ```mysql
  insert overwrite table dwt_uv_topic
  select
      nvl(new.mid_id,old.mid_id),
      nvl(new.model,old.model),
      nvl(new.brand,old.brand),
      if(old.mid_id is null,'2020-06-24',old.login_date_first),
      if(new.mid_id is not null,'2020-06-24',old.login_date_last),
      if(new.mid_id is not null, new.login_count,0),
      nvl(old.login_count,0)+if(new.login_count>0,1,0)
  from
  (
      select
          *
      from dwt_uv_topic
  )old
  full outer join
  (
      select
          *
      from dws_uv_detail_daycount
      where dt='2020-06-24'
  )new
  on old.mid_id=new.mid_id;
  ```



### 6.2 会员主题宽表

- 分析

  1. 概述：以会员维度统计会员的登陆、下单、支付信息
  2. 数据来源：dws层的每日会员行为表和宽表自身
  3. 详述：由于存在多个“近30天指标”，需要从dwd层表中抓取30天的数据，其中当天数据也可以通过if函数一并提取

- 建表语句

  ```mysql
  use gmall;
  drop table if exists dwt_user_topic;
  create external table dwt_user_topic
  (
      user_id string  comment '用户id',
      login_date_first string  comment '首次登录时间',
      login_date_last string  comment '末次登录时间',
      login_count bigint comment '累积登录天数',
      login_last_30d_count bigint comment '最近30日登录天数',
      order_date_first string  comment '首次下单时间',
      order_date_last string  comment '末次下单时间',
      order_count bigint comment '累积下单次数',
      order_amount decimal(16,2) comment '累积下单金额',
      order_last_30d_count bigint comment '最近30日下单次数',
      order_last_30d_amount bigint comment '最近30日下单金额',
      payment_date_first string  comment '首次支付时间',
      payment_date_last string  comment '末次支付时间',
      payment_count decimal(16,2) comment '累积支付次数',
      payment_amount decimal(16,2) comment '累积支付金额',
      payment_last_30d_count decimal(16,2) comment '最近30日支付次数',
      payment_last_30d_amount decimal(16,2) comment '最近30日支付金额'
   )COMMENT '用户主题宽表'
  stored as parquet
  location '/warehouse/gmall/dwt/dwt_user_topic/'
  tblproperties ("parquet.compression"="lzo");
  ```

- 数据插入

  ```mysql
  insert overwrite table dwt_user_topic
  select
      nvl(new.user_id,old.user_id),
      if(old.login_date_first is null and new.login_count>0,'2020-06-24',old.login_date_first),
      if(new.login_count>0,'2020-06-24',old.login_date_last),
      nvl(old.login_count,0)+if(new.login_count>0,1,0),
      nvl(new.login_last_30d_count,0),
      if(old.order_date_first is null and new.order_count>0,'2020-06-24',old.order_date_first),
      if(new.order_count>0,'2020-06-24',old.order_date_last),
      nvl(old.order_count,0)+nvl(new.order_count,0),
      nvl(old.order_amount,0)+nvl(new.order_amount,0),
      nvl(new.order_last_30d_count,0),
      nvl(new.order_last_30d_amount,0),
      if(old.payment_date_first is null and new.payment_count>0,'2020-06-24',old.payment_date_first),
      if(new.payment_count>0,'2020-06-24',old.payment_date_last),
      nvl(old.payment_count,0)+nvl(new.payment_count,0),
      nvl(old.payment_amount,0)+nvl(new.payment_amount,0),
      nvl(new.payment_last_30d_count,0),
      nvl(new.payment_last_30d_amount,0)
  from
  dwt_user_topic old
  full outer join
  (
      select
          user_id,
          sum(if(dt='2020-06-24',login_count,0)) login_count,
          sum(if(dt='2020-06-24',order_count,0)) order_count,
          sum(if(dt='2020-06-24',order_amount,0)) order_amount,
          sum(if(dt='2020-06-24',payment_count,0)) payment_count,
          sum(if(dt='2020-06-24',payment_amount,0)) payment_amount,
          sum(if(login_count>0,1,0)) login_last_30d_count,
          sum(order_count) order_last_30d_count,
          sum(order_amount) order_last_30d_amount,
          sum(payment_count) payment_last_30d_count,
          sum(payment_amount) payment_last_30d_amount
      from dws_user_action_daycount
      where dt>=date_add( '2020-06-24',-29)
      group by user_id
  )new
  on old.user_id=new.user_id;
  ```



### 6.3 商品主题宽表

- 分析

  1. 概述：以商品sku_id为维度对多商品的多项指标以累积数据、近30日数据进行统计
  2. 数据来源：dws层的每日商品行为表和宽表本身

- 建表语句

  ```mysql
  use gmall;
  drop table if exists dwt_sku_topic;
  create external table dwt_sku_topic
  (
      sku_id string comment 'sku_id',
      spu_id string comment 'spu_id',
      order_last_30d_count bigint comment '最近30日被下单次数',
      order_last_30d_num bigint comment '最近30日被下单件数',
      order_last_30d_amount decimal(16,2)  comment '最近30日被下单金额',
      order_count bigint comment '累积被下单次数',
      order_num bigint comment '累积被下单件数',
      order_amount decimal(16,2) comment '累积被下单金额',
      payment_last_30d_count   bigint  comment '最近30日被支付次数',
      payment_last_30d_num bigint comment '最近30日被支付件数',
      payment_last_30d_amount  decimal(16,2) comment '最近30日被支付金额',
      payment_count   bigint  comment '累积被支付次数',
      payment_num bigint comment '累积被支付件数',
      payment_amount  decimal(16,2) comment '累积被支付金额',
      refund_last_30d_count bigint comment '最近三十日退款次数',
      refund_last_30d_num bigint comment '最近三十日退款件数',
      refund_last_30d_amount decimal(16,2) comment '最近三十日退款金额',
      refund_count bigint comment '累积退款次数',
      refund_num bigint comment '累积退款件数',
      refund_amount decimal(16,2) comment '累积退款金额',
      cart_last_30d_count bigint comment '最近30日被加入购物车次数',
      cart_count bigint comment '累积被加入购物车次数',
      favor_last_30d_count bigint comment '最近30日被收藏次数',
      favor_count bigint comment '累积被收藏次数',
      appraise_last_30d_good_count bigint comment '最近30日好评数',
      appraise_last_30d_mid_count bigint comment '最近30日中评数',
      appraise_last_30d_bad_count bigint comment '最近30日差评数',
      appraise_last_30d_default_count bigint comment '最近30日默认评价数',
      appraise_good_count bigint comment '累积好评数',
      appraise_mid_count bigint comment '累积中评数',
      appraise_bad_count bigint comment '累积差评数',
      appraise_default_count bigint comment '累积默认评价数'
   )COMMENT '商品主题宽表'
  stored as parquet
  location '/warehouse/gmall/dwt/dwt_sku_topic/'
  tblproperties ("parquet.compression"="lzo");
  ```

- 数据插入

  ```mysql
  insert overwrite table dwt_sku_topic
  select 
      nvl(new.sku_id,old.sku_id),
      sku_info.spu_id,
      nvl(new.order_count30,0),
      nvl(new.order_num30,0),
      nvl(new.order_amount30,0),
      nvl(old.order_count,0) + nvl(new.order_count,0),
      nvl(old.order_num,0) + nvl(new.order_num,0),
      nvl(old.order_amount,0) + nvl(new.order_amount,0),
      nvl(new.payment_count30,0),
      nvl(new.payment_num30,0),
      nvl(new.payment_amount30,0),
      nvl(old.payment_count,0) + nvl(new.payment_count,0),
      nvl(old.payment_num,0) + nvl(new.payment_count,0),
      nvl(old.payment_amount,0) + nvl(new.payment_count,0),
      nvl(new.refund_count30,0),
      nvl(new.refund_num30,0),
      nvl(new.refund_amount30,0),
      nvl(old.refund_count,0) + nvl(new.refund_count,0),
      nvl(old.refund_num,0) + nvl(new.refund_num,0),
      nvl(old.refund_amount,0) + nvl(new.refund_amount,0),
      nvl(new.cart_count30,0),
      nvl(old.cart_count,0) + nvl(new.cart_count,0),
      nvl(new.favor_count30,0),
      nvl(old.favor_count,0) + nvl(new.favor_count,0),
      nvl(new.appraise_good_count30,0),
      nvl(new.appraise_mid_count30,0),
      nvl(new.appraise_bad_count30,0),
      nvl(new.appraise_default_count30,0)  ,
      nvl(old.appraise_good_count,0) + nvl(new.appraise_good_count,0),
      nvl(old.appraise_mid_count,0) + nvl(new.appraise_mid_count,0),
      nvl(old.appraise_bad_count,0) + nvl(new.appraise_bad_count,0),
      nvl(old.appraise_default_count,0) + nvl(new.appraise_default_count,0) 
  from 
  dwt_sku_topic old
  full outer join 
  (
      select 
          sku_id,
          sum(if(dt='2020-06-24',order_count,0 )) order_count,
          sum(if(dt='2020-06-24',order_num ,0 ))  order_num, 
          sum(if(dt='2020-06-24',order_amount,0 )) order_amount ,
          sum(if(dt='2020-06-24',payment_count,0 )) payment_count,
          sum(if(dt='2020-06-24',payment_num,0 )) payment_num,
          sum(if(dt='2020-06-24',payment_amount,0 )) payment_amount,
          sum(if(dt='2020-06-24',refund_count,0 )) refund_count,
          sum(if(dt='2020-06-24',refund_num,0 )) refund_num,
          sum(if(dt='2020-06-24',refund_amount,0 )) refund_amount,  
          sum(if(dt='2020-06-24',cart_count,0 )) cart_count,
          sum(if(dt='2020-06-24',favor_count,0 )) favor_count,
          sum(if(dt='2020-06-24',appraise_good_count,0 )) appraise_good_count,  
          sum(if(dt='2020-06-24',appraise_mid_count,0 ) ) appraise_mid_count ,
          sum(if(dt='2020-06-24',appraise_bad_count,0 )) appraise_bad_count,  
          sum(if(dt='2020-06-24',appraise_default_count,0 )) appraise_default_count,
          sum(order_count) order_count30 ,
          sum(order_num) order_num30,
          sum(order_amount) order_amount30,
          sum(payment_count) payment_count30,
          sum(payment_num) payment_num30,
          sum(payment_amount) payment_amount30,
          sum(refund_count) refund_count30,
          sum(refund_num) refund_num30,
          sum(refund_amount) refund_amount30,
          sum(cart_count) cart_count30,
          sum(favor_count) favor_count30,
          sum(appraise_good_count) appraise_good_count30,
          sum(appraise_mid_count) appraise_mid_count30,
          sum(appraise_bad_count) appraise_bad_count30,
          sum(appraise_default_count) appraise_default_count30 
      from dws_sku_action_daycount
      where dt >= date_add ('2020-06-24', -29)
      group by sku_id    
  )new 
  on new.sku_id = old.sku_id
  left join 
  (select * from dwd_dim_sku_info where dt='2020-06-24') sku_info
  on nvl(new.sku_id,old.sku_id)= sku_info.id;
  ```



### 6.4 活动主题宽表

- 分析

  1. 概述：以活动维度对多个指标的当日、累积数据进行统计
  2. 数据来源：dws层的每日活动统计表和宽表自身

- 建表语句

  ```mysql
  use gmall;
  drop table if exists dwt_activity_topic;
  create external table dwt_activity_topic(
      `id` string COMMENT '编号',
      `activity_name` string  COMMENT '活动名称',
      `activity_type` string  COMMENT '活动类型',
      `start_time` string  COMMENT '开始时间',
      `end_time` string  COMMENT '结束时间',
      `create_time` string  COMMENT '创建时间',
      `display_day_count` bigint COMMENT '当日曝光次数',
      `order_day_count` bigint COMMENT '当日下单次数',
      `order_day_amount` decimal(20,2) COMMENT '当日下单金额',
      `payment_day_count` bigint COMMENT '当日支付次数',
      `payment_day_amount` decimal(20,2) COMMENT '当日支付金额',
      `display_count` bigint COMMENT '累积曝光次数',
      `order_count` bigint COMMENT '累积下单次数',
      `order_amount` decimal(20,2) COMMENT '累积下单金额',
      `payment_count` bigint COMMENT '累积支付次数',
      `payment_amount` decimal(20,2) COMMENT '累积支付金额'
  ) COMMENT '活动主题宽表'
  row format delimited fields terminated by '\t'
  location '/warehouse/gmall/dwt/dwt_activity_topic/'
  tblproperties ("parquet.compression"="lzo");
  ```

- 数据插入

  ```mysql
  insert overwrite table dwt_activity_topic
  select
      nvl(new.id,old.id),
      nvl(new.activity_name,old.activity_name),
      nvl(new.activity_type,old.activity_type),
      nvl(new.start_time,old.start_time),
      nvl(new.end_time,old.end_time),
      nvl(new.create_time,old.create_time),
      nvl(new.display_count,0),
      nvl(new.order_count,0),
      nvl(new.order_amount,0.0),
      nvl(new.payment_count,0),
      nvl(new.payment_amount,0.0),
      nvl(new.display_count,0)+nvl(old.display_count,0),
      nvl(new.order_count,0)+nvl(old.order_count,0),
      nvl(new.order_amount,0.0)+nvl(old.order_amount,0.0),
      nvl(new.payment_count,0)+nvl(old.payment_count,0),
      nvl(new.payment_amount,0.0)+nvl(old.payment_amount,0.0)
  from
  (
      select
          *
      from dwt_activity_topic
  )old
  full outer join
  (
      select
          *
      from dws_activity_info_daycount
      where dt='2020-06-24'
  )new
  on old.id=new.id;
  ```



### 6.5 地区主题宽表

- 分析

  1. 概述：以地区为维度，统计当天和近30天的多项指标数据
  2. 数据来源：dws层的每日地区统计表

- 建表语句

  ```mysql
  use gmall;
  drop table if exists dwt_area_topic;
  create external table dwt_area_topic(
      `id` bigint COMMENT '编号',
      `province_name` string COMMENT '省份名称',
      `area_code` string COMMENT '地区编码',
      `iso_code` string COMMENT 'iso编码',
      `region_id` string COMMENT '地区ID',
      `region_name` string COMMENT '地区名称',
      `login_day_count` string COMMENT '当天活跃设备数',
      `login_last_30d_count` string COMMENT '最近30天活跃设备数',
      `order_day_count` bigint COMMENT '当天下单次数',
      `order_day_amount` decimal(16,2) COMMENT '当天下单金额',
      `order_last_30d_count` bigint COMMENT '最近30天下单次数',
      `order_last_30d_amount` decimal(16,2) COMMENT '最近30天下单金额',
      `payment_day_count` bigint COMMENT '当天支付次数',
      `payment_day_amount` decimal(16,2) COMMENT '当天支付金额',
      `payment_last_30d_count` bigint COMMENT '最近30天支付次数',
      `payment_last_30d_amount` decimal(16,2) COMMENT '最近30天支付金额'
  ) COMMENT '地区主题宽表'
  row format delimited fields terminated by '\t'
  location '/warehouse/gmall/dwt/dwt_area_topic/'
  tblproperties ("parquet.compression"="lzo");
  ```

- 数据插入

  ```mysql
  insert overwrite table dwt_area_topic
  select
      nvl(old.id,new.id),
      nvl(old.province_name,new.province_name),
      nvl(old.area_code,new.area_code),
      nvl(old.iso_code,new.iso_code),
      nvl(old.region_id,new.region_id),
      nvl(old.region_name,new.region_name),
      nvl(new.login_day_count,0),
      nvl(new.login_last_30d_count,0),
      nvl(new.order_day_count,0),
      nvl(new.order_day_amount,0.0),
      nvl(new.order_last_30d_count,0),
      nvl(new.order_last_30d_amount,0.0),
      nvl(new.payment_day_count,0),
      nvl(new.payment_day_amount,0.0),
      nvl(new.payment_last_30d_count,0),
      nvl(new.payment_last_30d_amount,0.0)
  from 
  (
      select
          *
      from dwt_area_topic
  )old
  full outer join
  (
      select
          id,
          province_name,
          area_code,
          iso_code,
          region_id,
          region_name,
          sum(if(dt='2020-06-24',login_count,0)) login_day_count,
          sum(if(dt='2020-06-24',order_count,0)) order_day_count,
          sum(if(dt='2020-06-24',order_amount,0.0)) order_day_amount,
          sum(if(dt='2020-06-24',payment_count,0)) payment_day_count,
          sum(if(dt='2020-06-24',payment_amount,0.0)) payment_day_amount,
          sum(login_count) login_last_30d_count,
          sum(order_count) order_last_30d_count,
          sum(order_amount) order_last_30d_amount,
          sum(payment_count) payment_last_30d_count,
          sum(payment_amount) payment_last_30d_amount
      from dws_area_stats_daycount
      where dt>=date_add('2020-06-24',-29)
      group by id,province_name,area_code,iso_code,region_id,region_name
  )new
  on old.id=new.id;
  ```



### 6.6 DWT层数据脚本

1. 创建脚本

   ```shell
   cd bin/
   vi dws_to_dwt.sh
   ```

   ```shell
   #!/bin/bash
   
   APP=gmall
   hive=/opt/module/hive-3.1.2/bin/hive
   
   if [ -n "$1" ] ;then
       do_date=$1
   else 
       do_date=`date -d "-1 day" +%F`
   fi
   
   sql="
   set mapreduce.job.queuename=hive;
   insert overwrite table ${APP}.dwt_uv_topic
   select
       nvl(new.mid_id,old.mid_id),
       nvl(new.model,old.model),
       nvl(new.brand,old.brand),
       if(old.mid_id is null,'$do_date',old.login_date_first),
       if(new.mid_id is not null,'$do_date',old.login_date_last),
       if(new.mid_id is not null, new.login_count,0),
       nvl(old.login_count,0)+if(new.login_count>0,1,0)
   from
   (
       select
           *
       from ${APP}.dwt_uv_topic
   )old
   full outer join
   (
       select
           *
       from ${APP}.dws_uv_detail_daycount
       where dt='$do_date'
   )new
   on old.mid_id=new.mid_id;
   
   insert overwrite table ${APP}.dwt_user_topic
   select
       nvl(new.user_id,old.user_id),
       if(old.login_date_first is null and new.login_count>0,'$do_date',old.login_date_first),
       if(new.login_count>0,'$do_date',old.login_date_last),
       nvl(old.login_count,0)+if(new.login_count>0,1,0),
       nvl(new.login_last_30d_count,0),
       if(old.order_date_first is null and new.order_count>0,'$do_date',old.order_date_first),
       if(new.order_count>0,'$do_date',old.order_date_last),
       nvl(old.order_count,0)+nvl(new.order_count,0),
       nvl(old.order_amount,0)+nvl(new.order_amount,0),
       nvl(new.order_last_30d_count,0),
       nvl(new.order_last_30d_amount,0),
       if(old.payment_date_first is null and new.payment_count>0,'$do_date',old.payment_date_first),
       if(new.payment_count>0,'$do_date',old.payment_date_last),
       nvl(old.payment_count,0)+nvl(new.payment_count,0),
       nvl(old.payment_amount,0)+nvl(new.payment_amount,0),
       nvl(new.payment_last_30d_count,0),
       nvl(new.payment_last_30d_amount,0)
   from
   ${APP}.dwt_user_topic old
   full outer join
   (
       select
           user_id,
           sum(if(dt='$do_date',login_count,0)) login_count,
           sum(if(dt='$do_date',order_count,0)) order_count,
           sum(if(dt='$do_date',order_amount,0)) order_amount,
           sum(if(dt='$do_date',payment_count,0)) payment_count,
           sum(if(dt='$do_date',payment_amount,0)) payment_amount,
           sum(if(login_count>0,1,0)) login_last_30d_count,
           sum(order_count) order_last_30d_count,
           sum(order_amount) order_last_30d_amount,
           sum(payment_count) payment_last_30d_count,
           sum(payment_amount) payment_last_30d_amount
       from ${APP}.dws_user_action_daycount
       where dt>=date_add( '$do_date',-29)
       group by user_id
   )new
   on old.user_id=new.user_id;
   
   insert overwrite table ${APP}.dwt_sku_topic
   select 
       nvl(new.sku_id,old.sku_id),
       sku_info.spu_id,
       nvl(new.order_count30,0),
       nvl(new.order_num30,0),
       nvl(new.order_amount30,0),
       nvl(old.order_count,0) + nvl(new.order_count,0),
       nvl(old.order_num,0) + nvl(new.order_num,0),
       nvl(old.order_amount,0) + nvl(new.order_amount,0),
       nvl(new.payment_count30,0),
       nvl(new.payment_num30,0),
       nvl(new.payment_amount30,0),
       nvl(old.payment_count,0) + nvl(new.payment_count,0),
       nvl(old.payment_num,0) + nvl(new.payment_count,0),
       nvl(old.payment_amount,0) + nvl(new.payment_count,0),
       nvl(new.refund_count30,0),
       nvl(new.refund_num30,0),
       nvl(new.refund_amount30,0),
       nvl(old.refund_count,0) + nvl(new.refund_count,0),
       nvl(old.refund_num,0) + nvl(new.refund_num,0),
       nvl(old.refund_amount,0) + nvl(new.refund_amount,0),
       nvl(new.cart_count30,0),
       nvl(old.cart_count,0) + nvl(new.cart_count,0),
       nvl(new.favor_count30,0),
       nvl(old.favor_count,0) + nvl(new.favor_count,0),
       nvl(new.appraise_good_count30,0),
       nvl(new.appraise_mid_count30,0),
       nvl(new.appraise_bad_count30,0),
       nvl(new.appraise_default_count30,0)  ,
       nvl(old.appraise_good_count,0) + nvl(new.appraise_good_count,0),
       nvl(old.appraise_mid_count,0) + nvl(new.appraise_mid_count,0),
       nvl(old.appraise_bad_count,0) + nvl(new.appraise_bad_count,0),
       nvl(old.appraise_default_count,0) + nvl(new.appraise_default_count,0) 
   from 
   (
       select
           sku_id,
           spu_id,
           order_last_30d_count,
           order_last_30d_num,
           order_last_30d_amount,
           order_count,
           order_num,
           order_amount  ,
           payment_last_30d_count,
           payment_last_30d_num,
           payment_last_30d_amount,
           payment_count,
           payment_num,
           payment_amount,
           refund_last_30d_count,
           refund_last_30d_num,
           refund_last_30d_amount,
           refund_count,
           refund_num,
           refund_amount,
           cart_last_30d_count,
           cart_count,
           favor_last_30d_count,
           favor_count,
           appraise_last_30d_good_count,
           appraise_last_30d_mid_count,
           appraise_last_30d_bad_count,
           appraise_last_30d_default_count,
           appraise_good_count,
           appraise_mid_count,
           appraise_bad_count,
           appraise_default_count 
       from ${APP}.dwt_sku_topic
   )old
   full outer join 
   (
       select 
           sku_id,
           sum(if(dt='$do_date', order_count,0 )) order_count,
           sum(if(dt='$do_date',order_num ,0 ))  order_num, 
           sum(if(dt='$do_date',order_amount,0 )) order_amount ,
           sum(if(dt='$do_date',payment_count,0 )) payment_count,
           sum(if(dt='$do_date',payment_num,0 )) payment_num,
           sum(if(dt='$do_date',payment_amount,0 )) payment_amount,
           sum(if(dt='$do_date',refund_count,0 )) refund_count,
           sum(if(dt='$do_date',refund_num,0 )) refund_num,
           sum(if(dt='$do_date',refund_amount,0 )) refund_amount,  
           sum(if(dt='$do_date',cart_count,0 )) cart_count,
           sum(if(dt='$do_date',favor_count,0 )) favor_count,
           sum(if(dt='$do_date',appraise_good_count,0 )) appraise_good_count,  
           sum(if(dt='$do_date',appraise_mid_count,0 ) ) appraise_mid_count ,
           sum(if(dt='$do_date',appraise_bad_count,0 )) appraise_bad_count,  
           sum(if(dt='$do_date',appraise_default_count,0 )) appraise_default_count,
           sum(order_count) order_count30 ,
           sum(order_num) order_num30,
           sum(order_amount) order_amount30,
           sum(payment_count) payment_count30,
           sum(payment_num) payment_num30,
           sum(payment_amount) payment_amount30,
           sum(refund_count) refund_count30,
           sum(refund_num) refund_num30,
           sum(refund_amount) refund_amount30,
           sum(cart_count) cart_count30,
           sum(favor_count) favor_count30,
           sum(appraise_good_count) appraise_good_count30,
           sum(appraise_mid_count) appraise_mid_count30,
           sum(appraise_bad_count) appraise_bad_count30,
           sum(appraise_default_count) appraise_default_count30 
       from ${APP}.dws_sku_action_daycount
       where dt >= date_add ('$do_date', -29)
       group by sku_id    
   )new 
   on new.sku_id = old.sku_id
   left join 
   (select * from ${APP}.dwd_dim_sku_info where dt='$do_date') sku_info
   on nvl(new.sku_id,old.sku_id)= sku_info.id;
   
   insert overwrite table ${APP}.dwt_activity_topic
   select
       nvl(new.id,old.id),
       nvl(new.activity_name,old.activity_name),
       nvl(new.activity_type,old.activity_type),
       nvl(new.start_time,old.start_time),
       nvl(new.end_time,old.end_time),
       nvl(new.create_time,old.create_time),
       nvl(new.display_count,0),
       nvl(new.order_count,0),
       nvl(new.order_amount,0.0),
       nvl(new.payment_count,0),
       nvl(new.payment_amount,0.0),
       nvl(new.display_count,0)+nvl(old.display_count,0),
       nvl(new.order_count,0)+nvl(old.order_count,0),
       nvl(new.order_amount,0.0)+nvl(old.order_amount,0.0),
       nvl(new.payment_count,0)+nvl(old.payment_count,0),
       nvl(new.payment_amount,0.0)+nvl(old.payment_amount,0.0)
   from
   (
       select
           *
       from ${APP}.dwt_activity_topic
   )old
   full outer join
   (
       select
           *
       from ${APP}.dws_activity_info_daycount
       where dt='$do_date'
   )new
   on old.id=new.id;
   
   insert overwrite table ${APP}.dwt_area_topic
   select
       nvl(old.id,new.id),
       nvl(old.province_name,new.province_name),
       nvl(old.area_code,new.area_code),
       nvl(old.iso_code,new.iso_code),
       nvl(old.region_id,new.region_id),
       nvl(old.region_name,new.region_name),
       nvl(new.login_day_count,0),
       nvl(new.login_last_30d_count,0),
       nvl(new.order_day_count,0),
       nvl(new.order_day_amount,0.0),
       nvl(new.order_last_30d_count,0),
       nvl(new.order_last_30d_amount,0.0),
       nvl(new.payment_day_count,0),
       nvl(new.payment_day_amount,0.0),
       nvl(new.payment_last_30d_count,0),
       nvl(new.payment_last_30d_amount,0.0)
   from 
   (
       select
           *
       from ${APP}.dwt_area_topic
   )old
   full outer join
   (
       select
           id,
           province_name,
           area_code,
           iso_code,
           region_id,
           region_name,
           sum(if(dt='$do_date',login_count,0)) login_day_count,
           sum(if(dt='$do_date',order_count,0)) order_day_count,
           sum(if(dt='$do_date',order_amount,0.0)) order_day_amount,
           sum(if(dt='$do_date',payment_count,0)) payment_day_count,
           sum(if(dt='$do_date',payment_amount,0.0)) payment_day_amount,
           sum(login_count) login_last_30d_count,
           sum(order_count) order_last_30d_count,
           sum(order_amount) order_last_30d_amount,
           sum(payment_count) payment_last_30d_count,
           sum(payment_amount) payment_last_30d_amount
       from ${APP}.dws_area_stats_daycount
       where dt>=date_add('$do_date',-29)
       group by id,province_name,area_code,iso_code,region_id,region_name
   )new
   on old.id=new.id;
   "
   
   $hive -e "$sql"
   ```

2. 添加执行权限

   ```shell
   chmod 777 dws_to_dwt.sh
   ```

3. 执行脚本

   ```shell
   dws_to_dwt.sh 2020-06-24
   ```



## 第七章 ADS层搭建

### 7.1 数据生成

### 7.2 设备主题

#### 7.2.1 活跃设备数

- 说明

  1. 概述：以日期维度统计当日、周、月的活跃用户数量指标
  2. 数据来源：dwt层的设备主题宽表

- 建表语句

  ```mysql
  use gmall;
  drop table if exists ads_uv_count;
  create external table ads_uv_count(
      `dt` string COMMENT '统计日期',
      `day_count` bigint COMMENT '当日用户数量',
      `wk_count`  bigint COMMENT '当周用户数量',
      `mn_count`  bigint COMMENT '当月用户数量',
      `is_weekend` string COMMENT 'Y,N是否是周末,用于得到本周最终结果',
      `is_monthend` string COMMENT 'Y,N是否是月末,用于得到本月最终结果' 
  ) COMMENT '活跃设备数'
  row format delimited fields terminated by '\t'
  location '/warehouse/gmall/ads/ads_uv_count/';
  ```

- 插入数据

  ```mysql
  insert into table ads_uv_count
  select
      '2020-06-24',
      sum(if(login_date_last='2020-06-24',1,0)),
      sum(if(login_date_last>=date_add(next_day('2020-06-24','Monday'),-7),1,0)),
      sum(if(date_format(login_date_last,'yyyy-MM')=date_format('2020-06-24','yyyy-MM'),1,0)),
      if('2020-06-24'=date_add(next_day('2020-06-24','Monday'),-1),'Y','N'),
      if('2020-06-24'=last_day('2020-06-24'),'Y','N')
  from dwt_uv_topic
  ```



#### 7.2.2 每日新增设备数

- 说明

  1. 概述：以日期维度统计当天新增的设备数量
  2. 数据来源：dwt层的设备主题宽表

- 建表语句

  ```mysql
  use gmall;
  drop table if exists ads_new_mid_count;
  create external table ads_new_mid_count
  (
      `create_date`     string comment '创建时间' ,
      `new_mid_count`   BIGINT comment '新增设备数量' 
  )  COMMENT '每日新增设备信息数量'
  row format delimited fields terminated by '\t'
  location '/warehouse/gmall/ads/ads_new_mid_count/';
  ```

- 插入数据

  ```mysql
  insert into table ads_new_mid_count 
  select
      '2020-06-24',
      count(*)
  from dwt_uv_topic
  where login_date_first='2020-06-24';
  ```



#### 7.2.3 设备留存率

- 说明

  1. 概述：以日期+留存天数维度统计每天新增用户的一日、两日、三日留存率
  2. 数据来源：dwt层的设备主题宽表
  3. 详述：
     - 对于某一天新增的所有用户，应在表中存有3行记录，分别对应一日、两日、三日留存率
     - 一日留存率指某天的所有新增用户中，第二天仍有启动记录的用户所占的比例
     - 两日留存率指某天的所有新增用户中，第三天仍有启动记录的用户所占的比例

- 建表语句

  ```mysql
  use gmall;
  drop table if exists ads_user_retention_day_rate;
  create external table ads_user_retention_day_rate 
  (
       `stat_date`          string comment '统计日期',
       `create_date`       string  comment '设备新增日期',
       `retention_day`     int comment '截止当前日期留存天数',
       `retention_count`    bigint comment  '留存数量',
       `new_mid_count`     bigint comment '设备新增数量',
       `retention_ratio`   decimal(16,2) comment '留存率'
  )  COMMENT '每日用户留存情况'
  row format delimited fields terminated by '\t'
  location '/warehouse/gmall/ads/ads_user_retention_day_rate/';
  ```

- 插入数据

  ```mysql
  insert into table ads_user_retention_day_rate
  select 
      '2020-06-24' stat_date
      date_add('2020-06-24',-1) create_date,
      1 retention_day,
      sum(if(login_date_last='2020-06-24',1,0)) retention_count,
      count(*) new_mid_count,
      round((sum(if(login_date_last='2020-06-24',1,0))/count(*)),2) retention_ratio
  from dwt_uv_topic
  where login_date_first = date_add('2020-06-24',-1)
  
  union all
  
  select 
      '2020-06-24' stat_date
      date_add('2020-06-24',-2) create_date,
      2 retention_day,
      sum(if(login_date_last='2020-06-24',1,0)) retention_count,
      count(*) new_mid_count,
      round((sum(if(login_date_last='2020-06-24',1,0))/count(*)),2) retention_ratio
  from dwt_uv_topic
  where login_date_first = date_add('2020-06-24',-2)
  
  union all
  
  select 
      '2020-06-24' stat_date
      date_add('2020-06-24',-3) create_date,
      3 retention_day,
      sum(if(login_date_last='2020-06-24',1,0)) retention_count,
      count(*) new_mid_count,
      round((sum(if(login_date_last='2020-06-24',1,0))/count(*)),2) retention_ratio
  from dwt_uv_topic
  where login_date_first = date_add('2020-06-24',-3);
  ```



#### 7.2.4 沉默用户数

- 说明

  1. 概述：以日期维度统计当天的沉默设备数量
  2. 数据来源：dwt层的设备主题宽表
  3. 详述：仅有首次登陆记录且近7天没有登录记录的用户即为沉默用户

- 建表语句

  ```mysql
  use gmall;
  drop table if exists ads_silent_count;
  create external table ads_silent_count( 
      `dt` string COMMENT '统计日期',
      `silent_count` bigint COMMENT '沉默设备数'
  ) 
  row format delimited fields terminated by '\t'
  location '/warehouse/gmall/ads/ads_silent_count';
  ```

- 插入数据

  ```mysql
  insert into table ads_silent_count
  select
      '2020-06-24',
      count(*) 
  from dwt_uv_topic
  where login_date_first=login_date_last
  and login_date_last<=date_add('2020-06-24',-7);
  ```



#### 7.2.5 本周回流用户数

- 说明

  1. 概述：以日期维度统计当周的回流用户总数
  2. 数据来源：dws层的每日设备行为表和dwt层的设备主题宽表
  3. 详述：上周没有启动记录、本周有启动记录的非本周新增用户，即为本周回流用户

- 建表语句

  ```mysql
  use gmall;
  drop table if exists ads_back_count;
  create external table ads_back_count( 
      `dt` string COMMENT '统计日期',
      `wk_dt` string COMMENT '统计日期所在周',
      `wastage_count` bigint COMMENT '回流设备数'
  ) 
  row format delimited fields terminated by '\t'
  location '/warehouse/gmall/ads/ads_back_count';
  ```

- 插入数据

  ```mysql
  insert into table ads_back_count
  select
      '2020-06-24',
      concat(date_add(next_day('2020-06-24','Monday'),-7),'_',date_add(next_day('2020-06-24','Monday'),-1)),
      count(*)
  from dwt_uv_topic
  where login_date_first < date_add(next_day('2020-06-24','Monday'),-7) 
  and login_date_last >= date_add(next_day('2020-06-24','Monday'),-7) 
  and mid_id not in 
  (
      select mid_id
      from dws_uv_detail_daycount
      where dt >= date_add(next_day('2020-06-24','Monday'),-14)
      and dt < date_add(next_day('2020-06-24','Monday'),-7)
  ) t; 
  ```



#### 7.2.6 流失用户数

- 说明

  1. 概述：以时间维度统计截至当天的流式用户总数
  2. 数据来源：dwt层的设备主题宽表
  3. 详述：近七天无启动记录的用户即为流失用户

- 建表语句

  ```mysql
  use gmall;
  drop table if exists ads_wastage_count;
  create external table ads_wastage_count( 
      `dt` string COMMENT '统计日期',
      `wastage_count` bigint COMMENT '流失设备数'
  ) 
  row format delimited fields terminated by '\t'
  location '/warehouse/gmall/ads/ads_wastage_count';
  ```

- 插入数据

  ```mysql
  insert into table ads_wastage_count
  select 
      '2020-06-24',
      count(*)
  from dwt_uv_topic
  where login_date_last <= date_add('2020-06-24',-7);
  ```



#### 7.2.7 最近连续三周活跃用户数

- 说明

  - 概述：以时间维度统计连续三周活跃用户数
  - 数据来源：dws层的每日设备行为表和dwt层的设备主题宽表
  - 详述：本周、上周、上上周都有启动记录的用户即为连续三周活跃用户

- 建表语句

  ```mysql
  use gmall;
  drop table if exists ads_continuity_wk_count;
  create external table ads_continuity_wk_count( 
      `dt` string COMMENT '统计日期,一般用结束周周日日期,如果每天计算一次,可用当天日期',
      `wk_dt` string COMMENT '持续时间',
      `continuity_count` bigint COMMENT '活跃次数'
  ) 
  row format delimited fields terminated by '\t'
  location '/warehouse/gmall/ads/ads_continuity_wk_count';
  ```

- 插入数据

  ```mysql
  insert into table ads_continuity_wk_count
  select
      '2020-06-24',
      concat(date_add(next_day('2020-06-24','MO'),-7*3),'_',date_add(next_day('2020-06-24','MO'),-1)),
      count(*)
  from
  (
      select
          mid_id
      from
      (
          select
              mid_id
          from dws_uv_detail_daycount
          where dt>=date_add(next_day('2020-06-24','monday'),-7)
          and dt<=date_add(next_day('2020-06-24','monday'),-1)
          group by mid_id
  
          union all
  
          select
              mid_id
          from dws_uv_detail_daycount
          where dt>=date_add(next_day('2020-06-24','monday'),-7*2)
          and dt<=date_add(next_day('2020-06-24','monday'),-7-1)
          group by mid_id
  
          union all
  
          select
              mid_id
          from dws_uv_detail_daycount
          where dt>=date_add(next_day('2020-06-24','monday'),-7*3)
          and dt<=date_add(next_day('2020-06-24','monday'),-7*2-1)
          group by mid_id
      )t1
      group by mid_id
      having count(*)=3
  )t2;
  ```



#### 7.2.8 最近7天内连续3天活跃用户数

- 说明

  1. 概述：以日期维度统计近7天内，有连续3天及以上启动记录的用户数量
  2. 数据来源：dws层的每日设备行为表
  3. 详述
     - 首先拉取七天内的设备启动数据，得到mid_id，dt字段组成的表
     - 通过窗口函数添加row_number列，根据dt - row_num是否一致来判断是否为连续启动行为
     - 对结果一致的数据分组计数，筛选出数量大于3的mid_id

- 建表语句

  ```mysql
  use gmall;
  drop table if exists ads_continuity_uv_count;
  create external table ads_continuity_uv_count( 
      `dt` string COMMENT '统计日期',
      `wk_dt` string COMMENT '最近7天日期',
      `continuity_count` bigint
  ) COMMENT '连续活跃设备数'
  row format delimited fields terminated by '\t'
  location '/warehouse/gmall/ads/ads_continuity_uv_count';
  ```

- 插入数据

  ```mysql
  select
      '2020-06-24',
      concat(date_add(next_day('2020-06-24','Monday'),-7),'_',date_add(next_day('2020-06-24','Monday'),-1)),
      count(*)
  from
  (
      select
          mid_id
      from
      (
          select
              mid_id,
              count(*) days_cnt
          from
          (
              select
                  mid_id,
                  date_sub(dt,rn) date_dif
              from
              (
                  select
                      mid_id,
                      dt,
                      row_number() over(partition by mid_id order by dt) rn
                  from dws_uv_detail_daycount
                  where dt > date_add('2020-06-24',-7)
                  group by mid_id,dt
              ) t1
          ) t2
          group by mid_id,date_dif
          having days_cnt >= 3
      ) t3
      group by mid_id
  ) t4
  ```



### 7.3 会员主题

#### 7.3.1 会员主题信息

- 说明

  1. 概述：以日期维度统计会员相关的多个指标数据
  2. 数据来源：dwt层的会员主题宽表
  3. 详述：会员新鲜度指当天新增会员占当天活跃会员的比例

- 建表语句

  ```mysql
  use gmall;
  drop table if exists ads_user_topic;
  create external table ads_user_topic(
      `dt` string COMMENT '统计日期',
      `day_users` string COMMENT '活跃会员数',
      `day_new_users` string COMMENT '新增会员数',
      `day_new_payment_users` string COMMENT '新增消费会员数',
      `payment_users` string COMMENT '总付费会员数',
      `users` string COMMENT '总会员数',
      `day_users2users` decimal(16,2) COMMENT '会员活跃率',
      `payment_users2users` decimal(16,2) COMMENT '会员付费率',
      `day_new_users2users` decimal(16,2) COMMENT '会员新鲜度'
  ) COMMENT '会员主题信息表'
  row format delimited fields terminated by '\t'
  location '/warehouse/gmall/ads/ads_user_topic';
  ```

- 插入数据

  ```mysql
  insert into table ads_user_topic
  select
      '2020-06-14',
      sum(if(login_date_last='2020-06-14',1,0)),
      sum(if(login_date_first='2020-06-14',1,0)),
      sum(if(payment_date_first='2020-06-14',1,0)),
      sum(if(payment_count>0,1,0)),
      count(*),
      sum(if(login_date_last='2020-06-14',1,0))/count(*),
      sum(if(payment_count>0,1,0))/count(*),
      sum(if(login_date_first='2020-06-14',1,0))/sum(if(login_date_last='2020-06-14',1,0))
  from dwt_user_topic;
  ```



#### 7.3.2 漏斗分析

- 说明

  1. 概述：以日期维度统计多个页面分级间的访问转化率，用以分析用户行为
  2. 数据来源：dwd层的页面日志表和dws层的每日会员行为表
  3. 详述
     - 统计首页和商品详情页的日访问用户数时，以设备id作为计数指标
     - 首页的 page_id 为'home'，商品详情页的 page_id 为'good_detail'

- 建表语句

  ```mysql
  use gmall;
  drop table if exists ads_user_action_convert_day;
  create external  table ads_user_action_convert_day(
      `dt` string COMMENT '统计日期',
      `home_count`  bigint COMMENT '浏览首页人数',
      `good_detail_count` bigint COMMENT '浏览商品详情页人数',
      `home2good_detail_convert_ratio` decimal(16,2) COMMENT '首页到商品详情转化率',
      `cart_count` bigint COMMENT '加入购物车的人数',
      `good_detail2cart_convert_ratio` decimal(16,2) COMMENT '商品详情页到加入购物车转化率',
      `order_count` bigint COMMENT '下单人数',
      `cart2order_convert_ratio` decimal(16,2) COMMENT '加入购物车到下单转化率',
      `payment_amount` bigint COMMENT '支付人数',
      `order2payment_convert_ratio` decimal(16,2) COMMENT '下单到支付的转化率'
   ) COMMENT '用户行为漏斗分析'
  row format delimited  fields terminated by '\t'
  location '/warehouse/gmall/ads/ads_user_action_convert_day/';
  ```

- 插入数据

  ```mysql
  with t1 as
  (
      select
          '2020-06-24' dt,
          sum(if(array_contains(page_set,'home'),1,0)) home_count,
          sum(if(array_contains(page_set,'good_detail'),1,0)) good_detail_count
      from 
      (
          select
              mid_id,
              collect_set(page_id) page_set
          from dwd_page_log
          where dt = '2020-06-24'
          group by mid_id
      ) tmp
  ),
  t2 as
  (
      select
          '2020-06-24' dt, 
          sum(if(order_count>0,1,0)) order_count,
          sum(if(payment_count>0,1,0)) payment_amount
      from dws_user_action_daycount
      where dt = '2020-06-24'   
  )
  insert into table ads_user_action_convert_day
  select
      '2020-06-24',
      t1.home_count,
      t1.good_detail_count,
      t1.good_detail_count/t1.home_count,
      t2.order_count,
      t2.order_count/t1.good_detail_count,
      t2.payment_amount,
      t2.payment_amount/t2.order_count
  from t1,t2
  ```



### 7.4 商品主题

#### 7.4.1 商品个数信息

- 说明

  1. 概述：以日期维度统计每日的sku总数及spu总数
  2. 数据来源：dwt层的商品主题宽表
  3. 详述：不论当日是否有销量，在售商品均要纳入统计范围

- 建表语句

  ```mysql
  use gmall;
  drop table if exists ads_product_info;
  create external table ads_product_info(
      `dt` string COMMENT '统计日期',
      `sku_num` string COMMENT 'sku个数',
      `spu_num` string COMMENT 'spu个数'
  ) COMMENT '商品个数信息'
  row format delimited fields terminated by '\t'
  location '/warehouse/gmall/ads/ads_product_info';
  ```

- 插入数据

  ```mysql
  with t1 as
  (
      select count(*) sku_num
      from dwt_sku_topic
  ),
  t2 as
  (
      select count(*) spu_num
      from
      (
          select spu_id
          from dwt_sku_topic
          group by spu_id
      )tmp
  )
  insert into table ads_product_info
  select
      '2020-06-24',
      t1.sku_num,
      t2.spu_num
  from t1,t2;
  ```



#### 7.4.2 商品销量排名

- 说明

  1. 概述：以日期维度统计当日的商品销量排名top10
  2. 数据来源： dws层的每日商品行为表
  3. 详述：此处的销量指标指商品的总销售金额

- 建表语句

  ```mysql
  use gmall;
  drop table if exists ads_product_sale_topN;
  create external table ads_product_sale_topN(
      `dt` string COMMENT '统计日期',
      `sku_id` string COMMENT '商品ID',
      `payment_amount` bigint COMMENT '销量'
  ) COMMENT '商品个数信息'
  row format delimited fields terminated by '\t'
  location '/warehouse/gmall/ads/ads_product_sale_topN';
  ```

- 插入数据

  ```mysql
  insert into ads_product_sale_topN
  select
      '2020-06-24'
      sku_id,
      payment_amount
  from dws_sku_action_daycount
  where dt = '2020-06-24'
  order by payment_amount desc
  limit 10;
  ```



#### 7.4.3 商品收藏排名

- 说明

  1. 概述：以日期维度统计当日被最多用户添加到收藏的商品排名top10
  2. 数据来源：dws层的每日商品行为表

- 建表语句

  ```mysql
  use gmall;
  drop table if exists ads_product_favor_topN;
  create external table ads_product_favor_topN(
      `dt` string COMMENT '统计日期',
      `sku_id` string COMMENT '商品ID',
      `favor_count` bigint COMMENT '收藏量'
  ) COMMENT '商品收藏TopN'
  row format delimited fields terminated by '\t'
  location '/warehouse/gmall/ads/ads_product_favor_topN';
  ```

- 插入数据

  ```mysql
  insert into ads_product_favor_topN
  select
      '2020-06-24',
      sku_id,
      favor_count
  from dws_sku_action_daycount
  where dt = '2020-06-24'
  order by favor_count desc
  limit 10;
  ```



#### 7.4.4 商品加入购物车排名

- 说明

  1. 概述：以日期维度统计当日被最多用户添加到购物车的商品top10
  2. 数据来源：dws层的每日商品行为表

- 建表语句

  ```mysql
  use gmall;
  drop table if exists ads_product_cart_topN;
  create external table ads_product_cart_topN(
      `dt` string COMMENT '统计日期',
      `sku_id` string COMMENT '商品ID',
      `cart_count` bigint COMMENT '加入购物车次数'
  ) COMMENT '商品加入购物车TopN'
  row format delimited fields terminated by '\t'
  location '/warehouse/gmall/ads/ads_product_cart_topN';
  ```

- 插入数据

  ```mysql
  insert into ads_product_cart_topN
  select
      '2020-06-24',
      sku_id,
      cart_count
  from dws_sku_action_daycount
  where dt = '2020-06-24'
  order by cart_count desc
  limit 10;
  ```



#### 7.4.5 商品退款率排名

- 说明

  1. 概述：以日期维度统计近30天内退款率最高的商品top10
  2. 数据来源：dwt层的商品主题宽表
  3. 详述：此处统计中不需要追踪具体的退款订单，只需要统计近30天的退款与支付次数之比

- 建表语句

  ```mysql
  use gmall;
  drop table if exists ads_product_refund_topN;
  create external table ads_product_refund_topN(
      `dt` string COMMENT '统计日期',
      `sku_id` string COMMENT '商品ID',
      `refund_ratio` decimal(16,2) COMMENT '退款率'
  ) COMMENT '商品退款率TopN'
  row format delimited fields terminated by '\t'
  location '/warehouse/gmall/ads/ads_product_refund_topN';
  ```

- 插入数据

  ```mysql
  insert into table ads_product_refund_topN
  select
      '2020-06-24',
      sku_id,
      nvl(refund_last_30d_count/payment_last_30d_count,0) refund_ratio
  from dwt_sku_topic
  order by refund_ratio
  limit 10;
  ```



#### 7.4.6 商品差评率排名

- 说明

  1. 概述：以时间维度统计当天的新增评论中，差评数与评论总数之比top10
  2. 数据来源：dws层的每日商品行为表

- 建表语句

  ```mysql
  use gmall;
  drop table if exists ads_appraise_bad_topN;
  create external table ads_appraise_bad_topN(
      `dt` string COMMENT '统计日期',
      `sku_id` string COMMENT '商品ID',
      `appraise_bad_ratio` decimal(16,2) COMMENT '差评率'
  ) COMMENT '商品差评率TopN'
  row format delimited fields terminated by '\t'
  location '/warehouse/gmall/ads/ads_appraise_bad_topN';
  ```

- 插入数据

  ```mysql
  insert into ads_product_cart_topN
  select
      '2020-06-24',
      sku_id,
      nvl(appraise_bad_count/(appraise_bad_count+appraise_mid_count+appraise_good_count+appraise_default_count),0) appraise_bad_ratio
  from dws_sku_action_daycount
  where dt = '2020-06-24'
  order by appraise_bad_ratio desc
  limit 10;
  ```



### 7.5 营销主题

#### 7.5.1 下单数目统计

- 说明

  1. 概述：以日期维度统计每日下单数据 
  2. 数据来源：dws层的每日用户行为表

- 建表语句

  ```mysql
  use gmall;
  drop table if exists ads_order_daycount;
  create external table ads_order_daycount(
      dt string comment '统计日期',
      order_count bigint comment '单日下单笔数',
      order_amount bigint comment '单日下单金额',
      order_users bigint comment '单日下单用户数'
  ) comment '每日订单总计表'
  row format delimited fields terminated by '\t'
  location '/warehouse/gmall/ads/ads_order_daycount';
  ```

- 插入数据

  ```mysql
  insert into table ads_order_daycount
  select
      '2020-06-24',
      sum(nvl(order_count,0)),
      sum(nvl(payment_amount,0)),
      sum(if(order_count>0,1,0))
  from dws_user_action_daycount
  where dt = '2020-06-24';
  ```



#### 7.5.2 支付信息统计

- 说明

  1. 概述：以日期维度统计每日的支付指标数据
  2. 数据来源：dws层的每日用户行为表、每日商品行为表、dwd层的订单事实表

- 建表语句

  ```mysql
  use gmall;
  drop table if exists ads_payment_daycount;
  create external table ads_payment_daycount(
      dt string comment '统计日期',
      payment_count bigint comment '单日支付笔数',
      payment_amount bigint comment '单日支付金额',
      payment_user_count bigint comment '单日支付人数',
      payment_sku_count bigint comment '单日支付商品数',
      payment_avg_time decimal(16,2) comment '下单到支付的平均时长，取分钟数'
  ) comment '每日订单总计表'
  row format delimited fields terminated by '\t'
  location '/warehouse/gmall/ads/ads_payment_daycount';
  ```

- 插入数据

  ```mysql
  with t1 as
  (
      select
          '2020-06-24' dt,
          sum(nvl(payment_count,0)) order_count,
          sum(nvl(payment_amount,0)) order_amount,
          sum(if(payment_count>0,1,0)) payment_user_count
      from dws_user_action_daycount
      where dt = '2020-06-24'
  ),
  t2 as
  (
      select
          '2020-06-24' dt,
          count(*) payment_sku_count
      from dws_sku_action_daycount
      where dt = '2020-06-24'
      and payment_num > 0
  ),
  t3 as
  (
      select
          '2020-06-24' dt,
          avg(unix_timestamp(payment_time)-unix_timestamp(create_time))/60 payment_avg_time
      from dwd_fact_order_info
      where dt = '2020-06-24'
      and payment_time is not null
  )
  insert into table ads_payment_daycount
  select
      '2020-06-24',
      t1.order_count,
      t1.order_amount,
      t1.payment_user_count,
      t2.payment_sku_count,
      t3.payment_avg_time
  from t1 
  join t2 on t1.dt = t2.dt
  join t3 on t1.dt = t3.dt;
  ```



#### 7.5.3 品牌复购率

- 说明

  1. 概述：以日期+品牌+一级品类id维度统计当月的品牌复购率
  2. 数据来源：dwd层的订单明细表和商品维度表
  3. 详述
     - 最终粒度为日期+品牌+一级品类id
     - 复购数指当月内重复购买同一品牌下同一品类产品的订单数，一个订单中有2种符合要求的产品时，算做2次购买。

- 建表语句

  ```mysql
  use gmall;
  drop table ads_sale_tm_category1_stat_mn;
  create external table ads_sale_tm_category1_stat_mn
  (  
      tm_id string comment '品牌id',
      category1_id string comment '1级品类id ',
      category1_name string comment '1级品类名称 ',
      buycount   bigint comment  '购买人数',
      buy_twice_last bigint  comment '两次以上购买人数',
      buy_twice_last_ratio decimal(16,2)  comment  '单次复购率',
      buy_3times_last   bigint comment   '三次以上购买人数',
      buy_3times_last_ratio decimal(16,2)  comment  '多次复购率',
      stat_mn string comment '统计月份',
      stat_date string comment '统计日期' 
  )   COMMENT '复购率统计'
  row format delimited fields terminated by '\t'
  location '/warehouse/gmall/ads/ads_sale_tm_category1_stat_mn/';
  ```

- 插入数据

  ```mysql
  insert into table ads_sale_tm_category1_stat_mn
  select
      t3.tm_id,
      t3.category1_id,
      t3.category1_name,        
      sum(if(t3.tm_by_times>0,1,0)),
      sum(if(t3.tm_by_times>1,1,0)),
      sum(if(t3.tm_by_times>1,1,0))/sum(if(t3.tm_by_times>0,1,0)),
      sum(if(t3.tm_by_times>2,1,0)),
      sum(if(t3.tm_by_times>2,1,0))/sum(if(t3.tm_by_times>0,1,0)),
      date_format('2020-06-24','yyyy-MM'),
      '2020-06-24'
  from
  (
      select
          t2.tm_id,
          t2.category1_id,
          t2.category1_name,
          t1.user_id,
          sum(sku_buy_times) tm_by_times
      from
      (
          select
             sku_id,
             user_id,
             count(*) sku_buy_times
          from dwd_fact_order_detail
          where date_format(dt,'yyyy-MM') = date_format('2020-06-24','yyyy-MM')
          group by sku_id, user_id
      )t1
      left join
      (
          select
              id sku_id,
              tm_id,
              category1_id,
              category1_name
          from dwd_dim_sku_info
          where dt = '2020-06-24'
      )t2
      on t1.sku_id = t2.sku_id
      group by 
          t2.tm_id,
          t2.category1_id,
          t2.category1_name,
          t1.user_id
  )t3
  group by 
      t3.tm_id,
      t3.category1_id,
      t3.category1_name;
  ```



### 7.6 地区主题

#### 7.6.1 地区主题信息

- 说明

  1. 概述：以日期+地区维度统计该地区当日的各项指标数据
  2. 数据来源：dwt层的地区主题宽表

- 建表语句

  ```mysql
  use gmall;
  drop table if exists ads_area_topic;
  create external table ads_area_topic(
      `dt` string COMMENT '统计日期',
      `id` bigint COMMENT '编号',
      `province_name` string COMMENT '省份名称',
      `area_code` string COMMENT '地区编码',
      `iso_code` string COMMENT 'iso编码',
      `region_id` string COMMENT '地区ID',
      `region_name` string COMMENT '地区名称',
      `login_day_count` bigint COMMENT '当天活跃设备数',
      `order_day_count` bigint COMMENT '当天下单次数',
      `order_day_amount` decimal(16,2) COMMENT '当天下单金额',
      `payment_day_count` bigint COMMENT '当天支付次数',
      `payment_day_amount` decimal(16,2) COMMENT '当天支付金额'
  ) COMMENT '地区主题宽表'
  row format delimited fields terminated by '\t'
  location '/warehouse/gmall/ads/ads_area_topic/';
  ```

- 插入数据

  ```mysql
  insert into table ads_area_topic
  select
      '2020-06-14',
      id,
      province_name,
      area_code,
      iso_code,
      region_id,
      region_name,
      login_day_count,
      order_day_count,
      order_day_amount,
      payment_day_count,
      payment_day_amount
  from dwt_area_topic;
  ```



### 7.7 ADS层导入脚本

1. 编写shell脚本

   ```shell
   cd /home/atguigu/bin
   vi dwt_to_ads.sh
   ```

   ```shell
   #!/bin/bash
   
   hive=/opt/module/hive-3.1.2/bin/hive
   APP=gmall
   
   if [ -n "$1" ] ;then
       do_date=$1
   else 
       do_date=`date -d "-1 day" +%F`
   fi
   
   sql="
   set mapreduce.job.queuename=hive;
   insert into table ${APP}.ads_uv_count 
   select  
       '$do_date' dt,
       daycount.ct,
       wkcount.ct,
       mncount.ct,
       if(date_add(next_day('$do_date','MO'),-1)='$do_date','Y','N') ,
       if(last_day('$do_date')='$do_date','Y','N') 
   from 
   (
       select  
           '$do_date' dt,
           count(*) ct
       from ${APP}.dwt_uv_topic
       where login_date_last='$do_date'  
   )daycount join 
   ( 
       select  
           '$do_date' dt,
           count (*) ct
       from ${APP}.dwt_uv_topic
       where login_date_last>=date_add(next_day('$do_date','MO'),-7) 
       and login_date_last<= date_add(next_day('$do_date','MO'),-1) 
   ) wkcount on daycount.dt=wkcount.dt
   join 
   ( 
       select  
           '$do_date' dt,
           count (*) ct
       from ${APP}.dwt_uv_topic
       where date_format(login_date_last,'yyyy-MM')=date_format('$do_date','yyyy-MM')  
   )mncount on daycount.dt=mncount.dt;
   
   insert into table ${APP}.ads_new_mid_count 
   select
       login_date_first,
       count(*)
   from ${APP}.dwt_uv_topic
   where login_date_first='$do_date'
   group by login_date_first;
   
   insert into table ${APP}.ads_silent_count
   select
       '$do_date',
       count(*) 
   from ${APP}.dwt_uv_topic
   where login_date_first=login_date_last
   and login_date_last<=date_add('$do_date',-7);
   
   
   insert into table ${APP}.ads_back_count
   select
   '$do_date',
   concat(date_add(next_day('$do_date','MO'),-7),'_', date_add(next_day('$do_date','MO'),-1)),
       count(*)
   from
   (
       select
           mid_id
       from ${APP}.dwt_uv_topic
       where login_date_last>=date_add(next_day('$do_date','MO'),-7) 
       and login_date_last<= date_add(next_day('$do_date','MO'),-1)
       and login_date_first<date_add(next_day('$do_date','MO'),-7)
   )current_wk
   left join
   (
       select
           mid_id
       from ${APP}.dws_uv_detail_daycount
       where dt>=date_add(next_day('$do_date','MO'),-7*2) 
       and dt<= date_add(next_day('$do_date','MO'),-7-1) 
       group by mid_id
   )last_wk
   on current_wk.mid_id=last_wk.mid_id
   where last_wk.mid_id is null;
   
   insert into table ${APP}.ads_wastage_count
   select
        '$do_date',
        count(*)
   from 
   (
       select 
           mid_id
       from ${APP}.dwt_uv_topic
       where login_date_last<=date_add('$do_date',-7)
       group by mid_id
   )t1;
   
   insert into table ${APP}.ads_user_retention_day_rate
   select
       '$do_date',--统计日期
       date_add('$do_date',-1),--新增日期
       1,--留存天数
       sum(if(login_date_first=date_add('$do_date',-1) and login_date_last='$do_date',1,0)),--$do_date的1日留存数
       sum(if(login_date_first=date_add('$do_date',-1),1,0)),--$do_date新增
       sum(if(login_date_first=date_add('$do_date',-1) and login_date_last='$do_date',1,0))/sum(if(login_date_first=date_add('$do_date',-1),1,0))*100
   from ${APP}.dwt_uv_topic
   
   union all
   
   select
       '$do_date',--统计日期
       date_add('$do_date',-2),--新增日期
       2,--留存天数
       sum(if(login_date_first=date_add('$do_date',-2) and login_date_last='$do_date',1,0)),--$do_date的2日留存数
       sum(if(login_date_first=date_add('$do_date',-2),1,0)),--$do_date新增
       sum(if(login_date_first=date_add('$do_date',-2) and login_date_last='$do_date',1,0))/sum(if(login_date_first=date_add('$do_date',-2),1,0))*100
   from ${APP}.dwt_uv_topic
   
   union all
   
   select
       '$do_date',--统计日期
       date_add('$do_date',-3),--新增日期
       3,--留存天数
       sum(if(login_date_first=date_add('$do_date',-3) and login_date_last='$do_date',1,0)),--$do_date的3日留存数
       sum(if(login_date_first=date_add('$do_date',-3),1,0)),--$do_date新增
       sum(if(login_date_first=date_add('$do_date',-3) and login_date_last='$do_date',1,0))/sum(if(login_date_first=date_add('$do_date',-3),1,0))*100
   from ${APP}.dwt_uv_topic;
   
   
   insert into table ${APP}.ads_continuity_wk_count
   select
       '$do_date',
       concat(date_add(next_day('$do_date','MO'),-7*3),'_',date_add(next_day('$do_date','MO'),-1)),
       count(*)
   from
   (
       select
           mid_id
       from
       (
           select
               mid_id
           from ${APP}.dws_uv_detail_daycount
           where dt>=date_add(next_day('$do_date','monday'),-7)
           and dt<=date_add(next_day('$do_date','monday'),-1)
           group by mid_id
   
           union all
   
           select
               mid_id
           from ${APP}.dws_uv_detail_daycount
           where dt>=date_add(next_day('$do_date','monday'),-7*2)
           and dt<=date_add(next_day('$do_date','monday'),-7-1)
           group by mid_id
   
           union all
   
           select
               mid_id
           from ${APP}.dws_uv_detail_daycount
           where dt>=date_add(next_day('$do_date','monday'),-7*3)
           and dt<=date_add(next_day('$do_date','monday'),-7*2-1)
           group by mid_id
       )t1
       group by mid_id
       having count(*)=3
   )t2;
   
   
   insert into table ${APP}.ads_continuity_uv_count
   select
       '$do_date',
       concat(date_add('$do_date',-6),'_','$do_date'),
       count(*)
   from
   (
       select mid_id
       from
       (
           select mid_id      
           from
           (
               select 
                   mid_id,
                   date_sub(dt,rank) date_dif
               from
               (
                   select 
                       mid_id,
                       dt,
                       rank() over(partition by mid_id order by dt) rank
                   from ${APP}.dws_uv_detail_daycount
                   where dt>=date_add('$do_date',-6) and dt<='$do_date'
               )t1
           )t2 
           group by mid_id,date_dif
           having count(*)>=3
       )t3 
       group by mid_id
   )t4;
   
   
   insert into table ${APP}.ads_user_topic
   select
       '$do_date',
       sum(if(login_date_last='$do_date',1,0)),
       sum(if(login_date_first='$do_date',1,0)),
       sum(if(payment_date_first='$do_date',1,0)),
       sum(if(payment_count>0,1,0)),
       count(*),
       sum(if(login_date_last='$do_date',1,0))/count(*),
       sum(if(payment_count>0,1,0))/count(*),
       sum(if(login_date_first='$do_date',1,0))/sum(if(login_date_last='$do_date',1,0))
   from ${APP}.dwt_user_topic;
   
   with
   tmp_uv as
   (
       select
           '$do_date' dt,
           sum(if(array_contains(pages,'home'),1,0)) home_count,
           sum(if(array_contains(pages,'good_detail'),1,0)) good_detail_count
       from
       (
           select
               mid_id,
               collect_set(page_id) pages
           from ${APP}.dwd_page_log
           where dt='$do_date'
           and page_id in ('home','good_detail')
           group by mid_id
       )tmp
   ),
   tmp_cop as
   (
       select 
           '$do_date' dt,
           sum(if(cart_count>0,1,0)) cart_count,
           sum(if(order_count>0,1,0)) order_count,
           sum(if(payment_count>0,1,0)) payment_count
       from ${APP}.dws_user_action_daycount
       where dt='$do_date'
   )
   insert into table ${APP}.ads_user_action_convert_day
   select
       tmp_uv.dt,
       tmp_uv.home_count,
       tmp_uv.good_detail_count,
       tmp_uv.good_detail_count/tmp_uv.home_count*100,
       tmp_cop.cart_count,
       tmp_cop.cart_count/tmp_uv.good_detail_count*100,
       tmp_cop.order_count,
       tmp_cop.order_count/tmp_cop.cart_count*100,
       tmp_cop.payment_count,
       tmp_cop.payment_count/tmp_cop.order_count*100
   from tmp_uv
   join tmp_cop
   on tmp_uv.dt=tmp_cop.dt;
   
   insert into table ${APP}.ads_product_info
   select
       '$do_date' dt,
       sku_num,
       spu_num
   from
   (
       select
           '$do_date' dt,
           count(*) sku_num
       from
           ${APP}.dwt_sku_topic
   ) tmp_sku_num
   join
   (
       select
           '$do_date' dt,
           count(*) spu_num
       from
       (
           select
               spu_id
           from
               ${APP}.dwt_sku_topic
           group by
               spu_id
       ) tmp_spu_id
   ) tmp_spu_num
   on
       tmp_sku_num.dt=tmp_spu_num.dt;
   
   
   insert into table ${APP}.ads_product_sale_topN
   select
       '$do_date' dt,
       sku_id,
       payment_amount
   from
       ${APP}.dws_sku_action_daycount
   where
       dt='$do_date'
   order by payment_amount desc
   limit 10;
   
   insert into table ${APP}.ads_product_favor_topN
   select
       '$do_date' dt,
       sku_id,
       favor_count
   from
       ${APP}.dws_sku_action_daycount
   where
       dt='$do_date'
   order by favor_count desc
   limit 10;
   
   insert into table ${APP}.ads_product_cart_topN
   select
       '$do_date' dt,
       sku_id,
       cart_count
   from
       ${APP}.dws_sku_action_daycount
   where
       dt='$do_date'
   order by cart_count desc
   limit 10;
   
   
   insert into table ${APP}.ads_product_refund_topN
   select
       '$do_date',
       sku_id,
       refund_last_30d_count/payment_last_30d_count*100 refund_ratio
   from ${APP}.dwt_sku_topic
   order by refund_ratio desc
   limit 10;
   
   
   insert into table ${APP}.ads_appraise_bad_topN
   select
       '$do_date' dt,
       sku_id,
   appraise_bad_count/(appraise_good_count+appraise_mid_count+appraise_bad_count+appraise_default_count) appraise_bad_ratio
   from
       ${APP}.dws_sku_action_daycount
   where
       dt='$do_date'
   order by appraise_bad_ratio desc
   limit 10;
   
   
   insert into table ${APP}.ads_order_daycount
   select
       '$do_date',
       sum(order_count),
       sum(order_amount),
       sum(if(order_count>0,1,0))
   from ${APP}.dws_user_action_daycount
   where dt='$do_date';
   
   
   insert into table ${APP}.ads_payment_daycount
   select
       tmp_payment.dt,
       tmp_payment.payment_count,
       tmp_payment.payment_amount,
       tmp_payment.payment_user_count,
       tmp_skucount.payment_sku_count,
       tmp_time.payment_avg_time
   from
   (
       select
           '$do_date' dt,
           sum(payment_count) payment_count,
           sum(payment_amount) payment_amount,
           sum(if(payment_count>0,1,0)) payment_user_count
       from ${APP}.dws_user_action_daycount
       where dt='$do_date'
   )tmp_payment
   join
   (
       select
           '$do_date' dt,
           sum(if(payment_count>0,1,0)) payment_sku_count 
       from ${APP}.dws_sku_action_daycount
       where dt='$do_date'
   )tmp_skucount on tmp_payment.dt=tmp_skucount.dt
   join
   (
       select
           '$do_date' dt,
           sum(unix_timestamp(payment_time)-unix_timestamp(create_time))/count(*)/60 payment_avg_time
       from ${APP}.dwd_fact_order_info
       where dt='$do_date'
       and payment_time is not null
   )tmp_time on tmp_payment.dt=tmp_time.dt;
   
   
   with 
   tmp_order as
   (
       select
           user_id,
           order_stats_struct.sku_id sku_id,
           order_stats_struct.order_count order_count
       from ${APP}.dws_user_action_daycount lateral view explode(order_detail_stats) tmp as order_stats_struct
       where date_format(dt,'yyyy-MM')=date_format('$do_date','yyyy-MM')
   ),
   tmp_sku as
   (
       select
           id,
           tm_id,
           category1_id,
           category1_name
       from ${APP}.dwd_dim_sku_info
       where dt='$do_date'
   )
   insert into table ${APP}.ads_sale_tm_category1_stat_mn
   select
       tm_id,
       category1_id,
       category1_name,
       sum(if(order_count>=1,1,0)) buycount,
       sum(if(order_count>=2,1,0)) buyTwiceLast,
       sum(if(order_count>=2,1,0))/sum( if(order_count>=1,1,0)) buyTwiceLastRatio,
       sum(if(order_count>=3,1,0))  buy3timeLast  ,
       sum(if(order_count>=3,1,0))/sum( if(order_count>=1,1,0)) buy3timeLastRatio ,
       date_format('$do_date' ,'yyyy-MM') stat_mn,
       '$do_date' stat_date
   from
   (
       select 
           tmp_order.user_id,
           tmp_sku.category1_id,
           tmp_sku.category1_name,
           tmp_sku.tm_id,
           sum(order_count) order_count
       from tmp_order
       join tmp_sku
       on tmp_order.sku_id=tmp_sku.id
       group by tmp_order.user_id,tmp_sku.category1_id,tmp_sku.category1_name,tmp_sku.tm_id
   )tmp
   group by tm_id, category1_id, category1_name;
   
   
   
   insert into table ${APP}.ads_area_topic
   select
       '$do_date',
       id,
       province_name,
       area_code,
       iso_code,
       region_id,
       region_name,
       login_day_count,
       order_day_count,
       order_day_amount,
       payment_day_count,
       payment_day_amount
   from ${APP}.dwt_area_topic;
   
   "
   
   $hive -e "$sql"
   ```

2. 授予脚本执行权限

   ```shell
   chmod 777 dwt_to_ads.sh
   ```

3. 执行脚本

   ```shell
   dwt_to_ads.sh 2020-06-24
   ```

   