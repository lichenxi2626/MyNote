# 实时数仓搭建

## 1.实时数仓架构

### 1.1 日志数据处理系统

<img src="实时数仓搭建.assets/image-20200806205705184.png" alt="image-20200806205705184" style="zoom:80%;" />



### 1.2 业务数据处理系统

![image-20200806213334166](实时数仓搭建.assets/image-20200806213334166.png)



## 2.模拟日志生成服务

- 日志生成部分共需要有2条通道
  1. 日志生成器（单机）→nginx（单机）→Web服务（集群）→KafkaTopic（集群）
  2. 日只生成器（单机）→nginx（单机）→Web服务（集群）→本地落盘（集群）



### 2.1 模拟日志生成器

1. 上传模拟日志生成jar包和配置文件

   ```shell
   gmall2020-mock-log-2020-05-10.jar
   application.properties
   ```

2. 测试执行jar包

   ```shell
   java -jar gmall2020-mock-log-2020-05-10.jar
   #由于没有数据接收通道,会抛出异常
   ```



### 2.2 编写Springboot服务

1. 新建Maven工程

2. 创建SpringBoot Module
   ![image-20200716094950024](实时数仓搭建.assets/image-20200716094950024.png)
   
   ![image-20200716095207650](实时数仓搭建.assets/image-20200716095207650.png)
   选择添加的依赖
   ![image-20200716095358843](实时数仓搭建.assets/image-20200716095358843.png)
   
   ![image-20200716095507562](实时数仓搭建.assets/image-20200716095507562.png)
   
   ![image-20200716095537831](实时数仓搭建.assets/image-20200716095537831.png)
   选择版本后确认   ![image-20200716095713723](实时数仓搭建.assets/image-20200716095713723.png)
3. 若新建SpringBoot后，resource目录下application.properties图标显示不正确，如下
   <img src="实时数仓搭建.assets/image-20200716095908522.png" alt="image-20200716095908522" style="zoom:80%;" />

   先remove module
   <img src="实时数仓搭建.assets/image-20200716100147885.png" alt="image-20200716100147885" style="zoom:80%;" />
   再重建module
   <img src="实时数仓搭建.assets/image-20200716100310052.png" alt="image-20200716100310052" style="zoom:80%;" />
   选择Maven工程
   <img src="实时数仓搭建.assets/image-20200716103218173.png" alt="image-20200716103218173" style="zoom:80%;" />
   重建后查看图标
   <img src="实时数仓搭建.assets/image-20200716103257414.png" alt="image-20200716103257414" style="zoom:80%;" />

4. 在application.properties中添加配置

   ```shell
   #防止和zookeeper冲突,修改端口号
   server.port=8090
   
   # kafka配置项
   # 指定代理的kafka集群地址,可配置多台
   spring.kafka.bootstrap-servers=hadoop201:9092,hadoop202:9092,hadoop203:9092
   # 指定消息key和消息体的编解码方式
   spring.kafka.producer.key-serializer=org.apache.kafka.common.serialization.StringSerializer
   spring.kafka.producer.value-serializer=org.apache.kafka.common.serialization.StringSerializer
   ```

5. 在pom.xml中添加依赖

   ```xml
   <dependency>
       <groupId>com.alibaba</groupId>
       <artifactId>fastjson</artifactId>
       <version>1.2.68</version>
   </dependency>
   ```

6. 编写代码
   <img src="实时数仓搭建.assets/image-20200716115134494.png" alt="image-20200716115134494" style="zoom:80%;" />

   ```java
   @RestController
   @Slf4j
public class LoggerController {
   
       @Autowired
       KafkaTemplate kafkaTemplate;
   
       //@RequestMapping用于配置访问路径,此处为通过hadoop201:80/applog访问
       @RequestMapping("/applog")
       public String applog(@RequestBody String json){
   
           JSONObject jsonObject = JSON.parseObject(json);
   
           //根据数据内容区分日志数据和业务数据,发送给Kafka
           if (jsonObject.containsKey("start") && jsonObject.getString("start").length() > 0){
               kafkaTemplate.send("GMALL_START",json);
           }else{
               kafkaTemplate.send("GMALL_EVENT",json);
           }
   
           //将数据额外输出一份到本地备份
           log.info(json);
   
           return "success";
       }
   
   }
   ```
   
7. 添加logback,xml配置文件 
   <img src="实时数仓搭建.assets/image-20200717183253366.png" alt="image-20200717183253366" style="zoom:80%;" />

   ```xml
   <?xml version="1.0" encoding="UTF-8"?>
   <configuration>
   
       <!-- 配置日志文件在本地的存储路径 -->
       <property name="LOG_HOME" value="/opt/applog/gmall" />
       <appender name="console" class="ch.qos.logback.core.ConsoleAppender">
           <encoder>
               <pattern>%msg%n</pattern>
           </encoder>
       </appender>
   
       <appender name="rollingFile" class="ch.qos.logback.core.rolling.RollingFileAppender">
           <file>${LOG_HOME}/app.log</file>
           <rollingPolicy class="ch.qos.logback.core.rolling.TimeBasedRollingPolicy">
               <fileNamePattern>${LOG_HOME}/app.%d{yyyy-MM-dd}.log</fileNamePattern>
           </rollingPolicy>
           <encoder>
               <pattern>%msg%n</pattern>
           </encoder>
       </appender>
   
       <!-- 将指定包下日志单独打印日志 -->
       <logger name="com.atguigu.gmall.logger.controller.LoggerController"
               level="INFO" additivity="false">
           <appender-ref ref="rollingFile" />
           <appender-ref ref="console" />
       </logger>
   
       <root level="error" additivity="false">
           <appender-ref ref="console" />
       </root>
   
   </configuration>
   ```

8. 打包后上传到linux，分发到所有节点

   ```shell
   #现在所有节点创建具有权限的路径
   cd /opt/applog
   xsync gmall-logger-0.0.1-SNAPSHOT.jar
   ```




### 2.3 部署Nginx（v1.12.2）

#### 2.3.1 Nginx概述

- Nginx是一个**高性能的HTTP和反向代理服务器**，具有内存占用低，并发能力强的特点。
  - 正向代理：用户通过代理服务器访问Web服务器
  - 反向代理：代理服务器代理多台Web服务器被用户访问
- 在数据采集模块中，Nginx用于代理三台服务器节点，接收模拟生成的日志数据，再分别分配给3台服务器，实现负载均衡。



#### 2.3.2 安装Nginx

1. 安装Nginx所需依赖环境

   ```shell
   sudo yum -y install openssl openssl-devel pcre pcre-devel zlib zlib-devel gcc gcc-c++
   ```

2. 解压安装包

   ```shell
   tar -zxvf nginx-1.12.2.tar.gz
   ```

3. 进入解压目录，修改nginx安装目录

   ```shell
   cd nginx-1.12.2
   ./configure --prefix=/opt/module/nginx
   ```

4. 执行make命令安装nginx

   ```shell
   make && make install
   ```

5. 修改nginx配置文件

   ```shell
   cd /opt/module/nginx/conf
   vi nginx.conf
   ```

   ```shell
   #在http{}中添加或修改如下配置,将数据发送到用于接收数据的web服务
       
       #定义服务器组,配置3台节点web服务的端口,该端口号与springboot服务中的server.port端口一致
       upstream logserver{
         server    hadoop201:8090 weight=1;  
         server    hadoop202:8090 weight=1;
         server    hadoop203:8090 weight=1;
       }
       server {
           listen       80;
           #自定义服务名(可以不改)
           server_name  logserver;
           location / {
               root   html;
               index  index.html index.htm;
               #配置反向代理的服务器组(upstream中定义)
               proxy_pass http://logserver;
               proxy_connect_timeout 10;
           }
   ```

6. 赋予nginx使用80端口的权限

   ```shell
   #默认情况下,非root用户启动进程不能占用1024以下端口号,因此需要赋予nginx启动命令使用端口的权限
   sudo setcap cap_net_bind_service=+eip /opt/module/nginx/sbin/nginx
   ```

7. 启动nginx

   ```shell
   cd /opt/module/nginx
   sbin/nginx
   
   #关闭nginx
   sbin/nginx -s stop
   #重启nginx
   sbin/nginx -s reload
   ```

8. 编写脚本群起web服务和nginx

   ```shell
   vi ~/bin/logger-cluster.sh
   ```
   
   ```shell
   #!/bin/bash
   JAVA_BIN=/opt/module/jdk1.8.0_212/bin/java
   APPNAME=gmall-logger-0.0.1-SNAPSHOT.jar
   SERVER_PORT=8090
    
   case $1 in
    "start")
      {
       for i in hadoop201 hadoop202 hadoop203
       do
        echo "==========$i=========="
       ssh $i  "$JAVA_BIN -Xms32m -Xmx64m -jar /opt/applog/$APPNAME --server.port=$SERVER_PORT >/dev/null 2>&1  &"
       done
        echo "==========NGINX=========="
       /opt/module/nginx/sbin/nginx
     };;
     "stop")
     { 
        echo "==========NGINX=========="
       /opt/module/nginx/sbin/nginx -s stop
       for i in  hadoop201 hadoop202 hadoop203
       do
        echo "==========$i=========="
        ssh $i "ps -ef|grep $APPNAME |grep -v grep|awk '{print \$2}'|xargs kill" >/dev/null 2>&1
       done
     };;
   esac
   ```



#### 2.3.4 测试实时日志采集通道

1. 启动zookeeper，kafka集群

   ```shell
   zkctl start
   kafkactl start
   ```

2. 启动nginx，web服务

   ```shell
   logger-cluster.sh start
   ```

3. 启动kafka消费者

   ```shell
   #创建2个目标topic
   kafka-topics.sh --create --bootstrap-server hadoop201:9092 --topic GMALL_START --partitions 4 --replication-factor 1
   
   kafka-topics.sh --create --bootstrap-server hadoop201:9092 --topic GMALL_EVENT --partitions 4 --replication-factor 1
   
   #启动消费命令
   kafka-console-consumer.sh \
   --bootstrap-server  hadoop201:9092,hadoop202:9092,hadoop203:9092 \
   --topic  GMALL_START
   
   kafka-console-consumer.sh \
   --bootstrap-server  hadoop201:9092,hadoop202:9092,hadoop203:9092 \
   --topic  GMALL_EVENT
   ```

4. 修改模拟日志配置

   ```shell
   cd /opt/applog
   vi application.properties
   ```

   ```properties
   # 外部配置打开
   # logging.config=./logback.xml
   #业务日期
   mock.date=2020-07-17
   
   #模拟数据发送模式
   mock.type=http
   #http模式下,发送的地址(配置nginx代理端口)
   mock.url=http://hadoop201:80/applog
   
   #启动次数
   mock.startup.count=10000
   #设备最大值
   mock.max.mid=50
   #会员最大值
   mock.max.uid=500
   #商品最大值
   mock.max.sku-id=10
   #页面平均访问时间
   mock.page.during-time-ms=20000
   #错误概率 百分比
   mock.error.rate=3
   #每条日志发送延迟 ms
   mock.log.sleep=10
   #商品详情来源  用户查询，商品推广，智能推荐, 促销活动
   mock.detail.source-type-rate=40:25:15:20
   ```

5. 启动模拟日志生成进程

   ```shell
   cd /opt/applog
   java -jar gmall2020-mock-log-2020-05-10.jar
   ```

6. 查看数据生成情况

   ```shell
   #1.kafka集群中的2个topic接收日志和业务数据
   #2.三台节点的/opt/applog/gmall路径下分别生成app.log日志文件
   ```



## 3.日志数据处理（统计日活）

- 日活数据处理服务流程
  1. 初始化SparkStreaming环境
  2. 从redis中获取上一次从Kafka中拉取日活数据相应的Topic的消费结束位置（offset）
  3. 从kafka中存储日活数据的Topic中拉取数据（从offset位置开始消费）
  4. 记录每一个DStream批次在kafka中消费的结束位置（offset）
  5. 处理从kafka中拉取的数据，并发送给redis，通过redis实现对设备mid的去重，只保留用户当天的1次登陆记录
  6. （手动操作）在ES中创建索引模版，使存储每天日活数据的索引自动套用模版
  7. 将去重后的数据使用样例类对象封装，发送给ES
  8. ES自动创建存储当天数据的索引，不断接受并存储日活数据
  9. 将第4步中记录的该批次DStream的offset位置提交给redis



### 3.1 部署 Redis（v3.2.5）

1. 上传并解压安装包到指定目录

   ```shell
   tar -zxvf redis-3.2.5.tar.gz -C /opt/module
   ```

2. 安装相关依赖

   ```shell
   sudo yum install gcc-c++
   ```

3. 进入安装目录，执行make命令

   ```shell
   cd /opt/module/redis-3.2.5
   make
   ```

4. 执行make install命令

   ```shell
   sudo make install
   
   #安装完成后查看redis相关命令(命令已自动配置到环境变量)
   cd usr/local/bin
   ```

5. 复制redis.conf目录，并修改配置文件

   ```shell
   cp /opt/module/redis-3.2.5/redis.conf ~/redis.conf
   vi ~/redis.conf
   ```

   ```shell
   #配置后台运行(默认no)
   daemonize yes
   ```

6. 使用修改的配置文件启动redis server

   ```shell
   redis-server ~/redis.conf
   ```

7. （可选）启动redis客户端

   ```shell
   redis-cli
   ```

8. 关闭redis server

   ```shell
   #方式1:在客户端中关闭
   shutdown
   
   #方式2:使用客户端命令关闭
   redis-cli shutdown
   ```



### 3.2 部署 Elasticsearch（v6.6.0）

#### 3.2.1 安装es

1. 上传并解压安装包到指定目录

   ```shell
   tar -zxvf elasticsearch-6.6.0.tar.gz -C /opt/module
   ```

2. 修改es配置文件

   ```shell
   cd /opt/module/elasticsearch-6.6.0/config
   vi elasticsearch.yml
   ```

   ```yaml
   #注意yml每行必须顶格编写,:后必须有空格
   
   #1.修改集群名称,同一集群节点集群名称必须相同
   cluster.name: my-es
   
   #2.修改节点名称
   node.name: node-1
   
   #3.修改网络地址,端口号默认9200
   network.host: hadoop201
   
   #4.关闭bootstrap自检程序(影响启动)
   bootstrap.memory_lock: false
   bootstrap.system_call_filter: false
   
   #5.修改自发现配置,用于节点找到集群并报道,保险起见配置2台节点
   discovery.zen.ping.unicast.hosts: ["hadoop201", "hadoop202"]
   ```

3. （生产环境无需修改）修改es的默认虚拟机内存占用

   ```shell
   vi jvm.options
   ```

   ```shell
   -Xms256m
   -Xmx256m
   ```

4. 分发es到所有节点

   ```shell
   cd /opt/module
   xsync 
   ```

5. 修改每个节点的配置文件（除hadoop201的其他节点）

   ```shell
   cd /opt/module/elasticsearch-6.6.0/config
   vi elasticsearch.yml
   ```

   ```shell
   #1.修改节点名称
   node.name: node-2
   
   #2.修改网络地址,端口号默认9200
   network.host: hadoop202
   ```

6. 修改linux配置，支持es高并发，修改单进程的虚拟内存占用（3台节点）

   ```shell
   sudo vim /etc/security/limits.conf
   ```

   ```shell
   #追加以下配置
   * soft nofile 65536
   * hard nofile 131072
   * soft nproc 2048
   * hard nproc 65536
   ```

   ```shell
   sudo vim /etc/sysctl.conf
   ```

   ```shell
   #添加如下配置
   vm.max_map_count=262144
   ```

7. （CenOS7以下需配置）修改最大进程数（3台节点）

   ```shell
   sudo vim /etc/security/limits.d/90-nproc.conf
   ```

   ```shell
   #修改如下配置
   * soft nproc 4096
   ```

8. 重启linux生效配置（3台节点）

   ```shell
   reboot
   ```




#### 3.2.2 安装kibana

1. 上传并解压安装包到指定目录

   ```shell
   tar -zxvf kibana-6.6.0-linux-x86_64.tar.gz -C /opt/module/
   cd /opt/module/
   mv kibana-6.6.0-linux-x86_64/ kibana-6.6.0
   ```

2. 修改配置文件

   ```shell
   cd kibana-6.6.0/config/
   vi kibana.yml
   ```

   ```yaml
   #修改以下配置
   server.host: "0.0.0.0"
   
   elasticsearch.hosts: ["http://hadoop201:9200"]
   ```

3. 编写启动es集群和kibana的脚本

   ```shell
   cd ~/bin
   vi esctl
   ```

   ```shell
   #!/bin/bash 
   es_home=/opt/module/elasticsearch-6.6.0
   kibana_home=/opt/module/kibana-6.6.0
   
   case $1  in
    "start") {
     for i in hadoop201 hadoop202 hadoop203
     do
       ssh $i  "source /etc/profile;${es_home}/bin/elasticsearch >/dev/null 2>&1 &" 
     done
     nohup ${kibana_home}/bin/kibana >/home/atguigu/kibana.log 2>&1 &
   };;
   
   "stop") {
     ps -ef|grep ${kibana_home} |grep -v grep|awk '{print $2}'|xargs kill
     for i in hadoop201 hadoop202 hadoop203
     do
         ssh $i "ps -ef|grep $es_home |grep -v grep|awk '{print \$2}'|xargs kill" >/dev/null 2>&1
     done
   };;
   esac
   ```

   ```shell
   chmod +x esctl
   ```

4. web端查看es节点状态

   ```http
   http://hadoop201:9200/_cat/nodes?v
   ```

   ```shell
   #可查看到3台节点状态,如下
   ip              heap.percent ram.percent cpu load_1m load_5m load_15m node.role master name
   192.168.145.201           55          43   3    1.00    0.40     0.26 mdi       *      node-1
   192.168.145.202           46          34   0    1.12    0.38     0.19 mdi       -      node-2
   192.168.145.203           44          34   0    0.84    0.35     0.18 mdi       -      node-3
   ```

5. web端访问kibana

   ```http
   http://hadoop201:5601/
   ```



#### 3.2.3 安装中文分词器

1. 下载分词器 https://github.com/medcl/elasticsearch-analysis-ik

2. 上传并解压安装包

   ```shell
   unzip elasticsearch-analysis-ik-6.6.0.zip -d ik/
   ```

3. 移动到es安装目录的插件目录下

   ```shell
   mv ik/ /opt/module/elasticsearch-6.6.0/plugins/
   ```

4. 分发到所有节点

   ```shell
   xsync ik
   ```

5. 重启es

   ```shell
   esctl stop
   esctl start
   ```



### 3.3  编写日活数据处理服务

#### 3.3.1 Maven环境准备

1. 创建Maven Module
   <img src="实时数仓搭建.assets/image-20200717215045421.png" alt="image-20200717215045421" style="zoom:80%;" />

2. 添加Scala框架支持
   <img src="实时数仓搭建.assets/image-20200717215142329.png" alt="image-20200717215142329" style="zoom:80%;" />

3. 向pom.xml中添加依赖

   ```xml
   <properties>
       <spark.version>2.4.0</spark.version>
       <scala.version>2.12.11</scala.version>
       <kafka.version>1.0.0</kafka.version>
       <project.build.sourceEncoding>UTF-8</project.build.sourceEncoding>
       <project.reporting.outputEncoding>UTF-8</project.reporting.outputEncoding>
       <java.version>1.8</java.version>
   </properties>
   
   <dependencies>
       <dependency>
           <groupId>com.alibaba</groupId>
           <artifactId>fastjson</artifactId>
           <version>1.2.68</version>
       </dependency>
   
       <dependency>
           <groupId>org.apache.spark</groupId>
           <artifactId>spark-core_2.12</artifactId>
           <version>${spark.version}</version>
       </dependency>
       <dependency>
           <groupId>org.apache.spark</groupId>
           <artifactId>spark-streaming_2.12</artifactId>
           <version>${spark.version}</version>
       </dependency>
       <dependency>
           <groupId>org.apache.kafka</groupId>
           <artifactId>kafka-clients</artifactId>
           <version>${kafka.version}</version>
   
       </dependency>
       <dependency>
           <groupId>org.apache.spark</groupId>
           <artifactId>spark-streaming-kafka-0-10_2.12</artifactId>
           <version>${spark.version}</version>
       </dependency>
   
       <dependency>
           <groupId>redis.clients</groupId>
           <artifactId>jedis</artifactId>
           <version>2.9.0</version>
       </dependency>
   
   
       <dependency>
           <groupId>org.apache.phoenix</groupId>
           <artifactId>phoenix-spark</artifactId>
           <version>4.14.2-HBase-1.3</version>
       </dependency>
   
       <dependency>
           <groupId>org.apache.spark</groupId>
           <artifactId>spark-sql_2.12</artifactId>
           <version>${spark.version}</version>
       </dependency>
   
       <dependency>
           <groupId>io.searchbox</groupId>
           <artifactId>jest</artifactId>
           <version>5.3.3</version>
           <exclusions>
               <exclusion>
                   <groupId>org.slf4j</groupId>
                   <artifactId>slf4j-api</artifactId>
               </exclusion>
           </exclusions>
       </dependency>
   
       <dependency>
           <groupId>net.java.dev.jna</groupId>
           <artifactId>jna</artifactId>
           <version>4.5.2</version>
       </dependency>
   
       <dependency>
           <groupId>org.codehaus.janino</groupId>
           <artifactId>commons-compiler</artifactId>
           <version>2.7.8</version>
       </dependency>
       
       <dependency>
           <groupId>org.elasticsearch</groupId>
           <artifactId>elasticsearch</artifactId>
           <version>2.4.6</version>
       </dependency>
   
       <dependency>
           <groupId>com.thoughtworks.paranamer</groupId>
           <artifactId>paranamer</artifactId>
           <version>2.8</version>
       </dependency>
   
   </dependencies>
   
   <build>
       <plugins>
           <!-- 该插件用于将Scala代码编译成class文件 -->
           <plugin>
               <groupId>net.alchim31.maven</groupId>
               <artifactId>scala-maven-plugin</artifactId>
               <version>3.4.6</version>
               <executions>
                   <execution>
                       <!-- 声明绑定到maven的compile阶段 -->
                       <goals>
                           <goal>compile</goal>
                           <goal>testCompile</goal>
                       </goals>
                   </execution>
               </executions>
           </plugin>
   
           <plugin>
               <groupId>org.apache.maven.plugins</groupId>
               <artifactId>maven-assembly-plugin</artifactId>
               <version>3.0.0</version>
               <configuration>
                   <descriptorRefs>
                       <descriptorRef>jar-with-dependencies</descriptorRef>
                   </descriptorRefs>
               </configuration>
               <executions>
                   <execution>
                       <id>make-assembly</id>
                       <phase>package</phase>
                       <goals>
                           <goal>single</goal>
                       </goals>
                </execution>
               </executions>
           </plugin>
       </plugins>
   </build>
   ```
   



#### 3.3.2 resources配置文件

1. config.properties

   ```properties
   # Kafka配置
   kafka.broker.list=hadoop201:9092,hadoop202:9092,hadoop203:9092
   # Redis配置
   redis.host=hadoop201
   redis.port=6379
   ```

2. log4j.properties

   ```properties
   log4j.appender.atguigu.MyConsole=org.apache.log4j.ConsoleAppender
   log4j.appender.atguigu.MyConsole.target=System.out
   log4j.appender.atguigu.MyConsole.layout=org.apache.log4j.PatternLayout    
   log4j.appender.atguigu.MyConsole.layout.ConversionPattern=%d{yyyy-MM-dd HH:mm:ss} %10p (%c:%M) - %m%n
   log4j.rootLogger =error,atguigu.MyConsole
   ```



#### 3.3.3 编写应用

目录
<img src="实时数仓搭建.assets/image-20200724110228090.png" alt="image-20200724110228090" style="zoom:80%;" />



1. 用于对接ES的document的样例类

   ```scala
   case class DauInfo(
       mid: String,
       uid: String,
       ar: String,
       ch: String,
       vc: String,
       var dt: String,
       var hr: String,
       var mi: String,
       ts: Long
   )
   ```

2. MyEsUtil

   ```scala
   object MyEsUtil {
   
       private var factory: JestClientFactory = null
   
       //返回es连接池对象的工具类方法
       def getJestClient(): JestClient = {
           if (factory != null) {
               factory.getObject
           } else {
               build()
               factory.getObject
           }
       }
   
       def build(): Unit = {
           factory = new JestClientFactory
           factory.setHttpClientConfig(new HttpClientConfig.Builder("http://hadoop201:9200")
                                       .multiThreaded(true)
                                       .maxTotalConnection(20)
                                       .connTimeout(10000).readTimeout(10000).build()
                                      )
       }
   
       //批次化写
       def bulkSave(list: List[(Any, String)], indexName: String): Unit = {
           val jestClient: JestClient = getJestClient()
           val bulkBuilder = new Bulk.Builder
           //设置默认的index和type,后续仅需使用id调用
           bulkBuilder.defaultIndex(indexName).defaultType("_doc")
           for ((doc, id) <- list){
               val index: Index = new Index.Builder(doc).id(id).build()
               bulkBuilder.addAction(index)
           }
           val bulk: Bulk = bulkBuilder.build()
           val items: util.List[BulkResult#BulkResultItem] = jestClient.execute(bulk).getItems
           jestClient.close()
           println("已保存到ES " + items.size() + " 条数据")
       }
   
   }
   ```

3. MyKafkaUtil

   ```scala
   object MyKafkaUtil {
   
       private val properties: Properties = PropertiesUtil.load("config.properties")
       val broker_list = properties.getProperty("kafka.broker.list")
   
       // kafka消费者配置
       var kafkaParam = collection.mutable.Map(
           "bootstrap.servers" -> broker_list,//用于初始化链接到集群的地址
           "key.deserializer" -> classOf[StringDeserializer],
           "value.deserializer" -> classOf[StringDeserializer],
           //用于标识这个消费者属于哪个消费团体
           "group.id" -> "gmall_consumer_group",
           //如果没有初始化偏移量或者当前的偏移量不存在任何服务器上,可以使用这个配置属性
           //可以使用这个配置,latest自动重置偏移量为最新的偏移量
           "auto.offset.reset" -> "latest",
           //如果是true,则这个消费者的偏移量会在后台自动提交,但是kafka宕机容易丢失数据
           //如果是false,会需要手动维护kafka偏移量
           "enable.auto.commit" -> (false: java.lang.Boolean)
       )
   
       // 创建DStream，返回接收到的输入数据
       // LocationStrategies：根据给定的主题和集群地址创建consumer
       // LocationStrategies.PreferConsistent：持续的在所有Executor之间分配分区
       // ConsumerStrategies：选择如何在Driver和Executor上创建和配置Kafka Consumer
       // ConsumerStrategies.Subscribe：订阅一系列主题
   
       def getKafkaStream(topic: String, ssc:StreamingContext): InputDStream[ConsumerRecord[String,String]]={
           val dStream = KafkaUtils.createDirectStream[String,String](ssc, LocationStrategies.PreferConsistent, ConsumerStrategies.Subscribe[String,String](Array(topic), kafkaParam ))
           dStream
       }
   
       def getKafkaStream(topic: String, ssc:StreamingContext, groupId:String): InputDStream[ConsumerRecord[String,String]]={
           kafkaParam("group.id") = groupId
           val dStream = KafkaUtils.createDirectStream[String,String](ssc, LocationStrategies.PreferConsistent, ConsumerStrategies.Subscribe[String,String](Array(topic), kafkaParam ))
           dStream
       }
   
       def getKafkaStream(topic: String, ssc:StreamingContext, offsets:Map[TopicPartition,Long], groupId:String): InputDStream[ConsumerRecord[String,String]]={
           kafkaParam("group.id") = groupId
           val dStream = KafkaUtils.createDirectStream[String,String](ssc, LocationStrategies.PreferConsistent, ConsumerStrategies.Subscribe[String,String](Array(topic), kafkaParam, offsets))
           dStream
       }
   
   }
   ```

4. OffsetManager

   ```scala
   import scala.collection.JavaConversions._
   
   object OffsetManager {
   
       //使用Redis的Hash集合存储管理kafkaTopic的偏移量
       //以topic+groupid为key,以partition为field,以offset为value进行存储
   
       //从redis获取指定topic中,目标消费者组消费的每个分区的偏移量
       def getOffset(topic: String, groupId: String): Map[TopicPartition, Long] ={
   
           //从redis中拉取指定topic+消费者组在kafka中存储的的所有分区的数据偏移量
           val jedis: Jedis = RedisUtil.getJedis()
           val offsetKey: String = topic + ":" + groupId
           val offsetMap: util.Map[String, String] = jedis.hgetAll(offsetKey)
           jedis.close()
   
           //将redis中的数据偏移量集合转换为Kafka拉取数据可识别的Map集合
           if (offsetMap != null && offsetMap.size() > 0){
               //将util.Map转换为scala的List需要import scala.collection.JavaConversions._
               val offsetList: List[(String, String)] = offsetMap.toList
               val offsetListForKafka: List[(TopicPartition, Long)] = offsetList.map(
                   kv => {
                       val topicPartition = new TopicPartition(topic, kv._1.toInt)
                       println("读取分区:偏移量 => " + kv._1 + ":" + kv._2)
                       (topicPartition, kv._2.toLong)
                   }
               )
               val offsetMapForKafka: Map[TopicPartition, Long] = offsetListForKafka.toMap
               offsetMapForKafka
           } else {
               null
           }
   
       }
   
       //将指定topic中,目标消费者组消费的每个分区的偏移量,写入redis中
       def setOffset(topic: String, groupId: String, offsetRanges: Array[OffsetRange]): Unit ={
   
           //获取要写入redis的key,field,value
           //key为topic+groupId, filed为分区, value为偏移量
           val offsetKey: String = topic + ":" + groupId
           val offsetMap = new util.HashMap[String, String]()
   
           //将从kafka获取的offsetRanges(封装了offset及其topic,partition,offset等信息)的分区/偏移量两个属性依次装入offsetMap
           for (offsetRange <- offsetRanges) {
               val partition: String = offsetRange.partition.toString
               val untilOffset: String = offsetRange.untilOffset.toString
               println("写入分区:偏移量 => " + partition + ":" + untilOffset)
               offsetMap.put(partition, untilOffset)
           }
   
           //写入redis
           val jedis: Jedis = RedisUtil.getJedis()
           jedis.hmset(offsetKey,offsetMap)
           jedis.close()
   
       }
   
   }
   ```

5. PropertiesUtil

   ```scala
   object PropertiesUtil {
   
       def main(args: Array[String]): Unit = {
   
           val properties: Properties = PropertiesUtil.load("config.properties")
   
           println(properties.getProperty("kafka.broker.list"))
   
       }
   
       def load(propertieName:String): Properties = {
           val prop = new Properties();
           prop.load(new InputStreamReader(Thread.currentThread().getContextClassLoader.getResourceAsStream(propertieName), "UTF-8"))
           prop
       }
   
   }
   ```

6. RedisUtil

   ```scala
   object RedisUtil {
   
       var jedisPool: JedisPool = null
   
       def getJedis(): Jedis ={
           if (jedisPool == null){
               val properties: Properties = PropertiesUtil.load("config.properties")
               val host: String = properties.getProperty("redis.host")
               val port: String = properties.getProperty("redis.port")
   
               val config = new JedisPoolConfig()
               config.setMaxTotal(100)
               config.setMaxIdle(20)
               config.setMinIdle(20)
               config.setBlockWhenExhausted(true)
               config.setMaxWaitMillis(500)
               config.setTestOnBorrow(true)
   
               jedisPool = new JedisPool(config, host, port.toInt)
           }
           jedisPool.getResource
       }
   
   }
   ```

7. DauApp

   ```scala
   object DauApp {
   
       def main(args: Array[String]): Unit = {
   
           val conf: SparkConf = new SparkConf().setMaster("local[4]").setAppName("")
           val ssc = new StreamingContext(conf,Seconds(5))
   
           //从redis中读取上一次记录的kafka的topic的消费偏移量(仅启动时执行1次)
           val topic: String = "GMALL_START"
           val groupId: String = "GMALL_DAU_CONSUMER"
           val offsetMapForKafka: Map[TopicPartition, Long] = OffsetManager.getOffset(topic, groupId)
   
           //使用拿到的偏移量到kafka中的相应offset位置开始加载数据流(仅启动时执行1次)
           var startInputDStream: InputDStream[ConsumerRecord[String, String]] = null
           if (offsetMapForKafka != null && offsetMapForKafka.size > 0){
               startInputDStream = MyKafkaUtil.getKafkaStream(topic,ssc,offsetMapForKafka,groupId)
           } else {
               startInputDStream = MyKafkaUtil.getKafkaStream(topic,ssc,groupId)
           }
   
           //从DStream中获取该批次数据结束位置的偏移量(每批次执行1次)
           var offsetRanges: Array[OffsetRange] = null
           val getAfterOffsetDStream: DStream[ConsumerRecord[String, String]] = startInputDStream.transform(
               rdd => {
                   println("取得结束offset")
                   offsetRanges = rdd.asInstanceOf[HasOffsetRanges].offsetRanges
                   rdd
               }
           )
   
           //处理数据流,将每条数据从json字符串转换为json对象
           val jsonObjectDStream: DStream[JSONObject] = getAfterOffsetDStream.map(
               json => {
                   val jsonString: String = json.value()
                   val jsonObject: JSONObject = JSON.parseObject(jsonString)
                   jsonObject
               }
           )
           println("jsonObject转换完成")
   
           //使用redis对json对象去重(日活需求要求同一用户一天的登陆记录仅保留1次)
           //将json对象的ts字段转换为日期格式作为key,mid字段作为value,插入redis的Map集合中,达到去重的效果
           val jsonFilterdDStream: DStream[JSONObject] = jsonObjectDStream.mapPartitions(
               jsonObjects => {
                   val jsonListBeforeFilter: List[JSONObject] = jsonObjects.toList
                   println("去重前: " + jsonListBeforeFilter.size)
                   val jsonListAfterFilter = new ListBuffer[JSONObject]
                   //获取redis客户端,向redis插入数据
                   val jedis: Jedis = RedisUtil.getJedis()
                   for (jsonObject <- jsonListBeforeFilter) {
                       val ts: lang.Long = jsonObject.getLong("ts")
                       val date: String = new SimpleDateFormat("yyyy-MM-dd").format(new Date(ts))
                       val mid: String = jsonObject.getJSONObject("common").getString("mid")
                       //redis中sadd方法插入kv对时,若value已存在,则返回0,若value为新数据,则返回1
                       val flag: lang.Long = jedis.sadd(date, mid)
                       if (flag == 1) {
                           jsonListAfterFilter.append(jsonObject)
                       }
                   }
                   jedis.close()
                   println("去重后: " + jsonListAfterFilter.size)
                   jsonListAfterFilter.toIterator
               }
           )
           println("redis去重完成")
   
           //将日活数据到ES
           jsonFilterdDStream.foreachRDD(
               rdd => {
   
                   rdd.foreachPartition(
                       jsonObjects => {
                           val jsonObjectList: List[JSONObject] = jsonObjects.toList
                           val formatter = new SimpleDateFormat("yyyy-MM-dd HH:mm")
   
                           val dauList: List[(DauInfo, String)] = jsonObjectList.map(
                               jsonObject => {
                                   //获取日期字段
                                   val ts: lang.Long = jsonObject.getLong("ts")
                                   val date: String = formatter.format(new Date(ts))
                                   println(date)
                                   val dateArr: Array[String] = date.split(" ")
                                   val dt: String = dateArr(0)
                                   val time: String = dateArr(1)
                                   val timeArr: Array[String] = time.split(":")
                                   val hour: String = timeArr(0)
                                   val min: String = timeArr(1)
                                   //获取其余字段
                                   val commonJsonObject: JSONObject = jsonObject.getJSONObject("common")
                                   val dauInfo = DauInfo(commonJsonObject.getString("mid"),
                                                         commonJsonObject.getString("uid"),
                                                         commonJsonObject.getString("ar"),
                                                         commonJsonObject.getString("ch"),
                                                         commonJsonObject.getString("vc"),
                                                         dt, hour, min, ts
                                                        )
                                   (dauInfo, dauInfo.mid)
                               }
                           )
   
                           //将DauInfo型的数据数据存入ES
                           val today: String = new SimpleDateFormat("yyyyMMdd").format(new Date())
                           MyEsUtil.bulkSave(dauList,"gmall_dau_info_" + today)
   
                       }
                   )
   
                   //手动提交偏移量到kafka(每批次执行1次)
                   OffsetManager.setOffset(topic,groupId,offsetRanges)
   
               }
           )
   
           ssc.start()
           ssc.awaitTermination()
   
       }
   
   }
   ```



#### 3.3.4 ES中创建索引模版

1. 启动ES，kibana

   ```shell
   esctl start
   ```

2. web访问kibana

   ```
   http://hadoop201:5601
   ```

3. 创建索引模版

   ```json
   PUT_template/gmall_dau_info_template
   {
       "index_patterns": ["gmall_dau_info*"],                  
       "settings": {                                             
           "number_of_shards": 1
       },
       "aliases" : { 
           "{index}-query": {},
           "gmall_dau_info-query":{}
       },
       "mappings": {
           "_doc":{  
               "properties":{
                   "mid":{
                       "type":"keyword"
                   },
                   "uid":{
                       "type":"keyword"
                   },
                   "ar":{
                       "type":"keyword"
                   },
                   "ch":{
                       "type":"keyword"
                   },
                   "vc":{
                       "type":"keyword"
                   },
                   "dt":{
                       "type":"keyword"
                   },
                   "hr":{
                       "type":"keyword"
                   },
                   "mi":{
                       "type":"keyword"
                   },
                   "ts":{
                       "type":"date"
                   }  
               }
           }
       }
   }
   ```



### 3.4 测试实时日志处理通道

1. 启动日志采集通道

   ```shell
   zkctl start
   kafkactl start
   logger-cluster start  #nginx
   ```

2. 启动redis，es，kibana

   ```shell
   redis-server ~/redis.conf
   esctl start
   ```

3. 执行主程序

4. 修改日志数据生成脚本配置，并启动脚本

   ```shell
   vi /opt/applog/application.properties
   java -jar /opt/applog/gmall2020-mock-log-2020-05-10.jar
   ```

5. kibana中查看导入的数据

   ```shell
   GET gmall_dau_info_20200723/_search
   ```



## 4.日活数据查询接口

### 4.1 Springboot服务提供可视化接口

1. 创建Springboot项目，选择依赖
   ![image-20200724230846671](实时数仓搭建.assets/image-20200724230846671.png)

2. 添加依赖

   ```xml
   <dependency>
       <groupId>org.apache.commons</groupId>
       <artifactId>commons-lang3</artifactId>
       <version>3.8.1</version>
   </dependency>
   ```

3. 分层编写web服务
   <img src="实时数仓搭建.assets/image-20200724231042363.png" alt="image-20200724231042363" style="zoom:80%;" />

4. application.properties

   ```shell
   #web服务接口
   server.port=8070
   
   #es集群接口
   spring.elasticsearch.jest.uris=http://hadoop201:9200,http://hadoop202:9200
   ```

5. Service层

   ```java
   public interface DauService {
   
       //求某日日活数
       public Long getDayTotal(String date);
   
       //求某日每小时活跃用户数
       public Map getDauHourCount(String date);
   
   }
   ```

   ```java
   @Service
   public class DauServiceImpl implements DauService {
   
       @Autowired
       JestClient jestClient;
   
       //获取活跃用户数
       @Override
       public Long getDayTotal(String date) {
   
           //从es中拉取当日索引的数据
           String indexName = "gmall_dau_info_" + date.replace("-", "");
           SearchSourceBuilder searchSourceBuilder = new SearchSourceBuilder();
           searchSourceBuilder.query(new MatchAllQueryBuilder());
           Search search = new Search.Builder(searchSourceBuilder.toString()).addIndex(indexName).addType("_doc").build();
   
           try {
               SearchResult searchResult = jestClient.execute(search);
               return searchResult.getTotal();
           } catch (IOException e) {
               e.printStackTrace();
               throw new RuntimeException("ES查询异常");
           }
   
       }
   
       //获取每小时的活跃用户数量
       @Override
       public Map getDauHourCount(String date) {
   
           //从es中拉取当日索引的数据
           String indexName = "gmall_dau_info_" + date.replace("-", "");
           SearchSourceBuilder searchSourceBuilder = new SearchSourceBuilder();
           TermsBuilder termBuilder = AggregationBuilders.terms("groupby_hour").field("hr").size(24);
           searchSourceBuilder.aggregation(termBuilder);
           Search search = new Search.Builder(searchSourceBuilder.toString())
               .addIndex(indexName)
               .addType("_doc").build();
   
           try {
               SearchResult searchResult = jestClient.execute(search);
               Map resultMap = new HashMap<>();
               if (searchResult.getAggregations().getTermsAggregation("groupby_hour") != null){
                   List<TermsAggregation.Entry> buckets = searchResult
                       .getAggregations()
                       .getTermsAggregation("groupby_hour").getBuckets();
                   for (TermsAggregation.Entry bucket : buckets) {
                       resultMap.put(bucket.getKey(), bucket.getCount());
                   }
               }
               return resultMap;
           } catch (IOException e) {
               e.printStackTrace();
               throw new RuntimeException("ES查询异常");
           }
   
       }
   
   }
   ```

6. Controller层

   ```java
   @RestController
   public class PublisherController {
   
       @Autowired
       DauService dauService;
   
       @RequestMapping("realtime-total")
       public String realtimeTotal(@RequestParam("date") String date){
   
           List<Map<String,Object>> totalList = new ArrayList<>();
   
           Map dauMap = new HashMap();
           dauMap.put("id", "dau");
           dauMap.put("name", "新增日活");
           Long dauTotal = dauService.getDayTotal(date);
           dauMap.put("value", dauTotal);
           totalList.add(dauMap);
   
           return JSON.toJSONString(totalList);
       }
   
       @RequestMapping("realtime-hour")
       public String realtimeHour(@RequestParam("date") String date, @RequestParam("id") String id){
   
           if("dau".equals(id)){
               //获取当日的每小时活跃用户Map
               Map dauHourCountToday = dauService.getDauHourCount(date);
               //获取昨日的每小时活跃用户Map
               String yesterday = getYesterday(date);
               Map dauHourCountYesterday = dauService.getDauHourCount(yesterday);
   
               //将今天和昨天的小时活跃数据装入Map
               Map<String,Map<String,Long>> hourCountMap = new HashMap<>();
               hourCountMap.put("today",dauHourCountToday);
               hourCountMap.put("yesterday", dauHourCountYesterday);
               return JSON.toJSONString(hourCountMap);
   
           }
   
   
           return null;
       }
   
       //获取前一天日期字符串
       private String getYesterday(String date){
   
           try {
               Date today = new SimpleDateFormat("yyyy-MM-dd").parse(date);
               Date yesterday = DateUtils.addDays(today, -1);
               return new SimpleDateFormat("yyyy-MM-dd").format(yesterday);
           } catch (ParseException e) {
               e.printStackTrace();
               throw new RuntimeException("日期格式转换有误");
           }
       }
   
   }
   ```

7. 启动web服务，web查看查询结果

   ```
   http://localhost:8070/realtime-total?date=2020-07-24
   http://localhost:8070/realtime-hour?id=dau&date=2020-07-24
   ```



### 4.2 Kibana配置ES数据可视化

1. 进入index Patterns
   ![image-20200724220319880](实时数仓搭建.assets/image-20200724220319880.png)
2. 创建index Patterns
   ![image-20200724220652358](实时数仓搭建.assets/image-20200724220652358.png)
3. 选择用于筛选展示范围的字段（一般为时间）
   ![image-20200724220916019](实时数仓搭建.assets/image-20200724220916019.png)
4. 创建单图
   ![image-20200724221144812](实时数仓搭建.assets/image-20200724221144812.png)
5. 选择图形
   ![image-20200724221246973](实时数仓搭建.assets/image-20200724221246973.png)
6. 选择创建的index Parttern
   ![image-20200724221334719](实时数仓搭建.assets/image-20200724221334719.png)
7. 配置查询方式，并执行
   ![image-20200724222057198](实时数仓搭建.assets/image-20200724222057198.png)



## 5.ODS层业务数据处理

- ODS层数据处理流程
  1. 执行模拟业务数据生成脚本，生成数据追加到mysql中
  2. canal/maxwell监测mysql中所有目标表的数据变化，将新数据发送到Kafka主题中
  3. 使用SparkStreaming实时消费追加到Kafka主题的数据
  4. 处理拿到的数据，分别再次写入Kafka的ODS层主题中



### 5.1 部署 Canal

1. 修改MySQL配置文件

   ```shell
   sudo vim /etc/my.cnf
   ```

   ```shell
   #在[mysqld]模块中追加如下配置
   
   server-id= 1
   #开启mysql的二进制日志mysql-bin,记录mysql的所有DDL和DML操作,此处用于被canal监控
   log-bin=mysql-bin
   #配置日志模式为row,记录每次操作后每行数据的变化
   binlog_format=row
   #配置binlog记录的数据库
   binlog-do-db=gmall_realtime
   ```

2. 创建mysql用户并赋予权限

   ```shell
   mysql -uroot -p123456
   ```

   ```mysql
   GRANT SELECT, REPLICATION SLAVE, REPLICATION CLIENT ON *.* TO 'canal'@'%' IDENTIFIED BY 'canal' ;
   ```

3. 重启mysql

4. 上传并解压canal安装包

   ```shell
   tar -zxvf canal.deployer-1.1.4.tar.gz -C /opt/module/canal
   ```

5. 修改配置文件

   ```shell
   vi /opt/module/canal/conf/canal.properties
   ```

   ```properties
   #配置zookeeper地址
   canal.zkServers =hadoop201:2181,hadoop202:2181,hadoop203:2181
   
   #配置数据出口为kafka
   canal.serverMode = kafka
   
   #配置kafka地址
   canal.mq.servers = hadoop201:9092,hadoop202:9092,hadoop203:9092
   ```

6. 修改追踪的mysql实例配置

   ```shell
   vi /opt/module/canal/conf/example/instance.properties
   ```

   ```properties
   #配置mysql服务器地址
   canal.instance.master.address=hadoop201:3306
   
   #配置创建的canal专用的mysql用户和密码
   canal.instance.dbUsername=canal
   canal.instance.dbPassword=canal
   
   #配置数据发送到kafka的目标主题
   canal.mq.topic=GMALL_DB_CANAL
   
   #配置分区规则(4个分区,按主键对hash取模)
   canal.mq.partition=4
   canal.mq.partitionHash=.*\\..*:$pk$
   ```

7. 启动canal

   ```shell
   bin/startup.sh
   ```

8. （可选）配置canal高可用（借由zookeeper实现），配置完成后重启canal

   ```shell
   vi /opt/module/canal/conf/canal.properties
   ```

   ```properties
   #注释配置
   # canal.instance.global.spring.xml = classpath:spring/file-instance.xml
   
   #打开注释
   canal.instance.global.spring.xml = classpath:spring/default-instance.xml
   ```



### 5.2 部署 Maxwell

1. 启动mysql，创建maxwell用于存储元数据的库

   ```shell
   mysql -uroot -p123456
   ```

   ```mysql
   create database maxwell;
   ```

2. 创建maxwell用户，赋予权限

   ```mysql
   GRANT ALL ON maxwell.* TO 'maxwell'@'%' IDENTIFIED BY 'maxwell';
   GRANT SELECT ,REPLICATION SLAVE, REPLICATION CLIENT ON *.* TO maxwell@'%'
   ```

3. 解压安装maxwell

   ```shell
   tar -zxvf maxwell-1.25.0.tar.gz -C /opt/module
   ```

4. 编写maxwell配置文件（路径任意）

   ```shell
   vi /opt/module/maxwell-1.25.0/maxwell.properties
   ```

   ```properties
   #配置数据发送到kafka
   producer=kafka
   #配置kafka集群地址
   kafka.bootstrap.servers=hadoop201:9092,hadoop202:9092,hadoop203:9092
   #配置kafka的目标topic
   kafka_topic=GMALL_DB_MAXWELL
   
   #配置监控的mysql节点
   host=hadoop201
   #配置mysql用户名和密码
   user=maxwell
   password=maxwell
   producer_partition_by=primary_key
   
   client_id=maxwell_1
   ```

5. 启动maxwell

   ```shell
   /opt/module/maxwell-1.25.0/bin/maxwell --config /opt/module/maxwell-1.25.0/maxwell.properties >/dev/null 2>&1 &
   ```

6. 编写maxwell启停脚本

   ```shell
   vi ~/bin/maxwellctl
   ```

   ```shell
   #!/bin/bash
   
   case $1 in
    "start")
       echo "==========启动Maxwell=========="
       /opt/module/maxwell-1.25.0/bin/maxwell --config /opt/module/maxwell-1.25.0/maxwell.properties >/dev/null 2>&1 &
     ;;
     "stop")
       echo "==========关闭Maxwell=========="
       ps -ef|grep Maxwell |grep -v grep|awk '{print $2}'|xargs kill >/dev/null 2>&1
     ;;
   esac
   ```

   ```shell
   chmod +x ~/bin/maxwellctl
   ```





