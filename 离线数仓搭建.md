# 离线数仓搭建

## 1. 集群服务器规划

| 服务名称           | 子服务                | hadoop201 | hadoop202 | hadoop203 |
| ------------------ | --------------------- | --------- | --------- | --------- |
| HDFS               | NameNode              | √         |           |           |
|                    | DataNode              | √         | √         | √         |
|                    | SecondaryNameNode     |           |           | √         |
| Yarn               | NodeManager           | √         | √         | √         |
|                    | Resourcemanager       |           | √         |           |
| Zookeeper          | Zookeeper Server      | √         | √         | √         |
| Flume(采集日志)    | Flume                 | √         | √         |           |
| Kafka              | Kafka                 | √         | √         | √         |
| Flume（消费Kafka） | Flume                 |           |           | √         |
| Hive               | Hive                  | √         |           |           |
| MySQL              | MySQL                 | √         |           |           |
| Sqoop              | Sqoop                 | √         |           |           |
| Presto             | Coordinator           | √         |           |           |
|                    | Worker                |           | √         | √         |
| Azkaban            | AzkabanWebServer      | √         |           |           |
|                    | AzkabanExecutorServer | √         |           |           |
| Kylin              |                       | √         |           |           |
| Hbase              | HMaster               | √         |           |           |
|                    | HRegionServer         | √         | √         | √         |
| Superset           |                       | √         |           |           |
| 服务数总计         |                       | 16        | 8         | 8         |



## 2. 服务器准备

### 2.1 虚拟机环境准备

1. root用户登录三台虚拟机，配置静态IP

   ```shell
   vi /etc/sysconfig/network-scripts/ifcfg-ens33
   ```

   ```properties
   #网络类型为以太网
   TYPE="Ethernet"
   
   #IP类型为静态IP
   BOOTPROTO="static"
   
   #配置网卡名
   NAME="ens33"
   DEVICE="ens33"
   
   #是否开机激活网卡
   ONBOOT="yes"
   
   #配置本机的内网IP
   IPADDR=192.168.145.202
   
   #配置集群的内网网关和DNS
   GATEWAY=192.168.145.2
   DNS1=192.168.145.2
   ```

2. 修改三台主机的主机名称

   ```shell
   vi /etc/hostname
   ```

   ```shell
   #以hadoop201为例
   hadoop201
   ```

3. 三台主机添加主机IP映射

   ```shell
   vi /etc/hosts
   ```

   ```properties
   #添加映射
   192.168.1.201 hadoop201
   192.168.1.202 hadoop202
   192.168.1.203 hadoop203
   ```

4. 修改windows系统的hosts文件，用于使用工具远程访问

   ```properties
   #修改 C:\Windows\System32\drivers\etc\hosts
   #添加映射
   192.168.1.201 hadoop201
   192.168.1.202 hadoop202
   192.168.1.203 hadoop203
   ```

5. 关闭三台主机的防火墙服务

   ```shell
   #关闭防火墙服务
   systemctl stop firewalld
   #关闭防火墙自启动
   systemctl disable firewalld
   ```

6. 在三台主机上添加普通用户atguigu

   ```shell
   #添加用户
   useradd atguigu
   #设置用户密码
   passwd atguigu
   ```

7. 为atguigu用户设置root权限

   ```shell
   vi /etc/sudoers
   ```

   ```shell
   #找到root ALL=(ALL) ALL行,在下一行添加
   lcx	ALL=(ALL)	NOPASSWD:ALL
   ```

8. 创建atguigu用户的工作目录

   ```shell
   mkdir /opt/module /opt/software
   #修改目录用户及用户组
   chown atguigu:atguigu /opt/module /opt/software
   ```

9. 重启生效所有配置

   ```shell
   reboot
   ```



### 2.2 修改yum源，安装常用rpm包

1. 安装wget，用于从指定URL下载文件

   ```shell
   yum install wget
   ```

2. 备份CentOS-Base.repo文件

   ```shell
   cd /etc/yum.repos.d
   cp CentOS-Base.repo CentOS-Base.repo.backup
   ```

3. 下载阿里云的repos文件

   ```shell
   wget http://mirrors.aliyun.com/repo/Centos-7.repo
   ```

4. 重命名下载的Centos-7.repo文件

   ```shell
   mv Centos-7.repo CentOS-Base.repo
   ```

5. 清理缓存数据，并缓存新服务器的包信息

   ```shell
   yum clean all
   yum makecache
   ```

6. 删除备份的CentOS-Base.repo文件（可选）

   ```shell
   rm CentOS-Base.repo.backup
   ```

7. 安装常用rpm包

   ```shell
   yum install -y epel-release
   yum install -y psmisc nc net-tools rsync vim lrzsz ntp libzstd openssl-static tree iotop
   ```



### 2.3 配置集群免密登陆，编写群发脚本

1. 在三台主机间，使用atguigu用户配置ssh免密登陆

   ```shell
   #分别三台主机执行以下所有命令
   #1.生成公钥和私钥,三次回车确认
   ssh-keygen -t rsa
   #2.将公钥分发到目标主机
   ssh-copy-id hadoop201
   ssh-copy-id hadoop202
   ssh-copy-id hadoop203
   ```

2. 在hadoop201配置root用户免密登陆三台主机

   ```shell
   su - root
   ssh-keygen -t rsa
   ssh-copy-id hadoop201
   ssh-copy-id hadoop202
   ssh-copy-id hadoop203
   exit
   ```

3. 编写群发脚本

   ```shell
   vi /home/atguigu/bin/xsync
   ```

   ```shell
   #!/bin/bash
   #1. 判断参数个数
   if [ $# -lt 1 ]
   then
     echo Not Enough Arguement!
     exit;
   fi
   #2. 遍历集群所有机器
   for host in hadoop202 hadoop203
   do
     echo ====================  $host  ====================
     #3. 遍历所有目录，挨个发送
     for file in $@
     do
       #4 判断文件是否存在
       if [ -e $file ]
       then
         #5. 获取父目录
         pdir=$(cd -P $(dirname $file); pwd)
         #6. 获取当前文件的名称
         fname=$(basename $file)
         ssh $host "mkdir -p $pdir"
         rsync -av $pdir/$fname $host:$pdir
       else
         echo $file does not exists!
       fi
     done
   done
   ```

   ```shell
   chmod 777 /home/atguigu/bin/xsync
   ```

4. 复制群发脚本到root用户bin目录

   ```shell
   sudo cp /home/atguigu/bin/xsync /bin/xsync
   #复制后可直接使用sudo xsync命令进行root用户的群发
   ```



### 2.4 编写集群群关脚本

```shell
vi /home/atguigu/shutdownall
```

```shell
#!/bin/bash
for i in hadoop203 hadoop202 hadoop201
do
        echo "----------- $i ----------"
        ssh $i sudo shutdown now
done
```

```shell
chmod 777 /home/atguigu/shutdownall
```



## 3.  部署Java环境（v1.8）

### 3.1 安装JDK

1. 在三台主机上卸载现有的JDK

   ```shell
   sudo rpm -qa | grep -i java | xargs -n1 sudo rpm -e --nodeps
   ```

2. 上传jdk包到hadoop201，解压安装到指定目录

   ```shell
   tar -zxvf jdk-8u212-linux-x64.tar.gz -C /opt/module/
   ```

3. 配置JDK到环境变量

   ```shell
   sudo vim /etc/profile.d/my_env.sh
   ```

   ```shell
   #添加以下内容
   #JAVA_HOME
   export JAVA_HOME=/opt/module/jdk1.8.0_212
   
   export PATH=$PATH:$JAVA_HOME/bin
   ```

   ```shell
   source /etc/profile.d/my_env.sh
   ```

4. 分发JDK及环境变量配置到hadoop202，hadoop203

   ```shell
   xsync /opt/module/jdk1.8.0_212
   sudo xsync /etc//profile.d/my_env.sh
   ```

   ```shell
   #在hadoop202,hadoop203上加载配置文件
   source /etc/profile.d/my_env.sh
   ```

5. 在三台主机上测试安装结果

   ```shell
   java -version
   ```




### 3.2. 准备模拟日志数据

1. 上传数据模拟组件

   ```shell
   mkdir -p /opt/module/applog
   ```

2. 测试数据生成命令

   ```shell
   cd /opt/module/applog
   java -jar gmall2020-mock-log-2020-04-01.jar
   
   #结束后生成log(模拟数据)和logs(执行日志)两个目录
   ```

3. 编写生成数据脚本

   ```shell
   vi /home/atguigu/bin/lg.sh
   ```

   ```shell
   #!/bin/bash
   for i in hadoop201 hadoop202; do
       echo "========== $i =========="
       ssh $i "cd /opt/module/applog/; java -jar gmall2020-mock-log-2020-04-01.jar 1>/dev/null 2>&1 &"
   done
   
   #命令说明
   # /dev/null 黑洞,表示linux的空设备文件,写入该路径的内容会直接丢弃
   # 标准输入0 从键盘获取输入,可使用 0< 从指定位置获取输入
   # 标准输出1 输出日志到控制台,可使用 1> 重定向日志输出,此脚本中输出到黑洞直接丢弃日志内容
   # 错误输出2 输出错误到控制台,可使用 2> 重定向错误输出,此脚本中错误输出方式同日志输出方式
   # & 非阻塞命令,在后台执行,控制台可继续其他操作
   # nohup 关闭当前控制台窗口后继续运行
   ```



## 4. 部署Hadoop集群（v3.1.3）

### 4.1 部署并配置分布式集群

1.  上传hadoop压缩包，解压安装到指定目录

   ```shell
   tar -zxvf hadoop-3.1.3.tar.gz -C /opt/module/
   ```

2. 配置hadoop环境变量

   ```shell
   vi /etc/profile.d/my_env.sh
   ```

   ```shell
   #JAVA_HOME
   export JAVA_HOME=/opt/module/jdk1.8.0_212
   #HADOOP_HOME
   export HADOOP_HOME=/opt/module/hadoop-3.1.3
   
   export PATH=$PATH:$JAVA_HOME/bin:$HADOOP_HOME/bin:$HADOOP_HOME/sbin
   ```

   ```shell
   #分发环境变量配置
   sudo xsync /etc/profile.d/my_env.sh
   #三台主机生效环境变量
   source /etc/profile.d/my_env.sh
   ```

3. 修改配置文件**core-site.xml**

   ```shell
   vi /opt/module/hadoop-3.1.3/etc/hadoop/core-site.xml
   ```

   ```xml
   <configuration>
       <!-- 配置NameNode节点端口 -->
       <property>
           <name>fs.defaultFS</name>
           <value>hdfs://hadoop201:8020</value>
       </property>
       <!-- 配置hdfs的本地存储目录 -->
       <property>
           <name>hadoop.tmp.dir</name>
           <value>/opt/module/hadoop-3.1.3/data</value>
       </property>
       <!-- hive的代理配置,允许atguigu用户代理所有节点的所有组的用户 -->
       <property>
           <name>hadoop.proxyuser.atguigu.hosts</name>
           <value>*</value>
       </property>
       <property>
           <name>hadoop.proxyuser.atguigu.groups</name>
           <value>*</value>
       </property>
       <!-- 开启web端指定用户的操作权限 -->
       <property>
           <name>hadoop.http.staticuser.user</name>
           <value>atguigu</value>
       </property>
   </configuration>
   ```

4. 修改配置文件**hdfs-site.xml**

   ```shell
   vi /opt/module/hadoop-3.1.3/etc/hadoop/hdfs-site.xml
   ```

   ```xml
   <configuration>
       <property>
           <!-- 配置SecondaryNameNode的主机节点 -->
           <name>dfs.namenode.secondary.http-address</name>
           <value>hadoop203:9868</value>
       </property>
   </configuration>
   ```

5. 修改配置文件**yarn-site.xml**

   ```shell
   vi /opt/module/hadoop-3.1.3/etc/hadoop/yarn-site.xml
   ```

   ```xml
   <configuration>
       <!-- 配置shuffle方式 -->
       <property>
           <name>yarn.nodemanager.aux-services</name>
           <value>mapreduce_shuffle</value>
       </property>
       <!-- 配置Yarn所在节点 -->
       <property>
           <name>yarn.resourcemanager.hostname</name>
           <value>hadoop202</value>
       </property>
       <property>
           <name>yarn.nodemanager.env-whitelist</name>
           <value>JAVA_HOME,HADOOP_COMMON_HOME,HADOOP_HDFS_HOME,HADOOP_CONF_DIR,CLASSPATH_PREPEND_DISTCACHE,HADOOP_YARN_HOME,HADOOP_MAPRED_HOME</value>
       </property>
       <!-- 1个Container可获取的最小内存 -->
       <property>
           <name>yarn.scheduler.minimum-allocation-mb</name>
           <value>512</value>
       </property>
       <!-- 1个Container可获取的最大内存 -->
       <property>
           <name>yarn.scheduler.maximum-allocation-mb</name>
           <value>4096</value>
       </property>
       <!-- 1个NodeManeger可支配的最大内存 -->
       <property>
           <name>yarn.nodemanager.resource.memory-mb</name>
           <value>4096</value>
       </property>
   </configuration>
   ```

6. 修改配置文件**mapred-site.xml**

   ```shell
   vi /opt/module/hadoop-3.1.3/etc/hadoop/yarn-site.xml
   ```

   ```xml
   <configuration>
       <!-- 配置mr的运行位置 -->
       <property>
           <name>mapreduce.framework.name</name>
           <value>yarn</value>
       </property>
   </configuration>
   ```

7. 配置**workers**（hadoop2以前为**slaves**），添加所有集群节点

   ```shell
   vi /opt/module/hadoop-3.1.3/etc/hadoop/workers
   ```

   ```shell
   hadoop201
   hadoop202
   hadoop203
   #注意不允许空格空行
   ```



### 4.2 配置历史服务器和日志聚集

1. 修改配置文件**mapred-site.xml**

   ```shell
   vi /opt/module/hadoop-3.1.3/etc/hadoop/mapred-site.xml
   ```

   ```xml
   <!-- 添加以下配置 -->
   <configuration>
       <!-- 历史服务器端地址 -->
       <property>
           <name>mapreduce.jobhistory.address</name>
           <value>hadoop201:10020</value>
       </property>
       <!-- 历史服务器web端地址 -->
       <property>
           <name>mapreduce.jobhistory.webapp.address</name>
           <value>hadoop201:19888</value>
       </property>
   </configuration>
   ```

2. 修改配置文件**yarn-site.xml**

   ```shell
   vi /opt/module/hadoop-3.1.3/etc/hadoop/yarn-site.xml
   ```

   ```xml
   <!-- 添加以下配置 -->
   <configuration>
       <!-- 开启日志聚集 -->
       <property>
           <name>yarn.log-aggregation-enable</name>
           <value>true</value>
       </property>
       <!-- 关联历史服务器 -->
       <property>  
           <name>yarn.log.server.url</name>  
           <value>http://hadoop201:19888/jobhistory/logs</value>
       </property>
       <property>
           <name>yarn.log-aggregation.retain-seconds</name>
           <value>604800</value>
       </property>
   </configuration>
   ```

3. 分发hadoop到所有节点

   ```shell
   xsync /opt/module/hadoop-3.1.3
   ```



### 4.3 集群的群起和群停

1. 首次启动集群需要确认安装目录下无data和logs目录，然后**格式化NameNode**

   ```shell
   hdfs namenode -format
   ```

2. 在NameNode节点**群起NameNode和DataNode**

   ```shell
   #启动
   start-dfs.sh
   #关闭
   stop-dfs.sh
   ```

3. 在ResourceManeger节点**群起ResourceManeger和NodeManager**

   ```shell
   #启动
   start-yarn.sh
   #关闭
   stop-yarn.sh
   ```

4. 启动历史服务器（可选）

   ```shell
   #启动
   mapred --daemon start historyserver
   #关闭
   mapred --daemon stop historyserver
   ```

5. web端查看确认集群状态

   ```shell
   NameNode	hadoop201:9870
   Yarn	hadoop202:8088
   SecondaryNameNode	hadoop203:9868 
   #2NN页面端异常解决方法
   https://blog.csdn.net/adsl624153/article/details/100040310
   ```
   
6. 编写群起群停脚本

   ```shell
   cd bin/
   vi hadoopctl
   ```

   ```shell
   #!/bin/bash
   
   if [ $# -lt 1 ]
     then
       echo "Input args is null"
     exit
   fi
   case $1 in
   "start")
       echo "---------- hdfs start -----------"
       ssh hadoop201 /opt/module/hadoop-3.1.3/sbin/start-dfs.sh
       echo "---------- yarn start -----------"
       ssh hadoop202 /opt/module/hadoop-3.1.3/sbin/start-yarn.sh
       ssh hadoop201 mapred --daemon start historyserver #启动历史服务器
   ;;
   "stop")
       echo "---------- yarn stop -----------"
       ssh hadoop202 /opt/module/hadoop-3.1.3/sbin/stop-yarn.sh
       ssh hadoop201 mapred --daemon stop historyserver #关闭历史服务器
       echo "---------- hdfs stop -----------"
       ssh hadoop201 /opt/module/hadoop-3.1.3/sbin/stop-dfs.sh
   ;;
   *)
     echo "Input args not found"
   ;;
   esac
   ```

   ```shell
   chmod 777 hadoopctl
   ```



### 4.4 多目录存储与数据均衡

1. 查看服务器磁盘挂载点

   ```shell
   df -h
   ```

2. 修改配置文件hdfs-site.xml

   ```shell
   vi /opt/module/hadoop-3.1.3/etc/hadoop/hdfs-site.xml
   ```

   ```xml
   <!-- 添加以下配置 -->
   <configuration>
       <property>
           <!-- 配置hdfs的data存储路径,默认为file://${hadoop.tmp.dir}/dfs/data -->
           <name>dfs.datanode.data.dir</name>
           <!-- 添加不同磁盘的挂载点路径 -->
           <value>file:///dfs/data1,file:///hd2/dfs/data2,file:///hd3/dfs/data3,file:///hd4/dfs/data4</value>
       </property>
   </configuration>
   ```

3. 开启主机节点间数据均衡

   ```shell
   start-balancer.sh -threshold 10
   #命令说明:平衡集群中所有节点的磁盘空间利用率不超过10%
   
   #开启后hadoop开始进行节点间的数据迁移,可使用命令随时终止数据均衡
   stop-balancer.sh
   ```

4. 开启节点下多个磁盘间的数据均衡

   ```shell
   #1.生成均衡计划
   hdfs diskbalancer -plan hadoop201
   
   #2.执行均衡计划
   hdfs diskbalancer -execute hadoop201.plan.json
   
   #3.查看均衡计划执行情况
   hdfs diskbalancer -query hadoop103
   
   #4.取消均衡计划
   hdfs diskbalancer -cancel hadoop103.plan.json
   ```



### 4.5  LZO压缩配置

1. 上传编译完成的lzo包到指定目录

   ```shell
   cp hadoop-lzo-0.4.20.jar /opt/module/hadoop-3.1.3/share/hadoop/common
   ```

2. 将jar包同步到所有主机节点

   ```shell
   xsync /opt/module/hadoop-3.1.3/share/hadoop/common/hadoop-lzo-0.4.20.jar
   ```

3. 修改配置文件，支持LZO压缩

   ```shell
   vi /opt/module/hadoop-3.1.3/etc/hadoop/core-site.xml
   ```

   ```xml
   <!-- 添加以下配置 -->
   <configuration>
       <!-- 可使用的所有压缩格式 -->
       <property>
           <name>io.compression.codecs</name>
           <value>
               org.apache.hadoop.io.compress.GzipCodec,
               org.apache.hadoop.io.compress.DefaultCodec,
               org.apache.hadoop.io.compress.BZip2Codec,
               org.apache.hadoop.io.compress.SnappyCodec,
               com.hadoop.compression.lzo.LzoCodec,
               com.hadoop.compression.lzo.LzopCodec
           </value>
       </property>
       <!-- lzo要求的配置 -->
       <property>
           <name>io.compression.codec.lzo.class</name>
           <value>com.hadoop.compression.lzo.LzoCodec</value>
       </property>
   </configuration>
   ```

   ```shell
   #分发配置文件到所有主机
   xsync /opt/module/hadoop-3.1.3/etc/hadoop/core-site.xml
   ```

4. 重启集群



### 4.6 Hadoop调优

- 修改配置文件hdfs-site.xml，调整并发数
  ![image-20200512213752721](离线数仓搭建.assets/image-20200512213752721.png)

  ```xml
  <!-- 添加以下配置 -->
  <configuration>
      <!-- 根据公式计算并修改datanode并发数 -->
      <property>
          <name>dfs.namenode.handler.count</name>
          <value>10</value>
      </property>
  </configuration>
  ```



## 5. 部署Zookeper集群（v3.5.7）

1. 上传zk压缩包并解压安装到指定目录

   ```shell
   tar -zxvf zookeeper-3.5.7.tar.gz -C /opt/module/
   ```

2. 修改安装目录名

   ```shell
   cd /opt/module
   mv apache-zookeeper-3.5.7-bin/ zookeeper-3.5.7
   ```

3. 修改配置文件zoo.cfg

   ```shell
   cd /opt/module/zookeeper-3.5.7/conf
   #修改配置文件名使其生效
   mv zoo_sample.cfg zoo.cfg
   vi zoo.cfg
   ```

   ```properties
   #修改数据存储路径
   dataDir=/opt/module/zookeeper-3.5.7/zkData
   #添加集群配置
   server.0=hadoop201:2888:3888
   server.1=hadoop202:2888:3888
   server.2=hadoop203:2888:3888
   
   #参数说明
   #格式: server.[节点编号]=[主机名]:[端口号1]:[端口号2]
   #节点编号	自定义的数字,标识集群中的每个的节点
   #主机名	Zookeeper集群需要部署的主机地址
   #端口号1	Follower与Leader交互的端口
   #端口号2	Leader节点失效后,其余节点进行选举使用的临时交互端口
   ```

4. 创建数据存储目录并新建myid文件

   ```shell
   #创建配置文件中的数据存储目录
   mkdir /opt/module/zookeeper-3.5.7/zkData
   #配置主机编号
   vi /opt/module/zookeeper-3.5.7/zkData/myid
   ```

   ```shell
   #直接写入配置文件中的节点编号
   1
   ```

5. 分发Zookeeper到所有节点

   ```shell
   xsync /opt/module/zookeeper-3.5.7
   #所有主机节点依次修改节点编号
   vi /opt/module/zookeeper-3.5.7/zkData/myid
   ```

6. 三台主机分别启动Zookeeper

   ```shell
   #启动
   bin/zkServer.sh start
   #关闭
   bin/zkServer.sh stop
   ```

7. 编写Zookeeper集群群起群停脚本

   ```shell
   vi /home/atguigu/bin/zkctl
   ```

   ```shell
   #! /bin/bash
   case $1 in
   "start"){
   	for i in hadoop201 hadoop202 hadoop203
   	do
           echo ---------- zookeeper $i 启动 ------------
   		ssh $i "/opt/module/zookeeper-3.5.7/bin/zkServer.sh start"
   	done
   };;
   "stop"){
   	for i in hadoop201 hadoop202 hadoop203
   	do
           echo ---------- zookeeper $i 停止 ------------    
   		ssh $i "/opt/module/zookeeper-3.5.7/bin/zkServer.sh stop"
   	done
   };;
   "status"){
   	for i in hadoop201 hadoop202 hadoop203
   	do
           echo ---------- zookeeper $i 状态 ------------    
   		ssh $i "/opt/module/zookeeper-3.5.7/bin/zkServer.sh status"
   	done
   };;
   esac
   ```

   ```shell
   chmod 777 /home/atguigu/bin/zkctl
   ```



## 6. 部署Kafka集群（v2.11-2.4.1）

### 6.1 kafka集群部署

1. 上传kafka压缩包，解压安装到指定目录，kafka版本号命名方式为scala版本号+kafka版本号

   ```shell
   tar -zxvf kafka_2.11-2.4.1.tgz -C /opt/module/
   ```

2. 配置kafka环境变量

   ```shell
   sudo vi /etc/profile.d/my_env.sh
   ```

   ```shell
   #JAVA_HOME
   export JAVA_HOME=/opt/module/jdk1.8.0_212
   #HADOOP_HOME
   export HADOOP_HOME=/opt/module/hadoop-3.1.3
   #KAFKA_HOME
   export KAFKA_HOME=/opt/module/kafka_2.11-2.4.1
   
   export PATH=$PATH:$JAVA_HOME/bin:$HADOOP_HOME/bin:$HADOOP_HOME/sbin:$KAFKA_HOME/bin
   ```

   ```shell
   #分发环境变量配置
   sudo xsync /etc/profile.d/my_env.sh
   #加载环境变量
   source /etc/profile.d/my_env.sh
   ```

3. 在kafka安装目录下创建logs目录

   ```shell
   mkdir /opt/module/kafka_2.11-2.4.1/logs
   ```

4. 修改配置文件

   ```shell
   vi /opt/module/kafka_2.11-2.4.1/conf/server.properties
   ```

   ```properties
   #修改或者添加以下配置
   #broker的全局唯一编号，不能重复
   broker.id=1
   #启用topic删除功能
   delete.topic.enable=true
   #kafka运行日志及topic数据存放目录
   log.dirs=/opt/module/kafka/logs
   #配置连接的Zookeeper的集群地址
   zookeeper.connect=hadoop102:2181,hadoop103:2181,hadoop104:2181/kafka
   ```

5. 分发Kafka到所有主机节点，并修改broker.id

   ```shell
   xsync /opt/module/kafka_2.11-2.4.1
   #在分发的节点下修改broker.id
   vi /opt/module/kafka_2.11-2.4.1/conf/server.properties
   ```

6. 依次在三台主机节点上启动kafka

   ```shell
   #启动
   kafka-server-start.sh -daemon /opt/module/kafka/config/server.properties
   #关闭
   kafka-server-stop.sh
   ```

7. 编写kafka群起群停脚本

   ```shell
   vi /home/atguigu/kafkactl
   ```

   ```shell
   #! /bin/bash
   case $1 in
   "start"){
       for i in hadoop201 hadoop202 hadoop203
       do
           echo " --------启动 $i Kafka-------"
           ssh $i "/opt/module/kafka_2.11-2.4.1/bin/kafka-server-start.sh -daemon /opt/module/kafka/config/server.properties "
       done
   };;
   "stop"){
       for i in hadoop201 hadoop202 hadoop203
       do
           echo " --------停止 $i Kafka-------"
           ssh $i "/opt/module/kafka_2.11-2.4.1/bin/kafka-server-stop.sh stop"
       done
   };;
   esac
   ```

   ```shell
   chmod 777 /home/atguigu/kafkactl
   ```



### 6.2 部署kafka eagle监控（v1.4.5）

1. 安装mysql

2. 下载并上传Kafka Eagle安装包到本地

3. 解压安装包，修改文件名

   ```shell
   tar -zxvf kafka-eagle-bin-1.4.5.tar.gz
   cd kafka-eagle-bin-1.4.5
   tar -zxvf kafka-eagle-web-1.4.5-bin.tar.gz -C /opt/module
   mv kafka-eagle-web-1.4.5 eagle-1.4.5
   ```

4. 配置环境变量

   ```shell
   sudo vi /etc/profile.d/my_env.sh
   ```

   ```properties
   #JAVA_HOME
   export JAVA_HOME=/opt/module/jdk1.8.0_212
   #HADOOP_HOME
   export HADOOP_HOME=/opt/module/hadoop-3.1.3
   #HIVE_HOME
   export HIVE_HOME=/opt/module/hive-3.1.2
   #KAFKA_HOME
   export KAFKA_HOME=/opt/module/kafka_2.11-2.4.1
   #KE_HOME
   export KE_HOME=/opt/module/eagle-1.4.5
   export PATH=$PATH:$JAVA_HOME/bin:$HADOOP_HOME/bin:$HADOOP_HOME/sbin:$HIVE_HOME/bin:$KAFKA_HOME/bin:$KE_HOME/bin
   ```

5. 赋予启动文件执行权限

   ```shell
   chmod 777 bin/ke.sh
   ```

6. 修改配置文件

   ```shell
   vim conf/system-config.properties
   ```

   ```properties
   #修改以下配置项
   #为KE监控的kafka集群命名,支持多个集群
   kafka.eagle.zk.cluster.alias=cluster1
   
   #配置集群cluster1的在zk中维护的数据位置
   cluster1.zk.list=hadoop201:2181,hadoop202:2181,hadoop203:2181/kafka
   
   #配置集群cluster1的offset的存储位置
   cluster1.kafka.eagle.offset.storage=kafka
   
   kafka.eagle.metrics.charts=true
   
   kafka.eagle.sql.fix.error=false
   
   #打开注释,配置mysql服务
   kafka.eagle.driver=com.mysql.jdbc.Driver
   kafka.eagle.url=jdbc:mysql://hadoop201:3306/ke?useUnicode=true&characterEncoding=UTF-8&zeroDateTimeBehavior=convertToNull
   #配置mysql的用户密码
   kafka.eagle.username=root
   kafka.eagle.password=123456
   ```

7. 修改kafka启动命令

   ```shell
   vi $KAFKA_HOME/bin/kafka-server-start.sh
   ```

   ```shell
   #修改以下内容
   if [ "x$KAFKA_HEAP_OPTS" = "x" ]; then
       export KAFKA_HEAP_OPTS="-Xmx1G -Xms1G"
   fi
   
   #修改为
   if [ "x$KAFKA_HEAP_OPTS" = "x" ]; then
       export KAFKA_HEAP_OPTS="-server -Xms2G -Xmx2G -XX:PermSize=128m -XX:+UseG1GC -XX:MaxGCPauseMillis=200 -XX:ParallelGCThreads=8 -XX:ConcGCThreads=5 -XX:InitiatingHeapOccupancyPercent=70"
       export JMX_PORT="9999"
       #export KAFKA_HEAP_OPTS="-Xmx1G -Xms1G"
   fi
   ```

   ```shell
   #同步到所有kafka集群节点
   xsync kafka-server-start.sh
   ```

8. 依次启动zk，kafka，kafka eagle

   ```shell
   #启动ke
   ke.sh start
   #关闭ke
   ke.sh stop
   ```

9. 使用控制台显示的用户密码访问web端 http://hadoop201:8048/ke



### 6.3 kafka相关计算

- kafka集群节点数计算

  ```shell
  #经验计算公式
  Kafka主机数量 = 2 * 峰值生产速度 * 副本数 / 100 + 1
  ```

- kafka的Topic分区数计算

  ```shell
  #计算公式
  分区数 = 目标吞吐量 / min(producer吞吐量, customer吞吐量)
  ```



## 7. 部署Flume（v1.9.0）

### 7.1 flume部署

1. 上传flume安装包，解压安装到指定目录

   ```shell
   tar -zxf /opt/software/apache-flume-1.9.0-bin.tar.gz -C /opt/module/
   ```

2. 重命名flume目录名

   ```shell
   mv /opt/module/apache-flume-1.9.0-bin /opt/module/flume-1.9.0
   ```

3. 删除与hadoop冲突的jar包

   ```shell
   rm /opt/module/flume-1.9.0/lib/guava-11.0.2.jar
   ```

4. 修改配置文件名并添加Java环境变量

   ```shell
   cd /opt/module/flume-1.9.0/conf
   mv flume-env.sh.template flume-env.sh
   vi flume-env.sh
   ```

   ```shell
   #添加Java环境变量
   export JAVA_HOME=/opt/module/jdk1.8.0_212
   ```

5. 分发flume

   ```shell
   xsync /opt/module/flume-1.9.0/
   ```



### 7.2 flume组件的选型

#### 7.2.1 Source

- **exec source**：使用shell命令收集数据

  ```properties
  #exec source的相关配置,示例中agent名为a1,source名为r1
  
  #必须配置
  #配置r1的source类型为exec
  a1.sources.r1.type = exec
  #配置执行的具体命令
  a1.sources.r1.command = tail -F /var/log/secure
  #配置使用的channel
  a1.sources.r1.channels = c1
  
  #重要配置
  #配置1个putList最大装载的event个数,数量满足时执行提交
  batchSize = 20
  #配置未达到batchSize的putList的最大等待时间,时间结束自动执行提交到channel
  batchTimeout = 3000
  
  #可选配置
  #配置用于执行命令的外部程序
  a1.sources.r1.shell = /bin/bash -c
  #配置channel selector的类型replicating(默认)/multiplexing
  a1.sources.r1.selector.type = replicating
  ```

- **spooling directory source**：监控指定目录，目录下文件一旦添加不可修改，支持断点续传

  ```properties
  #spooling directory source的相关配置,示例中agent名为a1,source名为r1
  
  #必须配置
  #配置r1的source类型为spooldir
  a1.sources.r1.type = spooldir
  #监控的目录
  a1.sources.r1.spoolDir = /var/log/apache/flumeSpool
  #配置使用的channel
  a1.sources.r1.channels = c1
  
  #重要配置
  #监听目录下的新增文件在读取完成后追加的后缀
  a3.sources.r1.fileSuffix = .COMPLETED
  #是否添加绝对路径到event的header
  a3.sources.r1.fileHeader = false
  #存储source文件元数据的目录,实现断点续传
  a3.sources.r1.trackerDir = /opt/module/flume/trackDir
  #配置1个putList最大装载的event个数,数量满足时执行提交,1个event大小约1k时建议配置为500-1000
  a1.sources.r1.batchSize	= 100
  
  #可选配置
  #定义忽略的文件名(使用正则表达式)
  a3.sources.r1.ignorePattern = ([^ ]*\.tmp)
  #配置channel selector的类型replicating(默认)/multiplexing
  a1.sources.r1.selector.type = replicating
  ```

- **taildir source**：监控指定目录，包括目录下文件内容的追加操作，支持断点续传

  ```properties
  #taildir source的相关配置,示例中agent名为a1,source名为r1
  
  #必须配置
  #配置r1的source类型为TAILDIR
  a1.sources.r1.type = TAILDIR
  #配置监控的目录群和每个目录的路径
  a1.sources.r1.filegroups = f1 f2
  a1.sources.r1.filegroups.f1 = /var/log/test1/example.log
  a1.sources.r1.filegroups.f2 = /var/log/test2/.*log.*
  
  #重要配置
  #配置json文件的存储目录,实现断点续传
  a1.sources.r1.positionFile = /var/log/flume/taildir_position.json
  #配置1个putList最大装载的event个数,数量满足时执行提交,1个event大小约1k时建议配置为500-1000
  a1.sources.r1.batchSize	= 100
  
  #可选配置
  #配置channel selector的类型replicating(默认)/multiplexing
  a1.sources.r1.selector.type = replicating
  ```

- **kafka source**：监控kafka集群中某个topic的数据

  ```properties
  #kafka source的相关配置,示例中agent名为a1,source名为r1
  
  #必须配置
  #配置source类型为kafka
  a1.sources.r1.type = org.apache.flume.channel.kafka.KafkaChannel
  #配置kafka集群地址,仅需配置1台节点即可,保险起见可配置2到3台
  a1.sources.r1.kafka.bootstrap.servers = hadoop201:9092,hadoop202:9092,hadoop203:9092
  #配置订阅的topic
  a1.sources.r1.kafka.topics = test1, test2
  
  #重要配置
  #使用正则表达式配置订阅的topic,会覆盖topics中配置的topic
  a1.sources.r1.kafka.topics.regex = ^topic[0-9]$
  #配置每次putList提交的event个数
  a1.sources.r1.batchSize = 5000
  #配置最大等待时间,若event数未达到batchSize的配置值,也自动提交数据到channel
  a1.sources.r1.batchDurationMillis = 2000
  ```



#### 7.2.2 Channel

- **memory channel**：数据存入本地内存，吞吐量大，有丢失数据风险，默认容量为100个event

  ```properties
  #memory channel的相关配置,示例中agent名为a1,channe名为c1
  
  #必须配置
  #配置channel类型为memory
  a1.channels.c1.type = memory
  
  #重要配置
  #配置channel的event总容量
  a1.channels.c1.capacity = 1000
  #配置channel一次输入或输出的event数量
  a1.channels.c1.transactionCapacity = 100
  #配置数据输入或输出时,最大的等待时间,时间结束依然没有成功输入或输出,则回滚数据
  a1.channels.c1.keep-alive = 3
  ```

- **file channel**：数据存入本地磁盘，吞吐量大，数据安全，默认容量为100万个event

  ```properties
  #file channel的相关配置,示例中agent名为a1,channe名为c1
  
  #必须配置
  #配置channel类型为file
  a1.channels.c1.type = file
  
  #重要配置
  #配置检查点文件和它的副本文件的存储目录,尽量配置在不同的磁盘目录下
  a1.channels.c1.checkpointDir = /mnt/flume/checkpoint
  a1.channels.c1.backupCheckpointDir = /mnt/flume/backupCheckpointDir
  #配置channel中数据文件的存储目录
  a1.channels.c1.dataDirs = /mnt/flume/data
  #配置单个数据文件的最大容量
  a1.channels.c1.maxFileSize = 2146435071
  #配置channel的event总容量
  a1.channels.c1.capacity = 1000000
  #配置数据输入或输出时,最大的等待时间,时间结束依然没有成功输入或输出,则回滚数据
  a1.channels.c1.keep-alive = 6
  ```

- **kafka channel**：数据存入远程kafka集群所在磁盘，兼具吞吐量和安全性能

  ```properties
  #kafka channel的相关配置,示例中agent名为a1,channe名为c1
  
  #必须配置
  #配置channel类型为kafka
  a1.channels.c1.type = org.apache.flume.channel.kafka.KafkaChannel
  #配置kafka集群地址,仅需配置1台节点即可,保险起见可配置2到3台
  a1.channels.c1.kafka.bootstrap.servers = hadoop201:9092,hadoop202:9092,hadoop203:9092
  
  #重要配置
  #配置用于充当channel的topic名
  a1.channels.c1.kafka.topic = topic_log
  #event写入kafka channel时,是否保留event中headers的信息
  a1.channels.c1.parseAsFlumeEvent = false
  ```




#### 7.2.3 Sink

- **hdfs sink**：最终输出数据到hdfs集群

  ```properties
  #hfds sink的相关配置,示例中agent名为a1,sink名为k1
  
  #必须配置
  #配置sink类型为hdfs
  a1.sinks.k1.type = hdfs
  #配置sink在hdfs上的存储目录,flume会自动加载本地$HDFS_HOME环境变量中配置的集群地址
  a1.sinks.k1.hdfs.path = /flume/events/%y-%m-%d/%H%M/%S
  
  #解决小文件过多的3个配置,非常重要
  #配置hdfs中生成一个新文件块的间隔时间(秒),生产环境配置为3600
  a4.sinks.k1.hdfs.rollInterval = 60
  #每个文件块的最大容量(Byte),生产环境配置为128M
  a4.sinks.k1.hdfs.rollSize = 134217700
  #每个文件块中的最大event个数(设置为0时不限制)
  a4.sinks.k1.hdfs.rollCount = 0
  
  #重要配置
  #上传后文件名的前缀
  a4.sinks.k1.hdfs.filePrefix = log-
  #是否按照时间进一步划分存储目录;划分的时间间隔;划分的时间单位(second/minute/hour)
  a4.sinks.k1.hdfs.round = true
  a4.sinks.k1.hdfs.roundValue = 1
  a4.sinks.k1.hdfs.roundUnit = hour
  #是否使用本地时间戳
  a4.sinks.k1.hdfs.useLocalTimeStamp = true
  #设置文件存储的方式(SequenceFile/DataStream/CompressedStream)
  a4.sinks.k1.hdfs.fileType = CompressedStream
  #若使用压缩文件方式存储,配置压缩格式
  a1.sinks.k1.hdfs.codeC = lzop
  #配置1个takeList最大装载的event个数,数量满足时执行提交
  a4.sinks.k1.hdfs.batchSize = 100
  ```



### 7.3 编写日志采集配置文件

#### 7.3.1 编写flume拦截器

1. 启动IDEA，新建Maven工程，导入依赖

   ```xml
   <dependencies>
       <!-- flume依赖,为避免打包上传后jar包冲突,scope配置为provided -->
       <dependency>
           <groupId>org.apache.flume</groupId>
           <artifactId>flume-ng-core</artifactId>
           <version>1.9.0</version>
           <scope>provided</scope>
       </dependency>
       <!-- json依赖 -->
       <dependency>
           <groupId>com.alibaba</groupId>
           <artifactId>fastjson</artifactId>
           <version>1.2.62</version>
       </dependency>
   </dependencies>
   <!-- 打包插件 -->
   <build>
       <plugins>
           <plugin>
               <artifactId>maven-compiler-plugin</artifactId>
               <version>2.3.2</version>
               <configuration>
                   <source>1.8</source>
                   <target>1.8</target>
               </configuration>
           </plugin>
           <plugin>
               <artifactId>maven-assembly-plugin</artifactId>
               <configuration>
                   <descriptorRefs>
                       <descriptorRef>jar-with-dependencies</descriptorRef>
                   </descriptorRefs>
               </configuration>
               <executions>
                   <execution>
                       <id>make-assembly</id>
                       <phase>package</phase>
                       <goals>
                           <goal>single</goal>
                       </goals>
                   </execution>
               </executions>
           </plugin>
       </plugins>
   </build>
   ```

2. 编写自定义类继承Interceptor

   ```java
   public class LogInterceptor implements Interceptor {
   
       @Override
       public void initialize() {}
   
       //简单清洗数据,筛除非json类型的日志数据
       @Override
       public Event intercept(Event event) {
           byte[] body = event.getBody();
           String log = new String(body, StandardCharsets.UTF_8);
           try {
               JSON.parse(log);
               return event;
           }catch (JSONException e){
               return null;
           }
       }
   
       //同理筛除list中非json类型的日志数据
       @Override
       public List<Event> intercept(List<Event> list) {
           Iterator<Event> iterator = list.iterator();
           while (iterator.hasNext()){
               Event next = iterator.next();
               if (intercept(next) == null){
                   iterator.remove();
               }
           }
           return list;
       }
   
       //定义静态内部类实现Interceptor.Builder,返回自定义类对象
       public static class Builder implements Interceptor.Builder{
   
           @Override
           public Interceptor build() {
               return new LogInterceptor();
           }
   
           @Override
           public void configure(Context context) {}
       }
   
       @Override
       public void close() {}
   }
   ```

3. 上传包含依赖的jar包到flume安装目录的lib目录下，并分发给所有主机节点

   ```
   xsync /opt/module/flume-1.9.0/lib
   ```



#### 7.3.2 编写日志采集配置文件

1. 创建job目录

   ```shell
   mkdir -p /opt/module/flume-1.9.0/jobs
   ```

2. 编写配置文件

   ```shell
   vi /opt/module/flume-1.9.0/jobs/flume-kafka.conf
   ```

   ```properties
   #为各组件命名
   a1.sources = r1
   a1.channels = c1
   
   #配置source
   a1.sources.r1.type = TAILDIR
   a1.sources.r1.filegroups = f1
   a1.sources.r1.filegroups.f1 = /opt/module/applog/log/app.*
   a1.sources.ri.batchSize = 500
   a1.sources.r1.positionFile = /opt/module/flume-1.9.0/taildir_position.json
   a1.sources.r1.interceptors =  i1
   a1.sources.r1.interceptors.i1.type = com.atguigu.flume.LogInterceptor$Builder
   
   #配置channel
   a1.channels.c1.type = org.apache.flume.channel.kafka.KafkaChannel
   a1.channels.c1.kafka.bootstrap.servers = hadoop201:9092,hadoop202:9092
   a1.channels.c1.kafka.topic = topic_log
   a1.channels.c1.parseAsFlumeEvent = false
   
   #绑定source和channel
   a1.sources.r1.channels = c1
   ```

3. 分发到所有节点

   ```shell
   xsync /opt/module/flume-1.9.0/jobs/flume-kafka.conf
   ```

4. 测试启动flume（已启动zk，kafka）

   ```shell
   bin/flume-ng agent -n a1 -c conf/ -f jobs/flume-kafka.conf -Dflume.root.logger=INFO,LOGFILE
   ```

5. 编写启动脚本

   ```shell
   vi /home/atguigu/bin/f1.sh
   ```

   ```shell
   #!/bin/bash
   case $1 in
   "start"){
           for i in hadoop201 hadoop202
           do
                   echo " --------启动 $i 采集flume-------"
                   ssh $i "nohup /opt/module/flume-1.9.0/bin/flume-ng agent --conf-file /opt/module/flume-1.9.0/jobs/flume-kafka.conf --name a1 -Dflume.root.logger=INFO,LOGFILE >/opt/module/flume-1.9.0/log1.txt 2>&1  &"
           done
   };;
   "stop"){
           for i in hadoop201 hadoop202
           do
                   echo " --------停止 $i 采集flume-------"
                   ssh $i "ps -ef | grep flume-kafka | grep -v grep |awk  '{print \$2}' | xargs -n1 kill -9 "
           done
   };;
   esac
   
   #说明
   # nohup	不挂断,关闭终端不结束进程
   # &	程序运行在后台
   # xargs	取上一命令的所有运行结果作为下一命令的输入参数循环运行
   # -n1	每次取1个参数作为下一个命令的输入参数
   ```

   ```shell
   chmod 777 /home/atguigu/bin/f1.sh
   ```



### 7.4 编写日志消费配置文件

1. 编写配置文件

   ```shell
   vi /opt/module/flume-1.9.0/jobs/kafka-flume-hdfs.conf
   ```

   ```properties
   #组件命名
   a1.sources = r1
   a1.channels = c1
   a1.sinks = k1
   
   #配置source
   a1.sources.r1.type = org.apache.flume.source.kafka.KafkaSource
   a1.sources.r1.batchSize = 5000
   a1.sources.r1.batchDurationMillis = 2000
   a1.sources.r1.kafka.bootstrap.servers = hadoop201:9092,hadoop202:9092,hadoop203:9092
   a1.sources.r1.kafka.topics = topic_log
   
   #配置channel
   a1.channels.c1.type = file
   a1.channels.c1.checkpointDir = /opt/module/flume-1.9.0/checkpoint/behavior1
   a1.channels.c1.dataDirs = /opt/module/flume-1.9.0/data/behavior1/
   a1.channels.c1.maxFileSize = 2146435071
   a1.channels.c1.capacity = 1000000
   a1.channels.c1.keep-alive = 6
   
   #配置sink
   a1.sinks.k1.type = hdfs
   a1.sinks.k1.hdfs.path = /origin_data/gmall/log/topic_log/%Y-%m-%d
   a1.sinks.k1.hdfs.filePrefix = log-
   a1.sinks.k1.hdfs.round = false
   a1.sinks.k1.hdfs.rollInterval = 10
   a1.sinks.k1.hdfs.rollSize = 134217728
   a1.sinks.k1.hdfs.rollCount = 0
   a1.sinks.k1.hdfs.fileType = CompressedStream
   a1.sinks.k1.hdfs.codeC = lzop
   
   #绑定channel
   a1.sources.r1.channels = c1
   a1.sinks.k1.channel = c1
   ```

2. 分发到所有节点

   ```shell
   xsync /opt/module/flume-1.9.0/jobs/kafka-flume-hdfs.conf
   ```

3. 测试启动flume（已启动zk，kafka）

   ```shell
   bin/flume-ng agent -n a1 -c conf/ -f jobs/kafka-flume-hdfs.conf -Dflume.root.logger=INFO,LOGFILE
   ```

4. 编写启动脚本

   ```shell
   vi /home/atguigu/bin/f2.sh
   ```

   ```shell
   #!/bin/bash
   case $1 in
   "start"){
           for i in hadoop203
           do
                   echo " --------启动 $i 消费flume-------"
                   ssh $i "nohup /opt/module/flume-1.9.0/bin/flume-ng agent --conf-file /opt/module/flume-1.9.0/jobs/kafka-flume-hdfs.conf --name a1 -Dflume.root.logger=INFO,LOGFILE >/opt/module/flume-1.9.0/log2.txt   2>&1 &"
           done
   };;
   "stop"){
           for i in hadoop203
           do
                   echo " --------停止 $i 消费flume-------"
                   ssh $i "ps -ef | grep kafka-flume-hdfs | grep -v grep |awk '{print \$2}' | xargs -n1 kill"
           done
   };;
   esac
   
   
   #说明
   # nohup	不挂断,关闭终端不结束进程
   # &	程序运行在后台
   # xargs	取上一命令的所有运行结果作为下一命令的输入参数循环运行
   # -n1	每次取1个参数作为下一个命令的输入参数
   ```

   ```shell
   chmod 777 /home/atguigu/bin/f2.sh
   ```



### 7.5  上传日志数据到hdfs

1. 启动hdfs，yarn

   ```shell
   #hadoop201
   start-dfs.sh
   #hadoop202
   start-yarn.sh
   ```

2. 启动zookeeper，kafka

   ```shell
   #hadoop201
   zkctl start
   kafkactl start
   ```

3. 启动flume进程

   ```shell
   #hadoop201
   f2.sh start
   f1.sh start
   ```

4. 生成日志

   ```shell
   #hadoop201
   #修改application.properties的日期配置
   lg.sh
   ```

   


## 8. 安装MySQL（v5.7.28）

### 8.1 mysql安装

1. 官网下载MySQL镜像，官网地址：http://dev.mysql.com/downloads/mysql/

2. 检查系统是否已经安装了mysql或mariadb（mysql的衍生版本）

   ```shell
   rpm -qa | grep mysql
   rpm -qa | grep mariadb
   #若已经安装则需要卸载
   rpm -e --nodeps  mariadb-libs
   ```

3. 上传mysql安装包

4. 解压mysql安装包

   ```shell
   tar -xvf mysql-5.7.28-1.el7.x86_64.rpm-bundle.tar
   ```

5. 依次安装rpm（因为RPM包间存在依赖关系，必须按照顺序安装）

   ``` shell
   #先安装可能需要的依赖
   yum install -y perl perl-devel
   yum install -y libaio
   #再安装rpm包
   rpm -ivh mysql-community-common-5.7.28-1.el7.x86_64.rpm
   rpm -ivh mysql-community-libs-5.7.28-1.el7.x86_64.rpm
   rpm -ivh mysql-community-libs-compat-5.7.28-1.el7.x86_64.rpm
   rpm -ivh mysql-community-client-5.7.28-1.el7.x86_64.rpm
   rpm -ivh mysql-community-server-5.7.28-1.el7.x86_64.rpm
   ```

6. 删除 /etc/my.cnf 配置文件中 datadir 指向的目录下的所有内容

   ```shell
   #/etc/my.cnf中的内容
   [mysqld]
   datadir=/var/lib/mysql
   
   #删除datadir目录下的所有内容
   cd /var/lib/mysql
   rm -rf ./*
   ```

7. 初始化数据库

   ```shell
   #初始化数据库
   mysqld --initialize --user=mysql
   ```

8. 启动mysql服务

   ``` shell
   systemctl start mysqld.service
   ```

9. 查看自动生成的root用户的12位密码

   ```shell
   cat /var/log/mysqld.log
   ```

10. 登陆mysql数据库

    ```shell
    mysql -uroot -p
    #输入8中生成的初始密码
    ```

11. 首次登陆需要修改密码才能进行后续操作

    ```mysql
    #设置密码为123456
    set password = password("123456");
    ```

12. 设置root用户的远程访问

    ```mysql
    update mysql.user set host = '%' where host = 'root';
    flush privileges;
    #设置后查看
    select Host,User,authentication_string from mysql.user;
    ```

13. 查看并开启mysql服务自启动

    ```shell
    systemctl list-unit-files | grep mysqld.service #查看
    systemctl enable mysqld.service #开启自启动
    systemctl disable mysqld.service #关闭自启动
    ```

14. 修改配置文件 /etc/my.cnf，最终配置如下

    ```properties
    # For advice on how to change settings please see
    # http://dev.mysql.com/doc/refman/5.7/en/server-configuration-defaults.html
    
    [mysqld]
    #修改默认字符集为utf-8
    character_set_server=utf8
    collation-server=utf8_general_ci
    #设置大小写不敏感
    lower_case_table_names=1
    #开启缓存
    query_cache_type=1
    #
    # Remove leading # and set to the amount of RAM for the most important data
    # cache in MySQL. Start at 70% of total RAM for dedicated server, else 10%.
    # innodb_buffer_pool_size = 128M
    #
    # Remove leading # to turn on a very important data integrity option: logging
    # changes to the binary log between backups.
    # log_bin
    #
    # Remove leading # to set options mainly useful for reporting servers.
    # The server defaults are faster for transactions and fast SELECTs.
    # Adjust sizes as needed, experiment to find the optimal values.
    # join_buffer_size = 128M
    # sort_buffer_size = 2M
    # read_rnd_buffer_size = 2M
    datadir=/var/lib/mysql
    socket=/var/lib/mysql/mysql.sock
    
    # Disabling symbolic-links is recommended to prevent assorted security risks
    symbolic-links=0
    
    log-error=/var/log/mysqld.log
    pid-file=/var/run/mysqld/mysqld.pid
    ```



### 8.2 生成业务数据

1. 启动SQLyog，连接mysql

2. 创建数据库gmall

   ```mysql
   create database gmall;
   ```

3. 使用脚本 gmall2020-04-01.sql 导入数据库

4. linux中创建目录，上传数据生成java程序和配置文件

   ```shell
   mkdir -p /opt/module/db_log
   #上传
   gmall2020-mock-db-2020-04-01.jar
   application.properties
   ```

5. 修改配置文件信息

   ```properties
   logging.level.root=info
   #jdbc连接配置
   spring.datasource.driver-class-name=com.mysql.jdbc.Driver
   spring.datasource.url=jdbc:mysql://hadoop201:3306/gmall?characterEncoding=utf-8&useSSL=false&serverTimezone=GMT%2B8
   spring.datasource.username=root
   spring.datasource.password=123456
   
   logging.pattern.console=%m%n
   
   mybatis-plus.global-config.db-config.field-strategy=not_null
   
   #业务日期(每次执行前修改)
   mock.date=2020-03-10
   #是否覆盖上次生成的数据(0-不覆盖,1-覆盖)
   mock.clear=1
   #生成新用户数量
   mock.user.count=1000
   #男性比例
   mock.user.male-rate=20
   #用户数据变化概率
   mock.user.update-rate:20
   #收藏取消比例
   mock.favor.cancel-rate=10
   #收藏数量
   mock.favor.count=100
   #购物车数量
   mock.cart.count=30
   #每个商品最多购物个数
   mock.cart.sku-maxcount-per-cart=3
   #购物车来源  用户查询，商品推广，智能推荐, 促销活动
   mock.cart.source-type-rate=60:20:10:10
   #用户下单比例
   mock.order.user-rate=95
   #用户从购物中购买商品比例
   mock.order.sku-rate=70
   #是否参加活动
   mock.order.join-activity=1
   #是否使用购物券
   mock.order.use-coupon=1
   #购物券领取人数
   mock.coupon.user-count=1000
   #支付比例
   mock.payment.rate=70
   #支付方式 支付宝：微信 ：银联
   mock.payment.payment-type=30:60:10
   #评价比例 好：中：差：自动
   mock.comment.appraise-rate=30:10:10:50
   #退款原因比例：质量问题 商品描述与实际描述不一致 缺货 号码不合适 拍错 不想买了 其他
   mock.refund.reason-rate=30:10:20:5:15:5:5
   ```

6. 执行jar包，数据自动生成并存入mysql数据库

   ```shell
   java -jar gmall2020-mock-db-2020-04-01.jar
   ```



## 9. 部署Sqoop（v1.4.6）

### 9.1 sqoop部署

1. 上传sqoop安装包并解压安装到指定目录

   ```shell
   tar -zxvf sqoop-1.4.6.bin__hadoop-2.0.4-alpha.tar.gz -C /opt/module/
   ```

2. 重命名安装目录

   ```shell
   mv sqoop-1.4.6.bin__hadoop-2.0.4-alpha sqoop-1.4.6
   ```

3. 重命名并修改配置文件

   ```shell
   cd /opt/module/sqoop-1.4.6/conf/
   mv sqoop-env-template.sh sqoop-env.sh
   vi sqoop-env.sh
   ```

   ```shell
   #添加hadoop,hive,zookeeper的相关环境变量
   export HADOOP_COMMON_HOME=/opt/module/hadoop-3.1.3
   export HADOOP_MAPRED_HOME=/opt/module/hadoop-3.1.3
   export HIVE_HOME=/opt/module/hive-3.1.2
   export ZOOKEEPER_HOME=/opt/module/zookeeper-3.5.7
   export ZOOCFGDIR=/opt/module/zookeeper-3.5.7/conf
   ```

4. 上传mysql的jdbc驱动到sqoop的lib目录下

   ```shell
   cp mysql-connector-java-5.1.48.jar /opt/module/sqoop-1.4.6/lib/
   ```

5. 测试sqoop连接mysql数据库

   ```shell
   bin/sqoop list-databases --connect jdbc:mysql://hadoop201:3306/ --username root --password 123456
   #输出mysql中所有数据库
   ```



### 9.2 表的同步策略

- 全量同步策略
  - 使用场景：表数据量不大，每天有新数据的插入和旧数据的修改
  - 同步方式：每天上传全表内容到hdfs
  - 举例：品牌表，商品分类表，活动表
- 增量同步策略
  - 使用场景：表数据量大，每天只有新数据的插入，旧数据不做修改
  - 同步方式：使用时间类型字段筛选，每天只上传当天增加的内容到hdfs
  - 举例：订单状态表，商品评论表，支付流水表
- 新增及变化同步策略
  - 使用场景：表数据量大，每天有新数据的插入和旧数据的修改
  - 同步方式：使用时间类型字段筛选，每天只上传当天新增或修改的内容到hdfs
  - 举例：用户表，订单表，优惠券领用表
- 特殊策略
  - 使用场景
    1. 客观维度（性别，民族，地区等）
    2. 日期维度（日历）
  - 同步方式：只上传一次，无需每天同步



### 9.3 sqoop上传业务数据到hdfs

1. 编写业务数据上传脚本

   ```shell
   cd /home/atguigu/bin
   vi mysql_to_hdfs.sh
   ```

   ```shell
   #! /bin/bash
   #配置sqoop命令变量
   sqoop=/opt/module/sqoop-1.4.6/bin/sqoop
   #上传到hdfs中的分区日期,默认为日期为前一天,可通过第2个传参选择指定日期的数据,如2020-01-01
   if [ -n "$2" ] ;then
       do_date=$2
   else
       do_date=`date -d '-1 day' +%F`
   fi
   #定义通用的import()函数,$1表示表名,$2表示插入数据的sql语句
   import_data(){
   #使用sqoop命令导入数据到hdfs
   $sqoop import \
   -Dmapreduce.job.queuename=hive \
   --connect jdbc:mysql://hadoop201:3306/gmall \
   --username root \
   --password 123456 \
   --target-dir /origin_data/gmall/db/$1/$do_date \
   --delete-target-dir \
   --query "$2 and  \$CONDITIONS" \
   --num-mappers 1 \
   --fields-terminated-by '\t' \
   --compress \
   --compression-codec lzop \
   --null-string '\\N' \
   --null-non-string '\\N'
   #sqoop命令配置说明
   #--null-string	当mysql中字符串字段的值为null时,上传到hdfs存储的方式(此处与hive底层一致使用\N)
   #--null-non-string	当mysql中非字符串字段的值为null时,上传到hdfs存储的方式(此处与hive底层一致使用\N)
   
   #调用lzo的jar包生成索引
   hadoop jar /opt/module/hadoop-3.1.3/share/hadoop/common/hadoop-lzo-0.4.20.jar com.hadoop.compression.lzo.DistributedLzoIndexer -Dmapreduce.job.queuename=hive /origin_data/gmall/db/$1/$do_date
   }
   #定义每个表的import函数
   import_order_info(){
     import_data order_info "select
                               id, 
                               final_total_amount, 
                               order_status, 
                               user_id, 
                               out_trade_no, 
                               create_time, 
                               operate_time,
                               province_id,
                               benefit_reduce_amount,
                               original_total_amount,
                               feight_fee      
                           from order_info
                           where (date_format(create_time,'%Y-%m-%d')='$do_date' 
                           or date_format(operate_time,'%Y-%m-%d')='$do_date')"
   }
   	
   import_coupon_use(){
     import_data coupon_use "select
                             id,
                             coupon_id,
                             user_id,
                             order_id,
                             coupon_status,
                             get_time,
                             using_time,
                             used_time
                           from coupon_use
                           where (date_format(get_time,'%Y-%m-%d')='$do_date'
                           or date_format(using_time,'%Y-%m-%d')='$do_date'
                           or date_format(used_time,'%Y-%m-%d')='$do_date')"
   }
   
   import_order_status_log(){
     import_data order_status_log "select
                                     id,
                                     order_id,
                                     order_status,
                                     operate_time
                                   from order_status_log
                                   where date_format(operate_time,'%Y-%m-%d')='$do_date'"
   }
   
   import_activity_order(){
     import_data activity_order "select
                                   id,
                                   activity_id,
                                   order_id,
                                   create_time
                                 from activity_order
                                 where date_format(create_time,'%Y-%m-%d')='$do_date'"
   }
   
   import_user_info(){
     import_data "user_info" "select 
                               id,
                               name,
                               birthday,
                               gender,
                               email,
                               user_level, 
                               create_time,
                               operate_time
                             from user_info 
                             where (DATE_FORMAT(create_time,'%Y-%m-%d')='$do_date' 
                             or DATE_FORMAT(operate_time,'%Y-%m-%d')='$do_date')"
   }
   
   import_order_detail(){
     import_data order_detail "select 
                                 od.id,
                                 order_id, 
                                 user_id, 
                                 sku_id,
                                 sku_name,
                                 order_price,
                                 sku_num, 
                                 od.create_time,
                                 source_type,
                                 source_id  
                               from order_detail od
                               join order_info oi
                               on od.order_id=oi.id
                               where DATE_FORMAT(od.create_time,'%Y-%m-%d')='$do_date'"
   }
   
   import_payment_info(){
     import_data "payment_info"  "select 
                                   id,  
                                   out_trade_no, 
                                   order_id, 
                                   user_id, 
                                   alipay_trade_no, 
                                   total_amount,  
                                   subject, 
                                   payment_type, 
                                   payment_time 
                                 from payment_info 
                                 where DATE_FORMAT(payment_time,'%Y-%m-%d')='$do_date'"
   }
   
   import_comment_info(){
     import_data comment_info "select
                                 id,
                                 user_id,
                                 sku_id,
                                 spu_id,
                                 order_id,
                                 appraise,
                                 comment_txt,
                                 create_time
                               from comment_info
                               where date_format(create_time,'%Y-%m-%d')='$do_date'"
   }
   
   import_order_refund_info(){
     import_data order_refund_info "select
                                   id,
                                   user_id,
                                   order_id,
                                   sku_id,
                                   refund_type,
                                   refund_num,
                                   refund_amount,
                                   refund_reason_type,
                                   create_time
                                 from order_refund_info
                                 where date_format(create_time,'%Y-%m-%d')='$do_date'"
   }
   
   import_sku_info(){
     import_data sku_info "select 
                             id,
                             spu_id,
                             price,
                             sku_name,
                             sku_desc,
                             weight,
                             tm_id,
                             category3_id,
                             create_time
                           from sku_info where 1=1"
   }
   
   import_base_category1(){
     import_data "base_category1" "select 
                                     id,
                                     name 
                                   from base_category1 where 1=1"
   }
   
   import_base_category2(){
     import_data "base_category2" "select
                                     id,
                                     name,
                                     category1_id 
                                   from base_category2 where 1=1"
   }
   
   import_base_category3(){
     import_data "base_category3" "select
                                     id,
                                     name,
                                     category2_id
                                   from base_category3 where 1=1"
   }
   
   import_base_province(){
     import_data base_province "select
                                 id,
                                 name,
                                 region_id,
                                 area_code,
                                 iso_code
                               from base_province
                               where 1=1"
   }
   
   import_base_region(){
     import_data base_region "select
                                 id,
                                 region_name
                               from base_region
                               where 1=1"
   }
   
   import_base_trademark(){
     import_data base_trademark "select
                                   tm_id,
                                   tm_name
                                 from base_trademark
                                 where 1=1"
   }
   
   import_spu_info(){
     import_data spu_info "select
                               id,
                               spu_name,
                               category3_id,
                               tm_id
                             from spu_info
                             where 1=1"
   }
   
   import_favor_info(){
     import_data favor_info "select
                             id,
                             user_id,
                             sku_id,
                             spu_id,
                             is_cancel,
                             create_time,
                             cancel_time
                           from favor_info
                           where 1=1"
   }
   
   import_cart_info(){
     import_data cart_info "select
                           id,
                           user_id,
                           sku_id,
                           cart_price,
                           sku_num,
                           sku_name,
                           create_time,
                           operate_time,
                           is_ordered,
                           order_time,
                           source_type,
                           source_id
                         from cart_info
                         where 1=1"
   }
   
   import_coupon_info(){
     import_data coupon_info "select
                             id,
                             coupon_name,
                             coupon_type,
                             condition_amount,
                             condition_num,
                             activity_id,
                             benefit_amount,
                             benefit_discount,
                             create_time,
                             range_type,
                             spu_id,
                             tm_id,
                             category3_id,
                             limit_num,
                             operate_time,
                             expire_time
                           from coupon_info
                           where 1=1"
   }
   
   import_activity_info(){
     import_data activity_info "select
                                 id,
                                 activity_name,
                                 activity_type,
                                 start_time,
                                 end_time,
                                 create_time
                               from activity_info
                               where 1=1"
   }
   
   import_activity_rule(){
       import_data activity_rule "select
                                       id,
                                       activity_id,
                                       condition_amount,
                                       condition_num,
                                       benefit_amount,
                                       benefit_discount,
                                       benefit_level
                                   from activity_rule
                                   where 1=1"
   }
   
   import_base_dic(){
       import_data base_dic "select
                               dic_code,
                               dic_name,
                               parent_code,
                               create_time,
                               operate_time
                             from base_dic
                             where 1=1"
   }
   
   case $1 in
     "order_info")
        import_order_info
   ;;
     "base_category1")
        import_base_category1
   ;;
     "base_category2")
        import_base_category2
   ;;
     "base_category3")
        import_base_category3
   ;;
     "order_detail")
        import_order_detail
   ;;
     "sku_info")
        import_sku_info
   ;;
     "user_info")
        import_user_info
   ;;
     "payment_info")
        import_payment_info
   ;;
     "base_province")
        import_base_province
   ;;
     "base_region")
        import_base_region
   ;;
     "base_trademark")
        import_base_trademark
   ;;
     "activity_info")
         import_activity_info
   ;;
     "activity_order")
         import_activity_order
   ;;
     "cart_info")
         import_cart_info
   ;;
     "comment_info")
         import_comment_info
   ;;
     "coupon_info")
         import_coupon_info
   ;;
     "coupon_use")
         import_coupon_use
   ;;
     "favor_info")
         import_favor_info
   ;;
     "order_refund_info")
         import_order_refund_info
   ;;
     "order_status_log")
         import_order_status_log
   ;;
     "spu_info")
         import_spu_info
   ;;
     "activity_rule")
         import_activity_rule
   ;;
     "base_dic")
         import_base_dic
   ;;
   
   "first")
      import_base_category1
      import_base_category2
      import_base_category3
      import_order_info
      import_order_detail
      import_sku_info
      import_user_info
      import_payment_info
      import_base_province
      import_base_region
      import_base_trademark
      import_activity_info
      import_activity_order
      import_cart_info
      import_comment_info
      import_coupon_use
      import_coupon_info
      import_favor_info
      import_order_refund_info
      import_order_status_log
      import_spu_info
      import_activity_rule
      import_base_dic
   ;;
   "all")
      import_base_category1
      import_base_category2
      import_base_category3
      import_order_info
      import_order_detail
      import_sku_info
      import_user_info
      import_payment_info
      import_base_trademark
      import_activity_info
      import_activity_order
      import_cart_info
      import_comment_info
      import_coupon_use
      import_coupon_info
      import_favor_info
      import_order_refund_info
      import_order_status_log
      import_spu_info
      import_activity_rule
      import_base_dic
   ;;
   esac
   ```

2. 修改脚本执行权限

   ```shell
   chmod 777 mysql_to_hdfs.sh
   ```

3. 执行脚本，从mysql导入数据到hdfs

   ```shell
   #初次执行
   mysql_to_hdfs.sh first 2020-06-24
   #每日执行
   mysql_to_hdfs.sh all 2020-06-24
   ```



## 10. 部署Hive（v3.1.2）

- 官方提供的3.1.2版本的Hive在metastore模式下插入数据存在bug，需要安装修复后的版本

### 10.1 hive部署

1. 上传hive压缩包，解压安装到指定目录

   ```shell
   tar -zxvf /opt/software/apache-hive-3.1.2-bin.tar.gz -C /opt/module/
   ```

2. 重命名hive安装目录

   ```shell
   mv /opt/module/apache-hive-3.1.2-bin/ /opt/module/hive-3.1.2
   ```

3. 添加hive到环境变量

   ```shell
   sudo vi /etc/profile.d/my_env.sh
   ```

   ```shell
   #JAVA_HOME
   export JAVA_HOME=/opt/module/jdk1.8.0_212
   #HADOOP_HOME
   export HADOOP_HOME=/opt/module/hadoop-3.1.3
   #KAFKA_HOME
   export KAFKA_HOME=/opt/module/kafka_2.11-2.4.1
   #KE_HOME
   export KE_HOME=/opt/module/eagle-1.4.5
   #HIVE_HOME
   export HIVE_HOME=/opt/module/hive-3.1.2
   export PATH=$PATH:$JAVA_HOME/bin:$HADOOP_HOME/bin:$HADOOP_HOME/sbin:$KAFKA_HOME/bin:$KE_HOME/bin:$HIVE_HOME/bin
   ```

   ```shell
   source /etc/profile.d/my_env.sh
   ```

4. 重命名冲突的jar包

   ```shell
   cd /opt/module/hive-3.1.2
   mv log4j-slf4j-impl-2.10.0.jar log4j-slf4j-impl-2.10.0.jar.bak
   ```

5. 拷贝mysql的JDBC驱动到lib中

   ```shell
   cp mysql-connector-java-5.1.27-bin.jar /opt/module/hive-3.1.2/lib/
   ```

6. 编写hive关联mysql的配置文件

   ```shell
   vi /opt/module/hive-3.1.2/conf/hive-site.xml
   ```

   ```xml
   <?xml version="1.0"?>
   <?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
   <configuration>
       <!-- jdbc连接的URL(mysql地址) -->
       <property>
           <name>javax.jdo.option.ConnectionURL</name>
           <value>jdbc:mysql://hadoop201:3306/metastore?useSSL=false</value>
       </property>
       <!-- jdbc连接的Driver-->
       <property>
           <name>javax.jdo.option.ConnectionDriverName</name>
           <value>com.mysql.jdbc.Driver</value>
       </property>
       <!-- jdbc连接的mysql用户的username-->
       <property>
           <name>javax.jdo.option.ConnectionUserName</name>
           <value>root</value>
       </property>
       <!-- jdbc连接的mysql用户的password -->
       <property>
           <name>javax.jdo.option.ConnectionPassword</name>
           <value>123456</value>
       </property>
       <!-- Hive默认在HDFS的工作目录 -->
       <property>
           <name>hive.metastore.warehouse.dir</name>
           <value>/user/hive/warehouse</value>
       </property>
       <!-- Hive元数据存储版本的验证 -->
       <property>
           <name>hive.metastore.schema.verification</name>
           <value>false</value>
       </property>
       <!-- 指定hiveserver2连接的端口号 -->
       <property>
           <name>hive.server2.thrift.port</name>
           <value>10000</value>
       </property>
       <!-- 指定hiveserver2连接的host -->
       <property>
           <name>hive.server2.thrift.bind.host</name>
           <value>hadoop201</value>
       </property>
       <!-- 元数据存储授权 -->
       <property>
           <name>hive.metastore.event.db.notification.api.auth</name>
           <value>false</value>
       </property>
       <!-- 是否显示字段名(列名) -->
       <property>
           <name>hive.cli.print.header</name>
           <value>true</value>
       </property>
       <!-- 是否显示数据库名 -->
       <property>
           <name>hive.cli.print.current.db</name>
           <value>true</value>
       </property>
           <!-- (可选配置)指定存储元数据要连接的地址
   	配置后hive客户端启动后通过metastore服务连接mysql进而访问数据库,因此启动hive前必须启动metastore服务
   	若不配置该项,则hive客户端直接连接mysql访问数据库 -->
       <property>
           <name>hive.metastore.uris</name>
           <value>thrift://hadoop201:9083</value>
       </property>
   </configuration>
   ```

7. 启动mysql，创建hive元数据库

   ```shell
   mysql -uroot -p123456
   ```

   ```mysql
   create database metastore;
   quit;
   ```

8. 初始化hive元数据库

   ```shell
   schematool -initSchema -dbType mysql -verbose
   ```

9. 启动hive客户端（未配置metastore启动）

   ```shell
   hive
   ```

10. 启动hive客户端（配置metastore启动 / JDBC访问）

    ```shell
    #启动metastore服务
    nohup hive --service metastore 2>&1 &
    #启动hiveServer2服务
    nohup hive --service hiveserver2 2>&1 &
    
    #两种客户端启动方式
    hive
    beeline -u jdbc:hive2://hadoop201:10000 -n atguigu
    ```

    ```shell
    #开启metastore及hiveserver2服务脚本
    #!/bin/bash
    #设置日志输出文件路径
    HIVE_LOG_DIR=$HIVE_HOME/logs
    if [ ! -d $HIVE_LOG_DIR ]
    then
    	mkdir -p $HIVE_LOG_DIR
    fi
    #检查进程是否运行正常，参数1为进程名，参数2为进程端口
    function check_process()
    {
        pid=$(ps -ef 2>/dev/null | grep -v grep | grep -i $1 | awk '{print $2}')
        ppid=$(netstat -nltp 2>/dev/null | grep $2 | awk '{print $7}' | cut -d '/' -f 1)
        echo $pid
        [[ "$pid" =~ "$ppid" ]] && [ "$ppid" ] && return 0 || return 1
    }
    
    function hive_start()
    {
        metapid=$(check_process HiveMetastore 9083)
        cmd="nohup hive --service metastore >$HIVE_LOG_DIR/metastore.log 2>&1 &"
        # nohup开头 表示不挂起,即关闭shell终端窗口也保持运行
        # 2>&1 表示将错误重定向到标准输出上
        # &结尾 表示后台运行
        cmd=$cmd" sleep 4; hdfs dfsadmin -safemode wait >/dev/null 2>&1"
        [ -z "$metapid" ] && eval $cmd || echo "Metastroe服务已启动"
        server2pid=$(check_process HiveServer2 10000)
        cmd="nohup hive --service hiveserver2 >$HIVE_LOG_DIR/hiveServer2.log 2>&1 &"
        [ -z "$server2pid" ] && eval $cmd || echo "HiveServer2服务已启动"
    }
    
    function hive_stop()
    {
        metapid=$(check_process HiveMetastore 9083)
        [ "$metapid" ] && kill $metapid || echo "Metastore服务未启动"
        server2pid=$(check_process HiveServer2 10000)
        [ "$server2pid" ] && kill $server2pid || echo "HiveServer2服务未启动"
    }
    
    case $1 in
    "start")
        hive_start
        ;;
    "stop")
        hive_stop
        ;;
    "restart")
        hive_stop
        sleep 2
        hive_start
        ;;
    "status")
        check_process HiveMetastore 9083 >/dev/null && echo "Metastore服务运行正常" || echo "Metastore服务运行异常"
        check_process HiveServer2 10000 >/dev/null && echo "HiveServer2服务运行正常" || echo "HiveServer2服务运行异常"
        ;;
    *)
        echo Invalid Args!
        echo 'Usage: '$(basename $0)' start|stop|restart|status'
        ;;
    esac
    ```




### 10.2 hive on spark部署（v2.4.5）

#### 3.1.1 Hive on Spark编译

1. 上传spark压缩包并解压安装

2. 进入安装目录执行编译命令

   ```shell
   ./dev/make-distribution.sh --name without-hive --tgz -Pyarn -Phadoop-3.1 -Dhadoop.version=3.1.3 -Pparquet-provided -Porc-provided -Phadoop-provided
   ```

3. 得到编译完成的文件，spark-2.4.5-bin-without-hive.tgz



#### 3.1.2 Hive on Spark部署（v2.4.5）

1. 解压安装spark-2.4.5-bin-without-hive.tgz

   ```shell
   tar -zxvf spark-2.4.5-bin-without-hive.tgz -C /opt/module/
   ```

2. 重命名安装目录

   ```shell
   mv /opt/module/spark-2.4.5-bin-without-hive /opt/module/spark-2.4.5
   ```

3. 添加环境变量

   ```shell
   vi /etc/profile.d/my_env.sh
   ```

   ```shell
   #SPARK_HOME
   export SPARK_HOME=/opt/module/spark-2.4.5
   export PATH=$PATH:$SPARK_HOME/bin
   ```

4. 新建spark配置文件，配置spark的日志存放目录

   ```shell
   cd /opt/module/hive-3.1.2/conf
   vi spark-defaults.conf
   ```

   ```shell
   spark.master            yarn
   spark.eventLog.enabled	true
   spark.eventLog.dir      hdfs://hadoop201:8020/spark-history
   spark.executor.memory   1g
   spark.driver.memory     1g
   ```

5. 在hdfs中创建日志存放目录

   ```shell
   hadoop fs -mkdir /spark-history
   ```

6. 上传Spark相关依赖到hdfs

   ```shell
   hadoop fs -mkdir /spark-jars
   hadoop fs -put /opt/module/spark-2.4.5/jars/* /spark-jars
   ```

7. 修改hive目录下的配置文件，添加配置执行引擎为spark

   ```shell
   vi /opt/module/hive-3.1.2/conf/hive-site.xml
   ```

   ```xml
   <!--Spark依赖位置-->
   <property>
       <name>spark.yarn.jars</name>
       <value>hdfs://hadoop201:8020/spark-jars/*</value>
   </property>
   
   <!--Hive执行引擎-->
   <property>
       <name>hive.execution.engine</name>
       <value>spark</value>
   </property>
   
   <!--Hive和spark连接超时时间-->
   <property>
       <name>hive.spark.client.connect.timeout</name>
       <value>10000ms</value>
   </property>
   ```



#### 3.1.3 Hive on Spark测试

1. 启动hdfs，yarn，hive

   ```shell
   start-dfs.sh
   start-yarn.sh
   hivejdbc start
   hive
   ```

2. 测试创建表并插入数据

   ```mysql
   create external table student(id int, name string) location '/student';
   insert into table student values(1,'abc');
   select * from student;
   ```

3. 异常解决

   ```shell
   #若插入数据时抛出异常
   Caused by: javax.security.sasl.SaslException: Server closed before SASL negotiation finished.
   
   #可修改hadoop的capacity-scheduler.xml配置文件并修改配置,同步到所有hadoop节点
   ```

   ```xml
   <property>
       <name>yarn.scheduler.capacity.maximum-am-resource-percent</name>
       <value>1</value>
   </property>
   ```



#### 3.1.4 增加Hive队列

1. 修改yarn配置文件capacity-scheduler.xml

   ```shell
   vi /opt/module/hadoop-3.1.3/etc/hadoop/capacity-scheduler
   ```

   ```xml
   <!-- 修改如下配置 -->
   <property>
       <name>yarn.scheduler.capacity.root.queues</name>
       <value>default,hive</value>
       <description>
           The queues at the this level (root is the root queue).
       </description>
   </property>
   <property>
       <name>yarn.scheduler.capacity.root.default.capacity</name>
       <value>50</value>
       <description>
           default队列的容量为50%
       </description>
   </property>
   ```

   ```xml
   <!-- 添加新队列的属性配置 -->
   <property>
       <name>yarn.scheduler.capacity.root.hive.capacity</name>
       <value>50</value>
       <description>
           hive队列的容量为50%
       </description>
   </property>
   
   <property>
       <name>yarn.scheduler.capacity.root.hive.user-limit-factor</name>
       <value>1</value>
       <description>
           一个用户最多能够获取该队列资源容量的比例
       </description>
   </property>
   
   <property>
       <name>yarn.scheduler.capacity.root.hive.maximum-capacity</name>
       <value>80</value>
       <description>
           hive队列的最大容量
       </description>
   </property>
   
   <property>
       <name>yarn.scheduler.capacity.root.hive.state</name>
       <value>RUNNING</value>
   </property>
   
   <property>
       <name>yarn.scheduler.capacity.root.hive.acl_submit_applications</name>
       <value>*</value>
       <description>
           访问控制，控制谁可以将任务提交到该队列
       </description>
   </property>
   
   <property>
       <name>yarn.scheduler.capacity.root.hive.acl_administer_queue</name>
       <value>*</value>
       <description>
           访问控制，控制谁可以管理(包括提交和取消)该队列的任务
       </description>
   </property>
   
   <property>
       <name>yarn.scheduler.capacity.root.hive.acl_application_max_priority</name>
       <value>*</value>
       <description>
           访问控制，控制用户可以提交到该队列的任务的最大优先级
       </description>
   </property>
   
   <property>
       <name>yarn.scheduler.capacity.root.hive.maximum-application-lifetime</name>
       <value>-1</value>
       <description>
           hive队列中任务的最大生命时长
       </description>
   </property>
   
   <property>
       <name>yarn.scheduler.capacity.root.hive.default-application-lifetime</name>
       <value>-1</value>
       <description>
           hive队列中任务的默认生命时长
       </description>
   </property>
   ```

2. 分发配置文件

   ```shell
   xsync capacity-scheduler.xml
   ```



## 11. 部署Azkaban（v3.84.4）



### 11.1 azkaban部署

1. Azkaban集群模式部署需要安装3个安装包，db、web包仅需安装1份，server包需要在所有集群节点安装。

2. 上传3个Azkaban安装包并解压

   ```shell
   mkdir /opt/module/azkaban
   tar -zxvf azkaban-db-3.84.4.tar.gz -C /opt/module/azkaban
   tar -zxvf azkaban-exec-server-3.84.4.tar.gz -C /opt/module/azkaban
   tar -zxvf azkaban-web-server-3.84.4.tar.gz -C /opt/module/azkaban
   ```

3. 登陆MySQL，进行相关配置

   ```mysql
   #1.创建Azkaban数据库
   create database azkaban;
   
   #2.创建Azkaban用户并赋予权限
   create user 'azkaban'@'%' identified by '123456';
   grant select, insert, update, delete on azkaban.* to 'azkaban'@'%' with grant option;
   flush privilege;
   
   #3.导入azkaban相关表
   use azkaban;
   source /opt/module/azkaban/azkaban-db-3.84.4/create-all-sql-3.84.4.sql
   ```

4. 修改配置文件，更改MySQL包的大小

   ```shell
   sudo vim /etc/my.cnf
   ```

   ```shell
   #在[mysqld]中添加配置
   max_allowed_packet=1024M
   ```

5. 重启MySQL服务

   ```shell
   sudo systemctl restart mysqld
   ```

6. 修改Excutotr Server的配置文件

   ```shell
   vim /opt/module/azkaban/azkaban-exec-server-3.84.4/conf/azkaban.properties
   ```

   ```properties
   #修改以下配置
   #1.默认时区
   default.timezone.id=Asia/Shanghai
   #2.WebServer地址
   azkaban.webserver.url=http://hadoop201:8081
   #3.mysql配置
   database.type=mysql
   mysql.port=3306
   mysql.host=hadoop201
   mysql.database=azkaban
   mysql.user=azkaban
   mysql.password=123456
   mysql.numconnections=100
   #4.最后添加配置
   executor.metric.reports=true
   executor.metric.milisecinterval.default=60000
   ```

7. 同步Excutor Server到所有Azkaban节点

   ```shell
   xsync /opt/module/azkaban/azkaban-exec-server-3.84.4
   ```

8. 在所有节点上启动并激活Excutor Server

   ```shell
   #注意:由于Azkaban启动时,加载配置文件默认使用了相对路径,因此必须到安装目录下启动脚本
   cd /opt/module/azkaban/azkaban-exec-server-3.84.4
   bin/start-exec.sh
   #激活
   curl -G "localhost:$(<./executor.port)/executor?action=activate" && echo
   #激活成功后出现如下提示
   {"status":"success"}
   ```

9. 修改Web Server的配置文件

   ```shell
   vim /opt/module/azkaban/azkaban-web-server-3.84.4/conf/azkaban.properties
   ```

   ```properties
   #修改以下配置
   #1.修改默认时区
   default.timezone.id=Asia/Shanghai
   #2.配置mysql
   database.type=mysql
   mysql.port=3306
   mysql.host=hadoop201
   mysql.database=azkaban
   mysql.user=azkaban
   mysql.password=123456
   mysql.numconnections=100
   #3.修改配置
   azkaban.executorselector.filters=StaticRemainingFlowSize,CpuStatus
   ```

10. 修改Web Server的用户配置文件

    ```shell
    vim /opt/module/azkaban/azkaban-web-server-3.84.4/conf/azkaban-users.xml
    ```

    ```xml
    <!-- 在<azkaban-users>标签中添加user -->
    <user password="atguigu" roles="metrics,admin" username="atguigu"/>
    ```

11. 启动Web Server

    ```shell
    #注意:由于Azkaban启动时,加载配置文件默认使用了相对路径,因此必须到安装目录下启动脚本
    cd /opt/module/azkaban/azkaban-web-server-3.84.4
    bin/start-web.sh
    ```

12. 打开浏览器查看 http://hadoop201:8081，使用Web Server中的用户登录

13. 关闭Azkaban

    ```shell
    #关闭web server
    bin/shutdown-exec.sh
    #关闭excutor server
    bin/shutdown-web.sh
    ```



### 11.2 mysql接收需求数据

1. 登陆mysql

   ```shell
   mysql -uroot -p123456
   ```

2. 创建数据库

   ```mysql
   create database `gmall_report` character set 'utf8' collate 'utf8_general_ci';
   use gmall_report;
   ```

3. 创建接收表

   ```mysql
   #注意
   #1.从hdfs拉取数据到mysql时,mysql的表在创建时必须指明主键
   #2.为满足后续可视化需求,日期字段必须声明为date类型
   ```

   ```mysql
   -- 用户主题表
   DROP TABLE IF EXISTS `ads_user_topic`;
   CREATE TABLE `ads_user_topic`  (
     `dt` date NOT NULL,
     `day_users` bigint(255) NULL DEFAULT NULL,
     `day_new_users` bigint(255) NULL DEFAULT NULL,
     `day_new_payment_users` bigint(255) NULL DEFAULT NULL,
     `payment_users` bigint(255) NULL DEFAULT NULL,
     `users` bigint(255) NULL DEFAULT NULL,
     `day_users2users` double(255, 2) NULL DEFAULT NULL,
     `payment_users2users` double(255, 2) NULL DEFAULT NULL,
     `day_new_users2users` double(255, 2) NULL DEFAULT NULL,
     PRIMARY KEY (`dt`) USING BTREE
   ) ENGINE = InnoDB CHARACTER SET = utf8 COLLATE = utf8_general_ci ROW_FORMAT = Compact;
   ```

   ```mysql
   -- 地区主题表
   DROP TABLE IF EXISTS `ads_area_topic`;
   CREATE TABLE `ads_area_topic`  (
     `dt` date NOT NULL,
     `id` int(11) NULL DEFAULT NULL,
     `province_name` varchar(255) CHARACTER SET utf8 COLLATE utf8_general_ci NULL DEFAULT NULL,
     `area_code` varchar(255) CHARACTER SET utf8 COLLATE utf8_general_ci NULL DEFAULT NULL,
     `iso_code` varchar(255) CHARACTER SET utf8 COLLATE utf8_general_ci NOT NULL,
     `region_id` int(11) NULL DEFAULT NULL,
     `region_name` varchar(255) CHARACTER SET utf8 COLLATE utf8_general_ci NULL DEFAULT NULL,
     `login_day_count` bigint(255) NULL DEFAULT NULL,
     `order_day_count` bigint(255) NULL DEFAULT NULL,
     `order_day_amount` double(255, 2) NULL DEFAULT NULL,
     `payment_day_count` bigint(255) NULL DEFAULT NULL,
     `payment_day_amount` double(255, 2) NULL DEFAULT NULL,
     PRIMARY KEY (`dt`, `iso_code`) USING BTREE
   ) ENGINE = InnoDB CHARACTER SET = utf8 COLLATE = utf8_general_ci ROW_FORMAT = Compact;
   ```



### 11.3 sqoop导出数据到mysql

1. 编写数据导出脚本

   ```shell
   cd bin/
   vi hdfs_to_mysql.sh
   ```

   ```shell
   #!/bin/bash
   #配置sqoop命令及hive/mysql数据库变量
   sqoop=/opt/module/sqoop-1.4.6/bin/sqoop
   hive_db_name=gmall
   mysql_db_name=gmall_report
   
   #sqoop命令函数 参数1:mysql中的接收表名 参数2:用于更新判定的字段
   export_data() {
   $sqoop export \
   -Dmapreduce.job.queuename=hive \
   --connect "jdbc:mysql://hadoop201:3306/${mysql_db_name}?useUnicode=true&characterEncoding=utf-8"  \
   --username root \
   --password 123456 \
   --table $1 \
   --num-mappers 1 \
   --export-dir /warehouse/$hive_db_name/ads/$1 \
   --input-fields-terminated-by "\t" \
   --update-mode allowinsert \
   --update-key $2 \
   --input-null-string '\\N'    \
   --input-null-non-string '\\N'
   }
   
   case $1 in
     "ads_uv_count")
        export_data "ads_uv_count" "dt"
   ;;
     "ads_user_action_convert_day") 
        export_data "ads_user_action_convert_day" "dt"
   ;;
     "ads_user_topic")
        export_data "ads_user_topic" "dt"
   ;;
     "ads_area_topic")
        export_data "ads_area_topic" "dt,iso_code"
   ;;
      "all")
        export_data "ads_user_topic" "dt"
        export_data "ads_area_topic" "dt,iso_code"
        #其余表省略未写
   ;;
   esac
   ```

   ```shell
   #sqoop命令参数说明
   # update-mode  updateonly(仅update,无法insert) / allowinsert(允许update及insert)
   # update-key  更新数据时用于唯一确定一行数据的字段(主键),多个字段使用","连接
   # input-null-string input-null-non-string 读取文件时判定字段为空的条件(hive中使用\N存储空值)
   ```

2. 授予脚本文件执行权限

   ```shell
   chmod 777 hdfs_to_mysql.sh
   ```

3. 执行脚本

   ```shell
   hdfs_to_mysql.sh all
   ```



### 11.4 配置azkaban调度任务

1. 编写azkaban.project文件

   ```
   azkaban-flow-version: 2.0
   ```

2. 编写gmall.flow文件

   ```yaml
   nodes:
     - name: mysql_to_hdfs
       type: command
       config:
        command: /home/atguigu/bin/mysql_to_hdfs.sh all ${dt}
       
     - name: hdfs_to_ods_log
       type: command
       config:
        command: /home/atguigu/bin/hdfs_to_ods_log.sh ${dt}
        
     - name: hdfs_to_ods_db
       type: command
       dependsOn: 
        - mysql_to_hdfs
       config: 
        command: /home/atguigu/bin/hdfs_to_ods_db.sh all ${dt}
        
     - name: ods_to_dwd_log
       type: command
       dependsOn: 
        - hdfs_to_ods_log
       config: 
        command: /home/atguigu/bin/ods_to_dwd_log.sh ${dt}
       
     - name: ods_to_dwd_db
       type: command
       dependsOn: 
        - hdfs_to_ods_db
       config: 
        command: /home/atguigu/bin/ods_to_dwd_db.sh all ${dt}
       
     - name: dwd_to_dws
       type: command
       dependsOn:
        - ods_to_dwd_log
        - ods_to_dwd_db
       config:
        command: /home/atguigu/bin/dwd_to_dws.sh ${dt}
       
     - name: dws_to_dwt
       type: command
       dependsOn:
        - dwd_to_dws
       config:
        command: /home/atguigu/bin/dws_to_dwt.sh ${dt}
       
     - name: dwt_to_ads
       type: command
       dependsOn: 
        - dws_to_dwt
       config:
        command: /home/atguigu/bin/dwt_to_ads.sh ${dt}
        
     - name: hdfs_to_mysql
       type: command
       dependsOn:
        - dwt_to_ads
       config:
         command: /home/atguigu/bin/hdfs_to_mysql.sh all
   ```

3. 启动azkaban，打开web端，登陆账户，新建Project

4. 将2个文件打包压缩为zip文件后上传到web端Project

5. 单次手动执行Project，配置参数

   ![image-20200706212518938](离线数仓搭建.assets/image-20200706212518938.png)useExecutor参数可以指定执行的节点，若不配置则随机选择节点执行Project，需要保证执行节点满足执行条件，即部署了hive、sqoop、及yaml文件中使用的脚本，节点的id可在mysql中查询

   ```mysql
   use azkaban;
   select * from executors;
   ```

6. 定时执行Project![image-20200706213003941](离线数仓搭建.assets/image-20200706213003941.png)
   无需配置dt，因为脚本中默认日期即为前一日

